{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords     \n",
    "from nltk.util import ngrams                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4181b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"Cancer2/\"\n",
    "output_folder = \"25-3-2025_14_28/\"\n",
    "os.makedirs(output_folder, exist_ok=True)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b80f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_words = set([\n",
    "    \"article\", \"et\", \"al\", \"in\", \"to\", \"and\", \"for\", \"a\", \"y\", \"is\", \"of\", \"all\", \"the\", \"from\", \"are\", \"terms\", \"conditions\", \"publication\", \"citation\",\n",
    "    \"open\", \"access\", \"license\", \"cc\", \"by\", \"creative\", \"commons\", \"attribution\", \"shown\", \"method\", \"state\", \"date\", \"plot\", \"trials\", \"per\", \"cent\",\n",
    "    \"new\", \"present\", \"iii\", \"iv\", \"v\", \"etc\", \"proc\", \"natl\", \"acad\", \"sci\", \"usa\", \"vol\", \"pp\", \"using\", \"also\", \"used\", \"based\", \"may\", \"however\", \"one\",\n",
    "    \"two\", \"three\", \"four\", \"five\", \"data\", \"set\", \"including\", \"due\", \"figure\", \"table\", \"fig\", \"found\", \"work\", \"among\", \"study\", \"analysis\", \"different\",\n",
    "    \"several\", \"order\", \"low\", \"high\", \"higher\", \"lower\", \"within\", \"between\", \"without\", \"results\", \"approach\", \"across\", \"group\", \"suggest\", \"suggests\",\n",
    "    \"indicate\", \"indicates\", \"according\", \"amongst\", \"even\", \"although\", \"further\", \"well\", \"known\", \"previously\", \"recent\", \"recently\", \"first\", \"second\",\n",
    "    \"example\", \"examples\", \"others\", \"another\", \"obtained\", \"show\", \"shows\", \"would\", \"could\", \"can\", \"might\", \"many\", \"much\", \"certain\", \"some\", \"such\",\n",
    "    \"particular\", \"often\", \"sometimes\", \"always\", \"never\", \"previous\", \"past\", \"future\", \"uncommon\", \"rare\", \"frequent\", \"other\", \"additional\", \"extra\",\n",
    "    \"pros\", \"cons\", \"effective\", \"ineffective\", \"efficacy\", \"efficiency\", \"slow\", \"early\", \"late\", \"earlier\", \"latest\", \"delayed\", \"quick\", \"quicker\",\n",
    "    \"quickest\", \"rapid\", \"rapidly\", \"slowly\", \"gradual\", \"sudden\", \"short\", \"long\", \"shorter\", \"shortest\", \"longer\", \"longest\", \"temporary\", \"permanent\",\n",
    "    \"transient\", \"persistent\", \"mild\", \"moderate\", \"severe\", \"slight\", \"significant\", \"insignificant\", \"noticeable\", \"unnoticeable\", \"detectable\", \"wiley\",\n",
    "    \"online\", \"library\", \"downloaded\", \"https\", \"see\", \"rules\", \"use\", \"oa\", \"articles\", \"governed\", \"applicable\", \"national\", \"center\", \"health\",\n",
    "    \"statistics\", \"centers\", \"mortality\", \"public\", \"tapes\", \"american\", \"society\", \"atlanta\", \"tape\", \"reviews\", \"december\", \"volume\", \"nature\",\n",
    "    \"publishing\", \"average\", \"annual\", \"percent\", \"change\", \"note\", \"trends\", \"analyzed\", \"joinpoint\", \"program\", \"total\", \"leading\", \"causes\", \"us\",\n",
    "    \"standard\", \"end\", \"seer\", \"department\", \"individual\", \"b\", \"dvm\", \"phd\", \"year\", \"approximately\", \"fewer\", \"cases\", \"estimated\", \"alabama\", \"alaska\",\n",
    "    \"arizona\", \"arkansas\", \"california\", \"colorado\", \"connecticut\", \"delaware\", \"dist\", \"columbia\", \"florida\", \"georgia\", \"hawaii\", \"idaho\", \"illinois\",\n",
    "    \"indiana\", \"iowa\", \"kansas\", \"kentucky\", \"louisiana\", \"maine\", \"maryland\", \"massachusetts\", \"michigan\", \"minnesota\", \"mississippi\", \"missouri\",\n",
    "    \"montana\", \"nebraska\", \"nevada\", \"hampshire\", \"jersey\", \"mexico\", \"york\", \"north\", \"carolina\", \"dakota\", \"ohio\", \"oklahoma\", \"oregon\", \"pennsylvania\",\n",
    "    \"rhode\", \"island\", \"south\", \"tennessee\", \"texas\", \"utah\", \"vermont\", \"virginia\", \"washington\", \"west\", \"wisconsin\", \"wyoming\", \"ca\", \"clin\", \"j\",\n",
    "    \"apc\", \"aapc\", \"ons\", \"mp\", \"cl\", \"ries\", \"lag\", \"eisner\", \"naaccr\", \"annu\", \"april\", \"author\", \"available\", \"c\", \"d\", \"e\", \"g\", \"h\", \"hruban\", \"i\", \"l\",\n",
    "    \"maitra\", \"manuscript\", \"m\", \"n\", \"p\", \"page\", \"pathol\", \"pmc\", \"r\", \"rev\", \"s\", \"t\", \"u\", \"area\", \"as\", \"been\", \"by\", \"has\", \"have\", \"increased\",\n",
    "    \"induced\", \"international\", \"mg\", \"meter\", \"not\", \"on\", \"reported\", \"research\", \"square\", \"sources\", \"studies\", \"substrate\", \"technol\", \"than\", \"that\",\n",
    "    \"this\", \"topical\", \"under\", \"was\", \"with\", \"years\", \"younger\", \"journal\", \"notes\", \"tropic\", \"tr\", \"de\", \"carlos\", \"o\", \"publishers\", \"limited\",\n",
    "    \"explore\", \"factor\", \"final\", \"edited\", \"there\", \"funders\",\"x\", \"na\", \"number\", \"uses\", \"related\", \"text\", \"mining\", \"ai\", \"training\", \"similar\", \"guest\", \"website\", \"email\", \"info\", \"law\", \"lib\", \"personal\",\n",
    "    \"only\", \"march\", \"february\", \"august\", \"july\", \"october\", \"june\", \"provided\", \"original\", \"properly\", \"cited\", \"http\", \"visit\", \"please\", \"repository\",\n",
    "    \"more\", \"publications\", \"document\", \"identical\", \"content\", \"version\", \"postprint\", \"except\", \"adjusted\", \"normal\", \"dimension\", \"greatest\", \"cm\",\n",
    "    \"you\", \"will\", \"be\", \"given\", \"note\", \"note:\", \"yes\", \"no\", \"line\", \"segment\", \"control\", \"strictinin\", \"or\", \"published\", \"academy\", \"advertisement\",\n",
    "    \"ak\", \"az\", \"co\", \"dc\", \"begin\", \"blackadar\", \"charting\", \"combined\", \"copyright\", \"course\", \"day\", \"delivering\", \"director\", \"each\", \"exclude\",\n",
    "    \"expected\", \"fl\", \"ga\", \"hi\", \"id\", \"ia\", \"guide\", \"had\", \"hallmarks\", \"hereby\", \"highest\", \"included\", \"inclusion\", \"interpreted\", \"january\",\n",
    "    \"september\", \"november\", \"marked\", \"model\", \"must\", \"name\", \"offered\", \"occurrence\", \"office\", \"peak\", \"pointed\", \"printing\", \"pubmed\", \"rights\",\n",
    "    \"rough\", \"should\", \"signs\", \"source\", \"suppl\", \"support\", \"system\", \"trend\", \"unexpected\", \"updated\", \"versus\", \"yes,\", \"no\",\n",
    "    \"guest \",\"http\",\" march\",\" gut\", \"training\",\"similar\", \"guest\",\"http\", \"jnci\" ,\"no\",\" at \",\"university\", \"rounded \",\"nearest\" , \"excludes\",\"basal\", \"rounded \",\"nearest\",\" exclude \",\" basal\",\n",
    "    \"deaths each age do\", \"each age do sum\", \"age do sum ages\", \"do sum ages combined\",\"aapck\", \"abdelmohsen\", \"adv\", \"ajcc\", \"appropriate\", \"artificial\", \"asr\",\n",
    "    \"authors\",  \"committee\", \"competing\", \"credit\", \"csc\", \"curr\", \"current\", \"cumulative\", \"declare\", \"edited\", \"elmer\", \"error\",\"europe\", \"exp\", \"explore\", \"factor\", \"final\", \"financial\", \"form\",\n",
    "    \"funders\", \"inovação\", \"instituto\", \"interdiscip\", \"interests\", \"investigação\", \"intelligence\", \"jobin\", \"joint\", \"leone\", \"link\", \"manuscripts\", \"matulonis\", \"medium\", \"nat\", \"official\", \"opin\", \"our\", \"paper\",\n",
    "    \"percentage\", \"porto\", \"powell\", \"press\", \"primers\", \"provide\", \"rank\", \"schwabe\", \"sites\",\n",
    "    \"statement\", \"statements\", \"supp\", \"survival\", \"there\", \"were\", \"world\"\n",
    "    \"o\", \"q\", \"í\", \"w\", \"f\", \"k\", \"md\", \"none\",\"explor\" , \"ther\" ,\"int\" , \"pre\"\n",
    "])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(pdf_file):\n",
    "    doc = fitz.open(pdf_file)\n",
    "    text = \"\"\n",
    "    capture = False\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()\n",
    "        if \"abstract\" in page_text.lower():\n",
    "            capture = True\n",
    "        if \"references\" in page_text.lower():\n",
    "            capture = False\n",
    "        if capture:\n",
    "            text += page_text + \" \"\n",
    "    return text.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63888c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_ngram = {i: Counter() for i in range(1, 5)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d780aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(input_folder, file_name)\n",
    "        raw_text = extract_text(pdf_path)\n",
    "\n",
    "        if not raw_text:\n",
    "            continue\n",
    "\n",
    "        tokens = word_tokenize(raw_text)\n",
    "        filtered_tokens = [\n",
    "            word for word in tokens\n",
    "            if word.isalpha() and word not in unwanted_words and len(word) > 2\n",
    "        ]\n",
    "\n",
    "        for n in range(1, 5):\n",
    "            global_ngram[n].update(Counter(ngrams(filtered_tokens, n))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_file_path = os.path.join(output_folder, \"ngrams.txt\")\n",
    "with open(ngram_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for n in range(1, 5):\n",
    "        f.write(f\"\\n{n}-grams:\\n\")\n",
    "        for ngram, count in global_ngram[n].items():\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "        f.write(\"\\n\" + \"=\" * 50 + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d213364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "file_path = \"25-3-2025_14_28/ngrams.txt\"\n",
    "ngram_data = defaultdict(Counter)\n",
    "pattern = re.compile(r'^(\\d+)-grams:$')\n",
    "current_n = 0\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        current_n = int(match.group(1))\n",
    "        continue\n",
    "    if current_n > 0 and ':' in line:\n",
    "        ngram_str, count = line.rsplit(':', 1)\n",
    "        ngram_tuple = tuple(ngram_str.strip().split())\n",
    "        ngram_data[current_n][ngram_tuple] += int(count)\n",
    "\n",
    "def get_subgrams(ngram, sub_len):\n",
    "    return [tuple(ngram[i:i+sub_len]) for i in range(len(ngram) - sub_len + 1)]\n",
    "\n",
    "for higher_n in range(4, 1, -1):  \n",
    "    for higher_ngram, higher_count in ngram_data[higher_n].items():\n",
    "        for sub_len in range(1, higher_n):\n",
    "            for sub_ngram in get_subgrams(higher_ngram, sub_len):\n",
    "                if sub_ngram in ngram_data[sub_len]:\n",
    "                    ngram_data[sub_len][sub_ngram] -= higher_count\n",
    "\n",
    "for n in list(ngram_data.keys()):\n",
    "    ngram_data[n] = Counter({k: v for k, v in ngram_data[n].items() if v > 0})\n",
    "\n",
    "output_dir = \"25-3-2025_14_28\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "txt_path = os.path.join(output_dir, \"clean_ngram.txt\")\n",
    "csv_path = os.path.join(output_dir, \"clean_ngram.csv\")\n",
    "\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as txt_out, open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_out:\n",
    "    writer = csv.writer(csv_out)\n",
    "    writer.writerow([\"N-gram\", \"Frequency\"])\n",
    "    for n in sorted(ngram_data):\n",
    "        txt_out.write(f\"\\n{n}-grams:\\n\")\n",
    "        for ngram, count in ngram_data[n].most_common():\n",
    "            line = \" \".join(ngram) + f\": {count}\"\n",
    "            txt_out.write(line + \"\\n\")\n",
    "            writer.writerow([\" \".join(ngram), count])\n",
    "\n",
    "def save_top_clean_ngrams(filtered_ngrams, filename=\"top_1500_ngram.txt\", top_n=1500):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Top 1500 Bigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[2].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "        f.write(\"\\nTop 1500 Trigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[3].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "        f.write(\"\\nTop 1500 Fourgrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[4].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "top_ngrams_1500_path = os.path.join(output_dir, \"top_1500_ngram.txt\")\n",
    "save_top_clean_ngrams(ngram_data, filename=top_ngrams_1500_path, top_n=1500)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk import ngrams, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def build_sparse_cooccurrence_csv(input_folder, top_ngrams_file, output_file):\n",
    "    paper_files = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "    fourgrams = []\n",
    "\n",
    "    with open(top_ngrams_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        capture = False\n",
    "        for line in lines:\n",
    "            if \"Top 1500 Fourgrams:\" in line:\n",
    "                capture = True\n",
    "                continue\n",
    "            if capture and \":\" in line:\n",
    "                fourgram = line.split(\":\")[0].strip()\n",
    "                if fourgram:\n",
    "                    fourgrams.append(fourgram)\n",
    "                if len(fourgrams) == 1500: \n",
    "                    break\n",
    "\n",
    "    cooccurrence_data = []\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    for file_name in paper_files:\n",
    "        pdf_path = os.path.join(input_folder, file_name)\n",
    "        raw_text = extract_text(pdf_path)\n",
    "\n",
    "        if not raw_text.strip():\n",
    "            print(f\"Skipped (no extractable text): {file_name}\")\n",
    "            continue\n",
    "\n",
    "        tokens = [word for word in word_tokenize(raw_text.lower()) if word.isalpha() and word not in stop_words]\n",
    "        paper_ngrams = Counter(ngrams(tokens, 4))\n",
    "\n",
    "        for fourgram in fourgrams:\n",
    "            fourgram_tuple = tuple(fourgram.split())\n",
    "            count = paper_ngrams.get(fourgram_tuple, 0)\n",
    "            if count > 0:\n",
    "                cooccurrence_data.append((fourgram, file_name, count))\n",
    "\n",
    "        print(f\"Processed: {file_name}\")\n",
    "\n",
    "    df_sparse = pd.DataFrame(cooccurrence_data, columns=[\"Fourgram\", \"Paper\", \"Count\"])\n",
    "    df_sparse.to_csv(output_file, index=False)\n",
    "    print(f\"Co-occurrence matrix saved to: {output_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"Cancer2\"\n",
    "top_ngrams_file = \"25-3-2025_14_28/top_1500_ngram.txt\"\n",
    "output_file = \"25-3-2025_14_28/fourgram_cooccurrence.csv\"\n",
    "\n",
    "build_sparse_cooccurrence_csv(input_folder, top_ngrams_file, output_file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def load_top_1500_fourgrams(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    fourgrams = []\n",
    "    capture = False\n",
    "    for line in lines:\n",
    "        if \"Top 1500 Fourgrams:\" in line:\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture and \":\" in line:\n",
    "            fourgram = line.split(\":\")[0].strip()\n",
    "            if fourgram:\n",
    "                fourgrams.append(fourgram)\n",
    "            if len(fourgrams) == 1500:\n",
    "                break\n",
    "    return fourgrams\n",
    "\n",
    "def build_corpus(input_folder, top_fourgrams):\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "    corpus = []\n",
    "    for file in files:\n",
    "        pdf_path = os.path.join(input_folder, file)\n",
    "        text = extract_text(pdf_path)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        doc_ngrams = [\" \".join(g) for g in ngrams(tokens, 4)]\n",
    "        counts = Counter(doc_ngrams)\n",
    "        filtered_doc = []\n",
    "        for fg in top_fourgrams:\n",
    "            filtered_doc.extend([fg] * counts.get(fg, 0))\n",
    "        corpus.append(\" \".join(filtered_doc))\n",
    "    return corpus\n",
    "\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"Topic {topic_idx+1}: {' | '.join(top_words)}\")\n",
    "        topics.append(top_words)\n",
    "    return topics\n",
    "\n",
    "def run_lsa_lda(input_folder, top_ngrams_file, num_topics=10):\n",
    "    top_fourgrams = load_top_1500_fourgrams(top_ngrams_file)\n",
    "    corpus = build_corpus(input_folder, top_fourgrams)\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(vocabulary=top_fourgrams)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    lsa = TruncatedSVD(n_components=num_topics, random_state=42)\n",
    "    lsa.fit(tfidf)\n",
    "    print(\"\\n LSA Topics:\")\n",
    "    display_topics(lsa, tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    count_vectorizer = CountVectorizer(vocabulary=top_fourgrams)\n",
    "    count_data = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(count_data)\n",
    "    print(\"\\n LDA Topics:\")\n",
    "    display_topics(lda, count_vectorizer.get_feature_names_out())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim import corpora, models\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def load_top_fourgrams(top_ngrams_file):\n",
    "    fourgrams = []\n",
    "    with open(top_ngrams_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        capture = False\n",
    "        for line in lines:\n",
    "            if \"Top 1500 Fourgrams:\" in line:\n",
    "                capture = True\n",
    "                continue\n",
    "            if capture and \":\" in line:\n",
    "                fg = line.split(\":\")[0].strip()\n",
    "                if fg:\n",
    "                    fourgrams.append(\" \".join(fg.split()))\n",
    "                if len(fourgrams) == 1500:\n",
    "                    break\n",
    "    return fourgrams\n",
    "\n",
    "def get_documents_from_pdfs(input_folder):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    documents = []\n",
    "\n",
    "    for file in sorted(os.listdir(input_folder)):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            text = extract_text(os.path.join(input_folder, file))\n",
    "            tokens = [t.lower() for t in word_tokenize(text) if t.isalpha() and t.lower() not in stop_words]\n",
    "            documents.append(\" \".join(tokens))\n",
    "    return documents\n",
    "\n",
    "def extract_top_fourgram_docs(documents, top_fourgrams):\n",
    "    doc_texts = []\n",
    "    for doc in documents:\n",
    "        tokens = doc.split()\n",
    "        doc_ngrams = [\" \".join(ng) for ng in ngrams(tokens, 4)]\n",
    "        filtered = [ng for ng in doc_ngrams if ng in top_fourgrams]\n",
    "        doc_texts.append(\" \".join(filtered))\n",
    "    return doc_texts\n",
    "\n",
    "def perform_lsa(documents, num_topics):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    svd = TruncatedSVD(n_components=num_topics, random_state=42)\n",
    "    lsa = svd.fit(tfidf_matrix)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    lsa_topics = []\n",
    "    for i, comp in enumerate(lsa.components_):\n",
    "        terms_in_topic = [terms[idx] for idx in comp.argsort()[:-11:-1]]\n",
    "        lsa_topics.append([f\"Topic {i+1}\"] + terms_in_topic)\n",
    "    return lsa_topics\n",
    "\n",
    "def perform_lda(documents, num_topics):\n",
    "    tokenized_docs = [doc.split() for doc in documents]\n",
    "    dictionary = corpora.Dictionary(tokenized_docs)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_docs]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n",
    "\n",
    "    lda_topics = []\n",
    "    for i in range(num_topics):\n",
    "        terms = [word for word, _ in lda_model.show_topic(i, topn=10)]\n",
    "        lda_topics.append([f\"Topic {i+1}\"] + terms)\n",
    "    return lda_topics\n",
    "\n",
    "def save_topics_to_csv(lsa_topics, lda_topics, lsa_csv_file, lda_csv_file):\n",
    "    lsa_df = pd.DataFrame(lsa_topics, columns=[\"Topic\"] + [f\"Term{i+1}\" for i in range(10)])\n",
    "    lda_df = pd.DataFrame(lda_topics, columns=[\"Topic\"] + [f\"Term{i+1}\" for i in range(10)])\n",
    "\n",
    "    lsa_df.to_csv(lsa_csv_file, index=False)\n",
    "    lda_df.to_csv(lda_csv_file, index=False)\n",
    "\n",
    "input_folder = \"cancer2\"\n",
    "top_ngrams_file = \"25-3-2025_14_28/top_1500_ngram.txt\"\n",
    "lsa_csv_file = \"25-3-2025_14_28/lsa_topics.csv\"\n",
    "lda_csv_file = \"25-3-2025_14_28/lda_topics.csv\"\n",
    "num_topics = 12\n",
    "\n",
    "top_fourgrams = load_top_fourgrams(top_ngrams_file)\n",
    "raw_documents = get_documents_from_pdfs(input_folder)\n",
    "filtered_docs = extract_top_fourgram_docs(raw_documents, top_fourgrams)\n",
    "lsa_topics = perform_lsa(filtered_docs, num_topics)\n",
    "lda_topics = perform_lda(filtered_docs, num_topics)\n",
    "save_topics_to_csv(lsa_topics, lda_topics, lsa_csv_file, lda_csv_file)\n",
    "\n",
    "print(f\"Saved LSA topics to {lsa_csv_file}\")\n",
    "print(f\"Saved LDA topics to {lda_csv_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebdddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lsa_df = pd.read_csv(\"25-3-2025_14_28/lsa_topics.csv\")\n",
    "lda_df = pd.read_csv(\"25-3-2025_14_28/lda_topics.csv\")\n",
    "\n",
    "topic_names = {\n",
    "    \"Topic 1\": \"Diagnosis\",\n",
    "    \"Topic 2\": \"Cancer Cell Behavior\",\n",
    "    \"Topic 3\": \"Expression Analysis\",\n",
    "    \"Topic 4\": \"Tumor Characteristics\",\n",
    "    \"Topic 5\": \"Tumor Biology\",\n",
    "    \"Topic 6\": \"Cancer Therapy\",\n",
    "    \"Topic 7\": \"Immune Response\",\n",
    "    \"Topic 8\": \"Metastasis & Migration\",\n",
    "    \"Topic 9\": \"Clinical Trials\",\n",
    "    \"Topic 10\": \"Gene Expression\",\n",
    "    \"Topic 11\": \"Side Effects\",\n",
    "    \"Topic 12\": \"Survival & Prognosis\"\n",
    "}\n",
    "\n",
    "lsa_df.insert(1, \"Topic_Name\", lsa_df[\"Topic\"].map(topic_names))\n",
    "lda_df.insert(1, \"Topic_Name\", lda_df[\"Topic\"].map(topic_names))\n",
    "\n",
    "lsa_df.to_csv(\"25-3-2025_14_28/lsa_topics_named.csv\", index=False)\n",
    "lda_df.to_csv(\"25-3-2025_14_28/lda_topics_named.csv\", index=False)\n",
    "\n",
    "print(\"✅ Named LSA topics saved to lsa_topics_named.csv\")\n",
    "print(\"✅ Named LDA topics saved to lda_topics_named.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = \"25-3-2025_14_28/fourgram_cooccurrence.csv\"\n",
    "output_file = \"25-3-2025_14_28/fourgram_jaccard_similarity_v3.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "cooccurrence_matrix = df.pivot_table(index=\"Fourgram\", columns=\"Paper\", values=\"Count\", fill_value=0)\n",
    "binary_matrix = (cooccurrence_matrix > 0).astype(int)\n",
    "jaccard_matrix = pd.DataFrame(index=binary_matrix.index, columns=binary_matrix.columns)\n",
    "\n",
    "for paper in binary_matrix.columns:\n",
    "    paper_set = set(binary_matrix[binary_matrix[paper] == 1].index)\n",
    "    for fourgram in binary_matrix.index:\n",
    "        fg_set = {fourgram}\n",
    "        intersection = len(paper_set & fg_set)\n",
    "        union = len(paper_set | fg_set)\n",
    "        jaccard_score = intersection / union if union != 0 else 0\n",
    "        jaccard_matrix.at[fourgram, paper] = jaccard_score\n",
    "\n",
    "jaccard_matrix.to_csv(output_file)\n",
    "print(f\"Jaccard similarity matrix saved to: {output_file}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135aa9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "input_folder = \"Cancer2\"\n",
    "ngram_file = \"25-3-2025_14_28/top_1500_ngram.txt\"\n",
    "output_file = \"25-3-2025_14_28/fourgram_tfidf.csv\"\n",
    "\n",
    "with open(ngram_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    fourgrams = []\n",
    "    capture = False\n",
    "    for line in lines:\n",
    "        if \"Top 1500 Fourgrams:\" in line:\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture and \":\" in line:\n",
    "            fg = line.split(\":\")[0].strip()\n",
    "            if fg:\n",
    "                fourgrams.append(fg)\n",
    "            if len(fourgrams) == 1500:\n",
    "                break\n",
    "\n",
    "paper_texts = {}\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        path = os.path.join(input_folder, file)\n",
    "        text = extract_text(path).lower()\n",
    "        tokens = [w for w in word_tokenize(text) if w.isalpha() and w not in stop_words]\n",
    "        paper_texts[file] = \" \".join(tokens)\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=fourgrams, ngram_range=(4, 4), lowercase=True)\n",
    "corpus = list(paper_texts.values())\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=paper_texts.keys())\n",
    "df_tfidf = df_tfidf.T  \n",
    "df_tfidf.to_csv(output_file)\n",
    "\n",
    "print(\"✅ TF-IDF matrix saved to:\", output_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "base_path = \"25-3-2025_14_28\"\n",
    "tfidf_path = os.path.join(base_path, \"fourgram_tfidf.csv\")\n",
    "jaccard_path = os.path.join(base_path, \"fourgram_jaccard_similarity_v3.csv\")\n",
    "cooccurrence_path = os.path.join(base_path, \"fourgram_cooccurrence.csv\")\n",
    "lsa_topic_file = os.path.join(base_path, \"lsa_topics.csv\")\n",
    "lda_topic_file = os.path.join(base_path, \"lda_topics.csv\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tfidf_df = pd.read_csv(tfidf_path, index_col=0)\n",
    "jaccard_df = pd.read_csv(jaccard_path, index_col=0)\n",
    "cooccurrence_df = pd.read_csv(cooccurrence_path)\n",
    "\n",
    "top_n = 300\n",
    "\n",
    "top_tfidf = tfidf_df.mean(axis=1).sort_values(ascending=False).head(top_n).index.tolist()\n",
    "jaccard_df[\"Avg_Jaccard\"] = jaccard_df.mean(axis=1)\n",
    "top_jaccard = jaccard_df[jaccard_df[\"Avg_Jaccard\"] >= 0.1].index.tolist()\n",
    "top_cooccur = (\n",
    "    cooccurrence_df.groupby(\"Fourgram\")[\"Count\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(top_n)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "combined_fourgrams = sorted(set(top_tfidf + top_jaccard + top_cooccur))\n",
    "\n",
    "if os.path.exists(lsa_topic_file):\n",
    "    lsa_df = pd.read_csv(lsa_topic_file)\n",
    "    for col in lsa_df.columns:\n",
    "        if \"fourgram\" in col.lower():\n",
    "            combined_fourgrams += lsa_df[col].dropna().astype(str).unique().tolist()\n",
    "            break\n",
    "\n",
    "if os.path.exists(lda_topic_file):\n",
    "    lda_df = pd.read_csv(lda_topic_file)\n",
    "    for col in lda_df.columns:\n",
    "        if \"fourgram\" in col.lower():\n",
    "            combined_fourgrams += lda_df[col].dropna().astype(str).unique().tolist()\n",
    "            break\n",
    "\n",
    "\n",
    "combined_fourgrams = sorted(set(combined_fourgrams))\n",
    "\n",
    "dependency_data = []\n",
    "svo_list = []\n",
    "\n",
    "for fg in combined_fourgrams:\n",
    "    text = fg.replace(\"_\", \" \")\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        dependency_data.append({\n",
    "            \"Fourgram\": fg,\n",
    "            \"Word\": token.text,\n",
    "            \"Dependency\": token.dep_,\n",
    "            \"Head\": token.head.text,\n",
    "            \"Head_POS\": token.head.pos_\n",
    "        })\n",
    "    subj, verb, obj = None, None, None\n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "            subj = token.text\n",
    "        elif token.dep_ == \"ROOT\":\n",
    "            verb = token.text\n",
    "        elif token.dep_ in {\"dobj\", \"pobj\", \"obj\"}:\n",
    "            obj = token.text\n",
    "    if subj and verb and obj:\n",
    "        svo_list.append({\"Subject\": subj, \"Verb\": verb, \"Object\": obj})\n",
    "\n",
    "parsing_df = pd.DataFrame(dependency_data)\n",
    "svo_df = pd.DataFrame(svo_list)\n",
    "\n",
    "parsing_csv = os.path.join(base_path, \"parsing.csv\")   \n",
    "svo_csv = os.path.join(base_path, \"svooooooo.csv\")\n",
    "\n",
    "parsing_df.to_csv(parsing_csv, index=False)\n",
    "svo_df.to_csv(svo_csv, index=False)\n",
    "\n",
    "print(\"✅ Dependency parsing saved to:\", parsing_csv)\n",
    "print(\"✅ SVO triplets saved to:\", svo_csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e68c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "parsing_file = \"25-3-2025_14_28/parsing.csv\"   \n",
    "svo_file = \"25-3-2025_14_28/svooooooo.csv\"\n",
    "\n",
    "parsing_df = pd.read_csv(parsing_file)\n",
    "svo_df = pd.read_csv(svo_file)\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"Sourabh@123\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "def insert_dependency(tx, source, target, relation):\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (a:Word {text: $source})\n",
    "        SET a.name = $source\n",
    "        MERGE (b:Word {text: $target})\n",
    "        SET b.name = $target\n",
    "        MERGE (a)-[r:DEPENDS_ON]->(b)\n",
    "        SET r.relation = $relation\n",
    "    \"\"\", source=source, target=target, relation=relation)\n",
    "\n",
    "def insert_svo(tx, subj, verb, obj):\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (s:Entity {text: $subj})\n",
    "        SET s.name = $subj\n",
    "        MERGE (o:Entity {text: $obj})\n",
    "        SET o.name = $obj\n",
    "        MERGE (s)-[r:RELATION]->(o)\n",
    "        SET r.verb = $verb\n",
    "    \"\"\", subj=subj, obj=obj, verb=verb)\n",
    "\n",
    "with driver.session() as session:\n",
    "    for _, row in parsing_df.iterrows():\n",
    "        source = str(row.get(\"Word\", \"\")).strip().lower()\n",
    "        target = str(row.get(\"Head\", \"\")).strip().lower()\n",
    "        relation = str(row.get(\"Dependency\", \"\")).strip().lower()\n",
    "        if source and target and relation and \"nan\" not in [source, target, relation]:\n",
    "            session.execute_write(insert_dependency, source, target, relation)\n",
    "\n",
    "    for _, row in svo_df.iterrows():\n",
    "        subj = str(row.get(\"Subject\", \"\")).strip().lower()\n",
    "        verb = str(row.get(\"Verb\", \"\")).strip().lower()\n",
    "        obj = str(row.get(\"Object\", \"\")).strip().lower()\n",
    "        if subj and verb and obj and \"nan\" not in [subj, verb, obj]:\n",
    "            session.execute_write(insert_svo, subj, verb, obj)\n",
    "\n",
    "driver.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6666fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import threading\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from neo4j import GraphDatabase\n",
    "from fastapi.responses import HTMLResponse\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"Sourabh@123\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Neo4j Dependency + SVO API is running\"}\n",
    "\n",
    "@app.get(\"/search-form\", response_class=HTMLResponse)\n",
    "def search_form():\n",
    "    return \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Neo4j Dependency + SVO Search</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: 'Segoe UI', sans-serif;\n",
    "                background-color: #eef6fc;\n",
    "                padding: 40px;\n",
    "                color: #003366;\n",
    "            }\n",
    "            h2, h3 {\n",
    "                color: #005f99;\n",
    "            }\n",
    "            input, button {\n",
    "                padding: 10px;\n",
    "                font-size: 16px;\n",
    "                margin-right: 10px;\n",
    "                border: 1px solid #005f99;\n",
    "                border-radius: 4px;\n",
    "            }\n",
    "            button {\n",
    "                background-color: #005f99;\n",
    "                color: white;\n",
    "                cursor: pointer;\n",
    "            }\n",
    "            button:hover {\n",
    "                background-color: #004c80;\n",
    "            }\n",
    "            pre {\n",
    "                background-color: #ffffff;\n",
    "                padding: 15px;\n",
    "                border: 1px solid #cce0ff;\n",
    "                margin-top: 10px;\n",
    "                white-space: pre-wrap;\n",
    "                border-radius: 5px;\n",
    "            }\n",
    "            .section {\n",
    "                margin-top: 30px;\n",
    "            }\n",
    "            #loading {\n",
    "                display: none;\n",
    "                font-style: italic;\n",
    "                color: #555;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h2>Neo4j Dependency and SVO Search</h2>\n",
    "        <form id=\"searchForm\">\n",
    "            <input type=\"text\" id=\"query\" placeholder=\"Enter a keyword...\" required>\n",
    "            <button type=\"submit\">Search</button>\n",
    "            <span id=\"loading\">Loading...</span>\n",
    "        </form>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h3>Dependency Relations</h3>\n",
    "            <pre id=\"depsFriendly\"></pre>\n",
    "            <pre id=\"depsRaw\"></pre>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h3>SVO Relations</h3>\n",
    "            <pre id=\"svoFriendly\"></pre>\n",
    "            <pre id=\"svoRaw\"></pre>\n",
    "        </div>\n",
    "\n",
    "        <script>\n",
    "            const relationMap = {\n",
    "                \"nsubj\": \"is the subject of\",\n",
    "                \"obj\": \"is the object of\",\n",
    "                \"compound\": \"is a part of\",\n",
    "                \"amod\": \"is described by\",\n",
    "                \"advmod\": \"modifies (adverbially)\",\n",
    "                \"prep\": \"is related via preposition to\",\n",
    "                \"conj\": \"is connected with\",\n",
    "                \"poss\": \"is possessed by\",\n",
    "                \"det\": \"has a determiner\",\n",
    "                \"nmod\": \"has a modifier\",\n",
    "                \"acl\": \"has a clause modifying it\",\n",
    "                \"case\": \"is linked by a case marker\"\n",
    "            };\n",
    "\n",
    "            const form = document.getElementById('searchForm');\n",
    "            const loading = document.getElementById('loading');\n",
    "\n",
    "            form.onsubmit = async (e) => {\n",
    "                e.preventDefault();\n",
    "                const query = document.getElementById('query').value;\n",
    "                loading.style.display = \"inline\";\n",
    "\n",
    "                const depResponse = await fetch('/dependencies/search', {\n",
    "                    method: 'POST',\n",
    "                    headers: {'Content-Type': 'application/json'},\n",
    "                    body: JSON.stringify({ query })\n",
    "                });\n",
    "                const svoResponse = await fetch('/svo/search', {\n",
    "                    method: 'POST',\n",
    "                    headers: {'Content-Type': 'application/json'},\n",
    "                    body: JSON.stringify({ query })\n",
    "                });\n",
    "\n",
    "                const deps = await depResponse.json();\n",
    "                const svos = await svoResponse.json();\n",
    "\n",
    "                document.getElementById('depsRaw').textContent = JSON.stringify(deps, null, 2);\n",
    "                document.getElementById('svoRaw').textContent = JSON.stringify(svos, null, 2);\n",
    "\n",
    "                document.getElementById('depsFriendly').textContent = deps.map(d => {\n",
    "                    const readable = relationMap[d.relation] || `is related to (via \"${d.relation}\")`;\n",
    "                    return `${capitalize(d.source)} ${readable} ${capitalize(d.target)}`;\n",
    "                }).join(\"\\\\n\");\n",
    "\n",
    "                document.getElementById('svoFriendly').textContent = svos.map(d => {\n",
    "                    return `${capitalize(d.subject)} ${d.verb} ${capitalize(d.object)}`;\n",
    "                }).join(\"\\\\n\");\n",
    "\n",
    "                loading.style.display = \"none\";\n",
    "            };\n",
    "\n",
    "            function capitalize(word) {\n",
    "                return word.charAt(0).toUpperCase() + word.slice(1);\n",
    "            }\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "@app.post(\"/dependencies/search\")\n",
    "def search_dependencies(req: SearchRequest):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (a:Word)-[r:DEPENDS_ON]->(b:Word)\n",
    "            WHERE toLower(a.text) CONTAINS toLower($query)\n",
    "               OR toLower(b.text) CONTAINS toLower($query)\n",
    "               OR toLower(r.relation) CONTAINS toLower($query)\n",
    "            RETURN a.text AS source, r.relation AS relation, b.text AS target\n",
    "        \"\"\", {\"query\": req.query})\n",
    "        return [record.data() for record in result]\n",
    "\n",
    "@app.post(\"/svo/search\")\n",
    "def search_svo(req: SearchRequest):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (s:Entity)-[r:RELATION]->(o:Entity)\n",
    "            WHERE toLower(s.text) CONTAINS toLower($query)\n",
    "               OR toLower(o.text) CONTAINS toLower($query)\n",
    "               OR toLower(r.verb) CONTAINS toLower($query)\n",
    "            RETURN s.text AS subject, r.verb AS verb, o.text AS object\n",
    "        \"\"\", {\"query\": req.query})\n",
    "        return [record.data() for record in result]\n",
    "\n",
    "def run_app():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "threading.Thread(target=run_app).start()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb2b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca8942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "base_path = \"25-3-2025_14_28\"\n",
    "tfidf_path = os.path.join(base_path, \"fourgram_tfidf.csv\")\n",
    "jaccard_path = os.path.join(base_path, \"fourgram_jaccard_similarity_v3.csv\")\n",
    "cooccurrence_path = os.path.join(base_path, \"fourgram_cooccurrence.csv\")\n",
    "lsa_topic_file = os.path.join(base_path, \"lsa_topics.csv\")\n",
    "lda_topic_file = os.path.join(base_path, \"lda_topics.csv\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")  \n",
    "\n",
    "tfidf_df = pd.read_csv(tfidf_path, index_col=0)\n",
    "jaccard_df = pd.read_csv(jaccard_path, index_col=0)\n",
    "cooccurrence_df = pd.read_csv(cooccurrence_path)\n",
    "\n",
    "top_n = 750\n",
    "\n",
    "top_tfidf = tfidf_df.mean(axis=1).sort_values(ascending=False).head(top_n).index.tolist()\n",
    "jaccard_df[\"Avg_Jaccard\"] = jaccard_df.mean(axis=1)\n",
    "top_jaccard = jaccard_df[jaccard_df[\"Avg_Jaccard\"] >= 0.1].index.tolist()\n",
    "top_cooccur = (\n",
    "    cooccurrence_df.groupby(\"Fourgram\")[\"Count\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(top_n)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "combined_fourgrams = sorted(set(top_tfidf + top_jaccard + top_cooccur))\n",
    "\n",
    "if os.path.exists(lsa_topic_file):\n",
    "    lsa_df = pd.read_csv(lsa_topic_file)\n",
    "    for col in lsa_df.columns:\n",
    "        if \"fourgram\" in col.lower():\n",
    "            combined_fourgrams += lsa_df[col].dropna().astype(str).unique().tolist()\n",
    "            break\n",
    "\n",
    "if os.path.exists(lda_topic_file):\n",
    "    lda_df = pd.read_csv(lda_topic_file)\n",
    "    for col in lda_df.columns:\n",
    "        if \"fourgram\" in col.lower():\n",
    "            combined_fourgrams += lda_df[col].dropna().astype(str).unique().tolist()\n",
    "            break\n",
    "\n",
    "combined_fourgrams = sorted(set(combined_fourgrams))\n",
    "\n",
    "dependency_data = []\n",
    "valid_relations = []  \n",
    "\n",
    "def is_complete_word(word):\n",
    "    return len(word) > 1 and word.isalpha()\n",
    "\n",
    "def is_valid_relation(subj, verb, obj):\n",
    "    return len(set([subj, verb, obj])) == 3  \n",
    "\n",
    "for fg in combined_fourgrams:\n",
    "    text = fg.replace(\"_\", \" \")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for token in doc:\n",
    "        dependency_data.append({\n",
    "            \"Fourgram\": fg,\n",
    "            \"Word\": token.text,\n",
    "            \"Dependency\": token.dep_,\n",
    "            \"Head\": token.head.text,\n",
    "            \"Head_POS\": token.head.pos_\n",
    "        })\n",
    "\n",
    "    subj, verb, obj = None, None, None\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"nsubj\", \"nsubjpass\"} and is_complete_word(token.text):\n",
    "            subj = token.text\n",
    "        elif token.dep_ == \"ROOT\" and is_complete_word(token.text):\n",
    "            verb = token.text\n",
    "        elif token.dep_ in {\"dobj\", \"pobj\", \"obj\"} and is_complete_word(token.text):\n",
    "            obj = token.text\n",
    "\n",
    "    if subj and verb and obj:\n",
    "        if is_valid_relation(subj, verb, obj) and \"is part of\" not in [subj, verb, obj] and \"of\" not in [subj, verb, obj]:\n",
    "            valid_relations.append({\"Subject\": subj, \"Verb\": verb, \"Object\": obj})\n",
    "\n",
    "parsing_df = pd.DataFrame(dependency_data)\n",
    "relations_df = pd.DataFrame(valid_relations)\n",
    "\n",
    "parsing_csv = os.path.join(base_path, \"parsing.csv\")\n",
    "relations_csv = os.path.join(base_path, \"relations_sci.csv\")\n",
    "\n",
    "parsing_df.to_csv(parsing_csv, index=False)\n",
    "relations_df.to_csv(relations_csv, index=False)\n",
    "\n",
    "print(\"✅ Dependency parsing saved to:\", parsing_csv)\n",
    "print(\"✅ Valid scientific relations saved to:\", relations_csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "parsing_file = \"25-3-2025_14_28/parsing.csv\"   \n",
    "relations_file = \"25-3-2025_14_28/relations_sci.csv\"  \n",
    "\n",
    "parsing_df = pd.read_csv(parsing_file)\n",
    "relations_df = pd.read_csv(relations_file)\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"sourabh@123\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "def insert_dependency(tx, source, target, relation):\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (a:Word {text: $source})\n",
    "        SET a.name = $source\n",
    "        MERGE (b:Word {text: $target})\n",
    "        SET b.name = $target\n",
    "        MERGE (a)-[r:DEPENDS_ON]->(b)\n",
    "        SET r.relation = $relation\n",
    "    \"\"\", source=source, target=target, relation=relation)\n",
    "\n",
    "def insert_svo(tx, subj, verb, obj):\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (s:Entity {text: $subj})\n",
    "        SET s.name = $subj\n",
    "        MERGE (o:Entity {text: $obj})\n",
    "        SET o.name = $obj\n",
    "        MERGE (s)-[r:RELATION]->(o)\n",
    "        SET r.verb = $verb\n",
    "    \"\"\", subj=subj, obj=obj, verb=verb)\n",
    "\n",
    "with driver.session() as session:\n",
    "    for _, row in parsing_df.iterrows():\n",
    "        source = str(row.get(\"Word\", \"\")).strip().lower()\n",
    "        target = str(row.get(\"Head\", \"\")).strip().lower()\n",
    "        relation = str(row.get(\"Dependency\", \"\")).strip().lower()\n",
    "        \n",
    "        if source and target and relation and \"nan\" not in [source, target, relation]:\n",
    "            session.execute_write(insert_dependency, source, target, relation)\n",
    "\n",
    "    for _, row in relations_df.iterrows():\n",
    "        subj = str(row.get(\"Subject\", \"\")).strip().lower()\n",
    "        verb = str(row.get(\"Verb\", \"\")).strip().lower()\n",
    "        obj = str(row.get(\"Object\", \"\")).strip().lower()\n",
    "        \n",
    "        if subj and verb and obj and \"nan\" not in [subj, verb, obj]:\n",
    "            session.execute_write(insert_svo, subj, verb, obj)\n",
    "\n",
    "driver.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d21b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import threading\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from neo4j import GraphDatabase\n",
    "from fastapi.responses import HTMLResponse\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"sourabh@123\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Neo4j Dependency + SVO API is running\"}\n",
    "\n",
    "@app.get(\"/search-form\", response_class=HTMLResponse)\n",
    "def search_form():\n",
    "    return \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Neo4j Dependency + SVO Search</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: 'Segoe UI', sans-serif;\n",
    "                background-color: #eef6fc;\n",
    "                padding: 40px;\n",
    "                color: #003366;\n",
    "            }\n",
    "            h2, h3 {\n",
    "                color: #005f99;\n",
    "            }\n",
    "            input, button {\n",
    "                padding: 10px;\n",
    "                font-size: 16px;\n",
    "                margin-right: 10px;\n",
    "                border: 1px solid #005f99;\n",
    "                border-radius: 4px;\n",
    "            }\n",
    "            button {\n",
    "                background-color: #005f99;\n",
    "                color: white;\n",
    "                cursor: pointer;\n",
    "            }\n",
    "            button:hover {\n",
    "                background-color: #004c80;\n",
    "            }\n",
    "            pre {\n",
    "                background-color: #ffffff;\n",
    "                padding: 15px;\n",
    "                border: 1px solid #cce0ff;\n",
    "                margin-top: 10px;\n",
    "                white-space: pre-wrap;\n",
    "                border-radius: 5px;\n",
    "            }\n",
    "            .section {\n",
    "                margin-top: 30px;\n",
    "            }\n",
    "            #loading {\n",
    "                display: none;\n",
    "                font-style: italic;\n",
    "                color: #555;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h2>Neo4j Dependency and SVO Search</h2>\n",
    "        <form id=\"searchForm\">\n",
    "            <input type=\"text\" id=\"query\" placeholder=\"Enter a keyword...\" required>\n",
    "            <button type=\"submit\">Search</button>\n",
    "            <span id=\"loading\">Loading...</span>\n",
    "        </form>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h3>Dependency Relations</h3>\n",
    "            <pre id=\"depsFriendly\"></pre>\n",
    "            <pre id=\"depsRaw\"></pre>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section\">\n",
    "            <h3>SVO Relations</h3>\n",
    "            <pre id=\"svoFriendly\"></pre>\n",
    "            <pre id=\"svoRaw\"></pre>\n",
    "        </div>\n",
    "\n",
    "        <script>\n",
    "            const relationMap = {\n",
    "                \"nsubj\": \"is the subject of\",\n",
    "                \"obj\": \"is the object of\",\n",
    "                \"compound\": \"is a part of\",\n",
    "                \"amod\": \"is described by\",\n",
    "                \"advmod\": \"modifies (adverbially)\",\n",
    "                \"prep\": \"is related via preposition to\",\n",
    "                \"conj\": \"is connected with\",\n",
    "                \"poss\": \"is possessed by\",\n",
    "                \"det\": \"has a determiner\",\n",
    "                \"nmod\": \"has a modifier\",\n",
    "                \"acl\": \"has a clause modifying it\",\n",
    "                \"case\": \"is linked by a case marker\"\n",
    "            };\n",
    "\n",
    "            const form = document.getElementById('searchForm');\n",
    "            const loading = document.getElementById('loading');\n",
    "\n",
    "            form.onsubmit = async (e) => {\n",
    "                e.preventDefault();\n",
    "                const query = document.getElementById('query').value;\n",
    "                loading.style.display = \"inline\";\n",
    "\n",
    "                const depResponse = await fetch('/dependencies/search', {\n",
    "                    method: 'POST',\n",
    "                    headers: {'Content-Type': 'application/json'},\n",
    "                    body: JSON.stringify({ query })\n",
    "                });\n",
    "                const svoResponse = await fetch('/svo/search', {\n",
    "                    method: 'POST',\n",
    "                    headers: {'Content-Type': 'application/json'},\n",
    "                    body: JSON.stringify({ query })\n",
    "                });\n",
    "\n",
    "                const deps = await depResponse.json();\n",
    "                const svos = await svoResponse.json();\n",
    "\n",
    "                document.getElementById('depsRaw').textContent = JSON.stringify(deps, null, 2);\n",
    "                document.getElementById('svoRaw').textContent = JSON.stringify(svos, null, 2);\n",
    "\n",
    "                document.getElementById('depsFriendly').textContent = deps.map(d => {\n",
    "                    const readable = relationMap[d.relation] || `is related to (via \"${d.relation}\")`;\n",
    "                    return `${capitalize(d.source)} ${readable} ${capitalize(d.target)}`;\n",
    "                }).join(\"\\\\n\");\n",
    "\n",
    "                document.getElementById('svoFriendly').textContent = svos.map(d => {\n",
    "                    return `${capitalize(d.subject)} ${d.verb} ${capitalize(d.object)}`;\n",
    "                }).join(\"\\\\n\");\n",
    "\n",
    "                loading.style.display = \"none\";\n",
    "            };\n",
    "\n",
    "            function capitalize(word) {\n",
    "                return word.charAt(0).toUpperCase() + word.slice(1);\n",
    "            }\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "@app.post(\"/dependencies/search\")\n",
    "def search_dependencies(req: SearchRequest):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (a:Word)-[r:DEPENDS_ON]->(b:Word)\n",
    "            WHERE toLower(a.text) CONTAINS toLower($query)\n",
    "               OR toLower(b.text) CONTAINS toLower($query)\n",
    "               OR toLower(r.relation) CONTAINS toLower($query)\n",
    "            RETURN a.text AS source, r.relation AS relation, b.text AS target\n",
    "        \"\"\", {\"query\": req.query})\n",
    "        return [record.data() for record in result]\n",
    "\n",
    "@app.post(\"/svo/search\")\n",
    "def search_svo(req: SearchRequest):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (s:Entity)-[r:RELATION]->(o:Entity)\n",
    "            WHERE toLower(s.text) CONTAINS toLower($query)\n",
    "               OR toLower(o.text) CONTAINS toLower($query)\n",
    "               OR toLower(r.verb) CONTAINS toLower($query)\n",
    "            RETURN s.text AS subject, r.verb AS verb, o.text AS object\n",
    "        \"\"\", {\"query\": req.query})\n",
    "        return [record.data() for record in result]\n",
    "\n",
    "def run_app():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "threading.Thread(target=run_app).start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c61b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
