{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f6931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz              \n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.util import ngrams\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f74aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "input_folder = \"Cancer2/\"\n",
    "output_folder = \"25-3-2025_14_29_3_5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c83204",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_stopwords = set(stopwords.words('english'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_words = set([\n",
    "    \"article\",\"http\", \"et\", \"al\", \"in\", \"to\", \"and\", \"for\", \"a\", \"y\", \"is\", \"of\", \"all\", \"the\", \"from\", \"are\", \"http\",\n",
    "    \"terms\", \"conditions\", \"publication\", \"citation\", \"open\", \"access\", \"license\", \"cc\", \"by\", \"creative\",\n",
    "    \"commons\", \"attribution\", \"shown\", \"method\", \"state\", \"date\", \"plot\", \"trials\", \"per\", \"cent\", \"new\",\n",
    "    \"present\", \"iii\", \"iv\", \"v\", \"etc\", \"proc\", \"natl\", \"acad\", \"sci\", \"usa\", \"vol\", \"pp\", \"using\", \"also\",\n",
    "    \"used\", \"based\", \"may\", \"however\", \"one\", \"two\", \"three\", \"four\", \"five\", \"data\", \"set\", \"including\",\n",
    "    \"due\", \"figure\", \"table\", \"fig\", \"found\", \"work\", \"among\", \"study\", \"analysis\", \"different\", \"several\",\n",
    "    \"order\", \"low\", \"high\", \"higher\", \"lower\", \"within\", \"between\", \"without\", \"results\", \"approach\", \"across\",\n",
    "    \"group\", \"suggest\", \"suggests\", \"indicate\", \"indicates\", \"according\", \"amongst\", \"even\", \"although\", \"further\",\n",
    "    \"well\", \"known\", \"previously\", \"recent\", \"recently\", \"first\", \"second\", \"example\", \"examples\", \"others\",\n",
    "    \"another\", \"obtained\", \"show\", \"shows\", \"would\", \"could\", \"can\", \"might\", \"many\", \"much\", \"certain\", \"some\",\n",
    "    \"such\", \"particular\", \"often\", \"sometimes\", \"always\", \"never\", \"previous\", \"past\", \"future\", \"uncommon\", \n",
    "    \"rare\", \"frequent\", \"other\", \"additional\", \"extra\", \"pros\", \"cons\", \"effective\", \"ineffective\", \"efficacy\", \n",
    "    \"efficiency\", \"slow\", \"early\", \"late\", \"earlier\", \"latest\", \"delayed\", \"quick\", \"quicker\", \"quickest\", \"rapid\", \n",
    "    \"rapidly\", \"slowly\", \"gradual\", \"sudden\", \"short\", \"long\", \"shorter\", \"shortest\", \"longer\", \"longest\", \"temporary\", \n",
    "    \"permanent\", \"transient\", \"persistent\", \"mild\", \"moderate\", \"severe\", \"slight\", \"significant\", \"insignificant\", \n",
    "    \"noticeable\", \"unnoticeable\", \"detectable\", \"wiley\", \"online\", \"library\", \"downloaded\", \"https\", \"see\", \"rules\", \n",
    "    \"use\", \"oa\", \"articles\", \"governed\", \"applicable\", \"national\", \"center\", \"health\", \"statistics\", \"centers\", \n",
    "    \"mortality\", \"public\", \"tapes\", \"american\", \"society\", \"atlanta\", \"tape\", \"reviews\", \"december\", \"volume\", \n",
    "    \"nature\", \"publishing\", \"average\", \"annual\", \"percent\", \"change\", \"note\", \"trends\", \"analyzed\", \"joinpoint\", \n",
    "    \"program\", \"total\", \"leading\", \"ca\", \"clin\", \"j\", \"apc\", \"aapc\", \"ons\", \"mp\", \"cl\", \"ries\", \"lag\", \"eisner\", \n",
    "    \"naaccr\", \"annu\", \"april\", \"author\", \"available\", \"c\", \"d\", \"e\", \"g\", \"h\", \"hruban\", \"i\", \"l\", \"maitra\", \"manuscript\",\n",
    "    \"m\", \"n\", \"p\", \"page\", \"pathol\", \"pmc\", \"r\", \"rev\", \"s\", \"t\", \"u\", \"area\", \"as\", \"been\", \"by\", \"has\", \"have\", \"increased\",\n",
    "    \"induced\", \"international\", \"mg\", \"meter\", \"not\", \"on\", \"reported\", \"research\", \"square\", \"sources\", \"studies\", \n",
    "    \"substrate\", \"technol\", \"than\", \"that\", \"this\", \"topical\", \"under\", \"was\", \"with\", \"years\", \"younger\", \"journal\", \n",
    "    \"notes\", \"tropic\", \"tr\", \"de\", \"carlos\", \"o\", \"publishers\", \"limited\", \"explore\", \"factor\", \"final\", \"edited\", \n",
    "    \"there\", \"funders\", \"x\", \"na\", \"number\", \"uses\", \"related\", \"text\", \"mining\", \"ai\", \"training\", \"similar\", \n",
    "    \"guest\", \"website\", \"email\", \"info\", \"law\", \"lib\", \"personal\", \"only\", \"march\", \"february\", \"august\", \"july\", \n",
    "    \"october\", \"june\", \"provided\", \"original\", \"properly\", \"cited\", \"http\", \"visit\", \"please\", \"repository\", \"more\", \n",
    "    \"publications\", \"document\", \"identical\", \"content\", \"version\", \"postprint\", \"except\", \"adjusted\", \"normal\", \n",
    "    \"dimension\", \"greatest\", \"cm\", \"you\", \"will\", \"be\", \"given\", \"note\", \"note:\", \"yes\", \"no\", \"line\", \"segment\", \n",
    "    \"control\", \"strictinin\", \"or\", \"published\", \"academy\", \"advertisement\", \"ak\", \"az\", \"co\", \"dc\", \"begin\", \"blackadar\", \n",
    "    \"charting\", \"combined\", \"copyright\", \"course\", \"day\", \"delivering\", \"director\", \"each\", \"exclude\", \"expected\", \n",
    "    \"fl\", \"ga\", \"hi\", \"id\", \"ia\", \"guide\", \"had\", \"hallmarks\", \"hereby\", \"highest\", \"included\", \"inclusion\", \"interpreted\", \n",
    "    \"january\", \"september\", \"november\", \"marked\", \"model\", \"must\", \"name\", \"offered\", \"occurrence\", \"office\", \n",
    "    \"peak\", \"pointed\", \"printing\", \"pubmed\", \"rights\", \"rough\", \"should\", \"signs\", \"source\", \"suppl\",\"https\", \"support\", \n",
    "    \"system\", \"trend\", \"unexpected\", \"updated\", \"versus\", \"yes,\", \"no\", \"guest \",\"http\",\" march\",\" gut\", \"training\",\n",
    "    \"similar\", \"guest\",\"http\", \"jnci\" ,\"no\",\" at \",\"university\", \"rounded \",\"nearest\" , \"excludes\",\"basal\", \n",
    "    \"rounded \",\"nearest\",\" exclude \",\" basal\",\"deaths each age do\", \"each age do sum\", \"age do sum ages\", \n",
    "    \"do sum ages combined\",\"aapck\", \"abdelmohsen\", \"adv\", \"ajcc\", \"appropriate\", \"artificial\", \"asr\",\"authors\",\n",
    "    \"committee\", \"competing\", \"credit\", \"csc\", \"curr\", \"current\", \"cumulative\", \"declare\", \"edited\", \"elmer\", \n",
    "    \"error\",\"europe\", \"exp\", \"explore\", \"factor\", \"final\", \"financial\", \"form\",\"funders\", \"inovação\",\"instituto\",\n",
    "    \"interdiscip\", \"interests\", \"investigação\", \"intelligence\", \"jobin\", \"joint\", \"leone\", \"link\", \"manuscripts\", \n",
    "    \"matulonis\", \"medium\", \"nat\", \"official\", \"opin\", \"our\", \"paper\",\"percentage\", \"porto\", \"powell\", \"press\", \"primers\", \n",
    "    \"provide\", \"rank\", \"schwabe\", \"sites\",\"statement\", \"statements\", \"supp\", \"survival\", \"there\", \"were\", \"world\",\n",
    "    \"o\", \"q\", \"í\", \"w\", \"f\", \"k\", \"md\", \"none\",\"explor\" , \"ther\" ,\"int\",\"abstract\", \"author\", \"manuscript\", \"pubmed\", \"copyright\", \"doi\", \n",
    "    \"journal\", \"page\", \"volume\", \"dataset\", \"support\", \"evidence\", \"available\", \"online\", \"open\", \"access\", \"introduction\", \"conclusion\", \"american\", \n",
    "    \"society\", \"united\", \"states\", \"london\", \"canada\", \"china\",  \"europe\", \"japan\", \"india\", \"harvard\", \"medical\", \"school\", \"boston\",  \"western\", \"fruit\", ])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef17f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = unwanted_words.union(standard_stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cce1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'): return wordnet.ADJ\n",
    "    elif tag.startswith('V'): return wordnet.VERB\n",
    "    elif tag.startswith('N'): return wordnet.NOUN\n",
    "    elif tag.startswith('R'): return wordnet.ADV\n",
    "    else: return wordnet.NOUN\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tagged = pos_tag(tokens)\n",
    "    return [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9222e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(pdf_file):\n",
    "    doc = fitz.open(pdf_file)\n",
    "    text = \"\"\n",
    "    capture = False\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()\n",
    "        if \"abstract\" in page_text.lower():\n",
    "            capture = True\n",
    "        if \"references\" in page_text.lower():\n",
    "            capture = False\n",
    "        if capture:\n",
    "            text += page_text + \" \"\n",
    "    return text.lower()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_ngram = {i: Counter() for i in range(1, 5)}\n",
    "ngram_documents = {i: defaultdict(set) for i in range(1, 5)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20554fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "standard_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Your global unwanted_words list must already be defined elsewhere\n",
    "unwanted_set = set(u.strip().lower() for u in unwanted_words)\n",
    "all_stopwords = unwanted_set.union(standard_stopwords)\n",
    "\n",
    "# Input/output setup\n",
    "min_word_length = 3\n",
    "thresholds = {1: 150, 2: 100, 3: 54, 4: 25}\n",
    "input_folder = \"Cancer2/\"\n",
    "output_folder = \"25-3-2025_14_29_3_5/\"\n",
    "ngram_file_path = os.path.join(output_folder, \"ngrams.txt\")\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    return \" \".join(page.get_text(\"text\") for page in doc)\n",
    "\n",
    "def is_valid_alphanumeric(word):\n",
    "    return bool(re.match(r'^[a-zA-Z]+\\d+$', word)) or bool(re.match(r'^\\d+[a-zA-Z]+$', word))\n",
    "\n",
    "def is_broken_word(word):\n",
    "    if is_valid_alphanumeric(word):\n",
    "        return False\n",
    "    return (\n",
    "        len(word) < min_word_length or\n",
    "        word.endswith('-') or\n",
    "        word.isdigit() or\n",
    "        not re.match(r'^[a-zA-Z0-9]+$', word)\n",
    "    )\n",
    "\n",
    "def valid_token(w):\n",
    "    return not is_broken_word(w)\n",
    "\n",
    "def clean_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [w for w in tokens if valid_token(w)]\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmas = [token.lemma_.lower() for token in doc]\n",
    "    return [lemma for lemma in lemmas if lemma not in all_stopwords]\n",
    "\n",
    "def is_bad_hyphenated(phrase):\n",
    "    if '-' not in phrase:\n",
    "        return False\n",
    "    return not re.match(r'^[a-zA-Z]+\\s*-\\s*\\d+$', phrase)\n",
    "\n",
    "def process_file(file_path, file_name):\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    local_ngrams = defaultdict(Counter)\n",
    "    ngram_docs = defaultdict(set)\n",
    "\n",
    "    raw_text = extract_text_from_pdf(file_path)\n",
    "    if not raw_text:\n",
    "        return local_ngrams, ngram_docs, file_name\n",
    "\n",
    "    tokens = clean_and_lemmatize(raw_text)\n",
    "\n",
    "    for n in range(1, 5):\n",
    "        for ng in ngrams(tokens, n):\n",
    "            phrase = \" \".join(ng)\n",
    "            if phrase not in unwanted_set and not is_bad_hyphenated(phrase):\n",
    "                local_ngrams[n][ng] += 1\n",
    "                ngram_docs[n].add(ng)\n",
    "\n",
    "    return local_ngrams, ngram_docs, file_name\n",
    "\n",
    "def process_pdfs_parallel(input_folder):\n",
    "    global_ngram = defaultdict(Counter)\n",
    "    ngram_documents = defaultdict(lambda: defaultdict(set))\n",
    "\n",
    "    tasks = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for file_name in os.listdir(input_folder):\n",
    "            if file_name.endswith(\".pdf\"):\n",
    "                file_path = os.path.join(input_folder, file_name)\n",
    "                tasks.append(executor.submit(process_file, file_path, file_name))\n",
    "\n",
    "        for task in tasks:\n",
    "            local_ngrams, ngram_docs, file_name = task.result()\n",
    "            for n in range(1, 5):\n",
    "                global_ngram[n].update(local_ngrams[n])\n",
    "                for ng in ngram_docs[n]:\n",
    "                    ngram_documents[n][ng].add(file_name)\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for n in range(1, 5):\n",
    "        with open(os.path.join(output_folder, f\"ngram_{n}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            for ngram, count in global_ngram[n].items():\n",
    "                f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "    with open(ngram_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for n in range(1, 5):\n",
    "            f.write(f\"\\n{n}-grams (≥{thresholds[n]} times & in ≥5 documents):\\n\")\n",
    "            for ngram, count in global_ngram[n].items():\n",
    "                if count >= thresholds[n] and len(ngram_documents[n][ngram]) >= 5:\n",
    "                    f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "            f.write(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "# Run\n",
    "process_pdfs_parallel(input_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict, Counter   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780dff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {1: 150, 2: 100, 3: 54, 4: 25}\n",
    "ngram_data = defaultdict(Counter)\n",
    "pattern = re.compile(r'^(\\d+)-grams.*:$')\n",
    "current_n = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504407b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"25-3-2025_14_29_3/ngrams.txt\"\n",
    "output_dir = \"25-3-2025_14_29_3_5/\"\n",
    "os.makedirs(output_dir, exist_ok=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31702d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e205df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        current_n = int(match.group(1))\n",
    "        continue\n",
    "    if current_n > 0 and ':' in line:\n",
    "        try:\n",
    "            ngram_str, count = line.rsplit(':', 1)\n",
    "            ngram_tuple = tuple(ngram_str.strip().split())\n",
    "            ngram_data[current_n][ngram_tuple] += int(count.strip())\n",
    "        except ValueError:\n",
    "            continue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd364b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgrams(ngram, sub_len):\n",
    "    return [tuple(ngram[i:i+sub_len]) for i in range(len(ngram) - sub_len + 1)]\n",
    "\n",
    "# Remove counts of sub-grams from higher-order n-grams\n",
    "for higher_n in range(4, 1, -1):\n",
    "    for higher_ngram, higher_count in ngram_data[higher_n].items():\n",
    "        for sub_len in range(1, higher_n):\n",
    "            for sub_ngram in get_subgrams(higher_ngram, sub_len):\n",
    "                if sub_ngram in ngram_data[sub_len]:\n",
    "                    ngram_data[sub_len][sub_ngram] -= higher_count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a889d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in list(ngram_data.keys()):\n",
    "    ngram_data[n] = Counter({k: v for k, v in ngram_data[n].items() if v > 0})\n",
    "\n",
    "# Function to check if an n-gram contains alphanumeric words (e.g., p53, cyp2d6)\n",
    "def contains_valid_alphanum(ngram):\n",
    "    # Regex checks if word is alphanumeric: either letters followed by digits or digits followed by letters\n",
    "    return any(re.match(r'^[a-zA-Z]+\\d+$', word) or re.match(r'^\\d+[a-zA-Z]+$', word) for word in ngram)\n",
    "\n",
    "# Apply thresholds but retain alphanumeric n-grams regardless of frequency\n",
    "for n in ngram_data:\n",
    "    threshold = thresholds[n]\n",
    "    ngram_data[n] = Counter({\n",
    "        k: v for k, v in ngram_data[n].items()\n",
    "        if v >= threshold or contains_valid_alphanum(k)  # Keep alphanumeric n-grams even if below threshold\n",
    "    })\n",
    "\n",
    "# Write the cleaned n-grams to text and CSV files\n",
    "txt_path = os.path.join(output_dir, \"clean_ngram.txt\")\n",
    "csv_path = os.path.join(output_dir, \"clean_ngram.csv\")\n",
    "\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as txt_out, open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_out:\n",
    "    writer = csv.writer(csv_out)\n",
    "    writer.writerow([\"N-gram\", \"Frequency\"])\n",
    "    for n in sorted(ngram_data):\n",
    "        txt_out.write(f\"\\n{n}-grams:\\n\")\n",
    "        for ngram, count in ngram_data[n].most_common():\n",
    "            txt_out.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "            writer.writerow([\" \".join(ngram), count])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_clean_ngrams(filtered_ngrams, filename=\"top_1500_ngram.txt\", top_n=1500):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Top 1500 unigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[1].most_common(top_n):     \n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "        f.write(\"Top 1500 Bigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[2].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "        f.write(\"\\nTop 1500 Trigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[3].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "        f.write(\"\\nTop 1500 Fourgrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[4].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "# Save the top 1500 n-grams\n",
    "top_ngrams_1500_path = os.path.join(output_dir, \"top_1500_ngram.txt\")\n",
    "save_top_clean_ngrams(ngram_data, filename=top_ngrams_1500_path, top_n=1500)\n",
    "\n",
    "print(\"Cleaned and top n-gram files saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6da45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict, Counter    \n",
    "\n",
    "thresholds = {1: 150, 2: 100, 3: 54, 4: 25}\n",
    "ngram_data = defaultdict(Counter)\n",
    "pattern = re.compile(r'^(\\d+)-grams.*:$')\n",
    "current_n = 0   \n",
    "\n",
    "file_path = \"25-3-2025_14_29_3/ngrams.txt\"\n",
    "output_dir = \"25-3-2025_14_29_3_5/\"\n",
    "os.makedirs(output_dir, exist_ok=True)   \n",
    "\n",
    "# Read n-grams from file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()  \n",
    "\n",
    "# Process the n-grams from the file\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        current_n = int(match.group(1))\n",
    "        continue\n",
    "    if current_n > 0 and ':' in line:\n",
    "        try:\n",
    "            ngram_str, count = line.rsplit(':', 1)\n",
    "            ngram_tuple = tuple(ngram_str.strip().split())\n",
    "            ngram_data[current_n][ngram_tuple] += int(count.strip())\n",
    "        except ValueError:\n",
    "            continue    \n",
    "\n",
    "# Function to generate sub-grams\n",
    "def get_subgrams(ngram, sub_len):\n",
    "    return [tuple(ngram[i:i+sub_len]) for i in range(len(ngram) - sub_len + 1)]\n",
    "\n",
    "# Remove counts of sub-grams from higher-order n-grams\n",
    "for higher_n in range(4, 1, -1):\n",
    "    for higher_ngram, higher_count in ngram_data[higher_n].items():\n",
    "        for sub_len in range(1, higher_n):\n",
    "            for sub_ngram in get_subgrams(higher_ngram, sub_len):\n",
    "                if sub_ngram in ngram_data[sub_len]:\n",
    "                    ngram_data[sub_len][sub_ngram] -= higher_count  \n",
    "\n",
    "# Remove zero or negative frequencies\n",
    "for n in list(ngram_data.keys()):\n",
    "    ngram_data[n] = Counter({k: v for k, v in ngram_data[n].items() if v > 0})\n",
    "\n",
    "# Function to keep alphanumeric n-grams even if below the threshold\n",
    "def contains_valid_alphanum(ngram):\n",
    "    return any(re.match(r'^[a-zA-Z]+\\d+$', word) or re.match(r'^\\d+[a-zA-Z]+$', word) for word in ngram)\n",
    "\n",
    "# Apply thresholds but retain alphanumeric n-grams regardless of frequency\n",
    "for n in ngram_data:\n",
    "    threshold = thresholds[n]\n",
    "    ngram_data[n] = Counter({\n",
    "        k: v for k, v in ngram_data[n].items()\n",
    "        if v >= threshold or contains_valid_alphanum(k)\n",
    "    })\n",
    "\n",
    "# Write the cleaned n-grams to text and CSV files\n",
    "txt_path = os.path.join(output_dir, \"clean_ngram.txt\")\n",
    "csv_path = os.path.join(output_dir, \"clean_ngram.csv\")\n",
    "\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as txt_out, open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_out:\n",
    "    writer = csv.writer(csv_out)\n",
    "    writer.writerow([\"N-gram\", \"Frequency\"])\n",
    "    for n in sorted(ngram_data):\n",
    "        txt_out.write(f\"\\n{n}-grams:\\n\")\n",
    "        for ngram, count in ngram_data[n].most_common():\n",
    "            txt_out.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "            writer.writerow([\" \".join(ngram), count])   \n",
    "\n",
    "# Function to save the top 1500 n-grams\n",
    "def save_top_clean_ngrams(filtered_ngrams, filename=\"top_1500_ngram.txt\", top_n=1500):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Top 1500 unigrams:\\n\")\n",
    "        # Always include alphanumeric n-grams in the top 1500 list\n",
    "        for ngram, count in filtered_ngrams[1].most_common(top_n):     \n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "        f.write(\"Top 1500 Bigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[2].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "        f.write(\"\\nTop 1500 Trigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[3].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "        f.write(\"\\nTop 1500 Fourgrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[4].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "# Save the top 1500 n-grams\n",
    "top_ngrams_1500_path = os.path.join(output_dir, \"top_1500_ngram.txt\")\n",
    "save_top_clean_ngrams(ngram_data, filename=top_ngrams_1500_path, top_n=1500)\n",
    "\n",
    "print(\"Cleaned and top n-gram files saved.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict, Counter    \n",
    "\n",
    "thresholds = {1: 150, 2: 100, 3: 54, 4: 25}\n",
    "ngram_data = defaultdict(Counter)\n",
    "pattern = re.compile(r'^(\\d+)-grams.*:$')\n",
    "current_n = 0   \n",
    "\n",
    "file_path = \"25-3-2025_14_29_3/ngrams.txt\"\n",
    "output_dir = \"25-3-2025_14_29_3_5/\"\n",
    "os.makedirs(output_dir, exist_ok=True)   \n",
    "\n",
    "# Read n-grams from file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()  \n",
    "\n",
    "# Process the n-grams from the file\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        current_n = int(match.group(1))\n",
    "        continue\n",
    "    if current_n > 0 and ':' in line:\n",
    "        try:\n",
    "            ngram_str, count = line.rsplit(':', 1)\n",
    "            ngram_tuple = tuple(ngram_str.strip().split())\n",
    "            ngram_data[current_n][ngram_tuple] += int(count.strip())\n",
    "        except ValueError:\n",
    "            continue    \n",
    "\n",
    "# Function to generate sub-grams\n",
    "def get_subgrams(ngram, sub_len):\n",
    "    return [tuple(ngram[i:i+sub_len]) for i in range(len(ngram) - sub_len + 1)]\n",
    "\n",
    "# Remove counts of sub-grams from higher-order n-grams\n",
    "for higher_n in range(4, 1, -1):\n",
    "    for higher_ngram, higher_count in ngram_data[higher_n].items():\n",
    "        for sub_len in range(1, higher_n):\n",
    "            for sub_ngram in get_subgrams(higher_ngram, sub_len):\n",
    "                if sub_ngram in ngram_data[sub_len]:\n",
    "                    ngram_data[sub_len][sub_ngram] -= higher_count  \n",
    "\n",
    "# Remove zero or negative frequencies\n",
    "for n in list(ngram_data.keys()):\n",
    "    ngram_data[n] = Counter({k: v for k, v in ngram_data[n].items() if v > 0})\n",
    "\n",
    "# Function to check for alphanumeric n-grams (must always be kept)\n",
    "def contains_valid_alphanum(ngram):\n",
    "    # Accept mixed alphanumeric words like TP53, IL-6, CD4+, BRCA1/2, H3K27me3\n",
    "    return any(\n",
    "        re.search(r'[a-zA-Z]', word) and re.search(r'[\\d/+.-]', word)\n",
    "        for word in ngram\n",
    "    )\n",
    "\n",
    "# Apply thresholds but retain alphanumeric n-grams regardless of frequency\n",
    "for n in ngram_data:\n",
    "    threshold = thresholds[n]\n",
    "    ngram_data[n] = Counter({\n",
    "        k: v for k, v in ngram_data[n].items()\n",
    "        if v >= threshold or contains_valid_alphanum(k)  # Keep alphanumeric n-grams regardless of frequency\n",
    "    })\n",
    "\n",
    "# Write the cleaned n-grams to text and CSV files\n",
    "txt_path = os.path.join(output_dir, \"clean_ngram.txt\")\n",
    "csv_path = os.path.join(output_dir, \"clean_ngram.csv\")\n",
    "\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as txt_out, open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_out:\n",
    "    writer = csv.writer(csv_out)\n",
    "    writer.writerow([\"N-gram\", \"Frequency\"])\n",
    "    for n in sorted(ngram_data):\n",
    "        txt_out.write(f\"\\n{n}-grams:\\n\")\n",
    "        for ngram, count in ngram_data[n].most_common():\n",
    "            txt_out.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "            writer.writerow([\" \".join(ngram), count])   \n",
    "\n",
    "# Function to save the top 1500 n-grams\n",
    "def save_top_clean_ngrams(filtered_ngrams, filename=\"top_1500_ngram.txt\", top_n=1500):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Top 1500 unigrams:\\n\")\n",
    "        # Always include alphanumeric n-grams in the top 1500 list\n",
    "        for ngram, count in filtered_ngrams[1].most_common(top_n):     \n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "        f.write(\"Top 1500 Bigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[2].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "        f.write(\"\\nTop 1500 Trigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[3].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "        f.write(\"\\nTop 1500 Fourgrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[4].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "# Save the top 1500 n-grams\n",
    "top_ngrams_1500_path = os.path.join(output_dir, \"top_1500_ngram.txt\")\n",
    "save_top_clean_ngrams(ngram_data, filename=top_ngrams_1500_path, top_n=1500)\n",
    "\n",
    "print(\"Cleaned and top n-gram files saved.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53764aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict, Counter    \n",
    "\n",
    "thresholds = {1: 150, 2: 100, 3: 54, 4: 25}\n",
    "ngram_data = defaultdict(Counter)\n",
    "pattern = re.compile(r'^(\\d+)-grams.*:$')\n",
    "current_n = 0   \n",
    "\n",
    "file_path = \"25-3-2025_14_29_3/ngrams.txt\"\n",
    "output_dir = \"25-3-2025_14_29_3_5/\"\n",
    "os.makedirs(output_dir, exist_ok=True)   \n",
    "\n",
    "# Read n-grams from file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()  \n",
    "\n",
    "# Process the n-grams from the file\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        current_n = int(match.group(1))\n",
    "        continue\n",
    "    if current_n > 0 and ':' in line:\n",
    "        try:\n",
    "            ngram_str, count = line.rsplit(':', 1)\n",
    "            ngram_tuple = tuple(ngram_str.strip().split())\n",
    "            ngram_data[current_n][ngram_tuple] += int(count.strip())\n",
    "        except ValueError:\n",
    "            continue    \n",
    "\n",
    "# Function to generate sub-grams\n",
    "def get_subgrams(ngram, sub_len):\n",
    "    return [tuple(ngram[i:i+sub_len]) for i in range(len(ngram) - sub_len + 1)]\n",
    "\n",
    "# Function to detect alphanumerical tokens (like \"p53\", \"H1N1\")\n",
    "def contains_valid_alphanum(ngram):\n",
    "    return any(re.match(r'^[a-zA-Z]+\\d+$', word) or re.match(r'^\\d+[a-zA-Z]+$', word) for word in ngram)\n",
    "\n",
    "# Subtract higher-order n-grams from lower-order ones\n",
    "for higher_n in range(4, 1, -1):\n",
    "    for higher_ngram, higher_count in ngram_data[higher_n].items():\n",
    "        for sub_len in range(1, higher_n):               \n",
    "            for sub_ngram in get_subgrams(higher_ngram, sub_len):\n",
    "                if sub_ngram in ngram_data[sub_len]:\n",
    "                    ngram_data[sub_len][sub_ngram] -= higher_count\n",
    "\n",
    "# Clean: Keep only positive counts or alphanumerics even if ≤ 0\n",
    "for n in list(ngram_data.keys()):\n",
    "    updated = Counter()\n",
    "    for k, v in ngram_data[n].items():\n",
    "        if v > 0 or contains_valid_alphanum(k):  # Protect alphanumeric n-grams even if their count is ≤ 0\n",
    "            if v <= 0 and contains_valid_alphanum(k):\n",
    "                v = original_ngram_data[n][k]  # Restore original frequency for alphanumeric n-grams\n",
    "            updated[k] = v\n",
    "    ngram_data[n] = updated\n",
    "\n",
    "# Apply frequency thresholds but keep alphanumerics\n",
    "for n in list(ngram_data.keys()):\n",
    "    ngram_data[n] = Counter({\n",
    "        k: v for k, v in ngram_data[n].items()\n",
    "        if v >= thresholds[n] or contains_valid_alphanum(k)  # Always keep alphanumeric n-grams\n",
    "    })\n",
    "\n",
    "# Write cleaned n-grams to files\n",
    "txt_path = os.path.join(output_dir, \"clean_ngram_2.txt\")\n",
    "csv_path = os.path.join(output_dir, \"clean_ngram_2.csv\")\n",
    "\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as txt_out, open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_out:\n",
    "    writer = csv.writer(csv_out)\n",
    "    writer.writerow([\"N-gram\", \"Frequency\"])\n",
    "    for n in sorted(ngram_data):\n",
    "        txt_out.write(f\"\\n{n}-grams:\\n\")\n",
    "        for ngram, count in ngram_data[n].most_common():\n",
    "            txt_out.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "            writer.writerow([\" \".join(ngram), count])\n",
    "\n",
    "# Save top 1500 n-grams (with alphanumeric protection) per n-level\n",
    "def save_top_clean_ngrams(filtered_ngrams, filename=\"top_1500_ngram_2.txt\", top_n=1500):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for n in range(1, 5):\n",
    "            f.write(f\"\\nTop {top_n} {['Unigrams', 'Bigrams', 'Trigrams', 'Fourgrams'][n-1]}:\\n\")\n",
    "            top_ngrams = filtered_ngrams[n].most_common(top_n)\n",
    "            seen = set(ngram for ngram, _ in top_ngrams)\n",
    "            alphanum_ngrams = [\n",
    "                (ngram, count) for ngram, count in filtered_ngrams[n].items()\n",
    "                if contains_valid_alphanum(ngram) and ngram not in seen\n",
    "            ]\n",
    "            combined = top_ngrams + alphanum_ngrams\n",
    "            for ngram, count in combined:\n",
    "                f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "top_ngrams_1500_path = os.path.join(output_dir, \"top_1500_ngram_2.txt\")\n",
    "save_top_clean_ngrams(ngram_data, filename=top_ngrams_1500_path, top_n=1500)\n",
    "\n",
    "print(\"✅ Cleaned and top n-gram files saved with alphanumeric protection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea10f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfghjkl;kjhugvb nm, kjhugv nmjhgfcv bn,mkjhb nbhgv ccxzcdcyvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb0f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict, Counter    \n",
    "\n",
    "thresholds = {1: 150, 2: 100, 3: 54, 4: 25}\n",
    "ngram_data = defaultdict(Counter)\n",
    "pattern = re.compile(r'^(\\d+)-grams.*:$')\n",
    "current_n = 0   \n",
    "\n",
    "file_path = \"25-3-2025_14_29_3/ngrams.txt\"\n",
    "output_dir = \"25-3-2025_14_29_3_5/\"\n",
    "os.makedirs(output_dir, exist_ok=True)   \n",
    "\n",
    "# Read n-grams from file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()  \n",
    "\n",
    "# Process the n-grams from the file\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        current_n = int(match.group(1))\n",
    "        continue\n",
    "    if current_n > 0 and ':' in line:\n",
    "        try:\n",
    "            ngram_str, count = line.rsplit(':', 1)\n",
    "            ngram_tuple = tuple(ngram_str.strip().split())\n",
    "            ngram_data[current_n][ngram_tuple] += int(count.strip())\n",
    "        except ValueError:\n",
    "            continue    \n",
    "\n",
    "# Function to generate sub-grams\n",
    "def get_subgrams(ngram, sub_len):\n",
    "    return [tuple(ngram[i:i+sub_len]) for i in range(len(ngram) - sub_len + 1)]\n",
    "\n",
    "# Function to detect alphanumerical tokens (like \"p53\", \"H1N1\")\n",
    "def contains_valid_alphanum(ngram):\n",
    "    return any(re.match(r'^[a-zA-Z]+\\d+$', word) or re.match(r'^\\d+[a-zA-Z]+$', word) for word in ngram)\n",
    "\n",
    "# Subtract higher-order n-grams from lower-order ones\n",
    "for higher_n in range(4, 1, -1):\n",
    "    for higher_ngram, higher_count in ngram_data[higher_n].items():\n",
    "        for sub_len in range(1, higher_n):\n",
    "            for sub_ngram in get_subgrams(higher_ngram, sub_len):\n",
    "                if sub_ngram in ngram_data[sub_len]:\n",
    "                    ngram_data[sub_len][sub_ngram] -= higher_count\n",
    "\n",
    "# Clean: Keep only positive counts or alphanumerics even if ≤ 0\n",
    "for n in list(ngram_data.keys()):\n",
    "    updated = Counter()\n",
    "    for k, v in ngram_data[n].items():\n",
    "        if v > 0 or contains_valid_alphanum(k):  # Protect alphanumeric n-grams even if their count is ≤ 0\n",
    "            # Ensure alphanumeric n-grams retain their original frequency if modified\n",
    "            if v <= 0 and contains_valid_alphanum(k):\n",
    "                v = ngram_data[n][k]  # Keep original count for alphanumeric n-grams\n",
    "            updated[k] = v\n",
    "    ngram_data[n] = updated\n",
    "\n",
    "# Apply frequency thresholds but keep alphanumerics\n",
    "for n in list(ngram_data.keys()):\n",
    "    ngram_data[n] = Counter({\n",
    "        k: v for k, v in ngram_data[n].items()\n",
    "        if v >= thresholds[n] or contains_valid_alphanum(k)  # Always keep alphanumeric n-grams\n",
    "    })\n",
    "\n",
    "# Write cleaned n-grams to files\n",
    "txt_path = os.path.join(output_dir, \"clean_ngram_2.txt\")\n",
    "csv_path = os.path.join(output_dir, \"clean_ngram_2.csv\")\n",
    "\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as txt_out, open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_out:\n",
    "    writer = csv.writer(csv_out)\n",
    "    writer.writerow([\"N-gram\", \"Frequency\"])\n",
    "    for n in sorted(ngram_data):\n",
    "        txt_out.write(f\"\\n{n}-grams:\\n\")\n",
    "        for ngram, count in ngram_data[n].most_common():\n",
    "            txt_out.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "            writer.writerow([\" \".join(ngram), count])\n",
    "\n",
    "# Save top 1500 n-grams (with alphanumeric protection) per n-level\n",
    "def save_top_clean_ngrams(filtered_ngrams, filename=\"top_1500_ngram_2.txt\", top_n=1500):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for n in range(1, 5):\n",
    "            f.write(f\"\\nTop {top_n} {['Unigrams', 'Bigrams', 'Trigrams', 'Fourgrams'][n-1]}:\\n\")\n",
    "            top_ngrams = filtered_ngrams[n].most_common(top_n)\n",
    "            seen = set(ngram for ngram, _ in top_ngrams)\n",
    "            alphanum_ngrams = [\n",
    "                (ngram, count) for ngram, count in filtered_ngrams[n].items()\n",
    "                if contains_valid_alphanum(ngram) and ngram not in seen\n",
    "            ]\n",
    "            combined = top_ngrams + alphanum_ngrams\n",
    "            for ngram, count in combined:\n",
    "                f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "top_ngrams_1500_path = os.path.join(output_dir, \"top_1500_ngram_2.txt\")\n",
    "save_top_clean_ngrams(ngram_data, filename=top_ngrams_1500_path, top_n=1500)\n",
    "\n",
    "print(\"✅ Cleaned and top n-gram files saved with alphanumeric protection.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f843098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict, Counter    \n",
    "\n",
    "thresholds = {1: 150, 2: 100, 3: 54, 4: 25}\n",
    "ngram_data = defaultdict(Counter)\n",
    "pattern = re.compile(r'^(\\d+)-grams.*:$')\n",
    "current_n = 0   \n",
    "\n",
    "file_path = \"25-3-2025_14_29_3_5/ngrams.txt\"\n",
    "output_dir = \"25-3-2025_14_29_3_5/\"\n",
    "os.makedirs(output_dir, exist_ok=True)   \n",
    "\n",
    "# Read n-grams from file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()  \n",
    "\n",
    "# Process the n-grams from the file\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        current_n = int(match.group(1))\n",
    "        continue\n",
    "    if current_n > 0 and ':' in line:\n",
    "        try:\n",
    "            ngram_str, count = line.rsplit(':', 1)\n",
    "            ngram_tuple = tuple(ngram_str.strip().split())\n",
    "            ngram_data[current_n][ngram_tuple] += int(count.strip())\n",
    "        except ValueError:\n",
    "            continue    \n",
    "\n",
    "# Function to generate sub-grams\n",
    "def get_subgrams(ngram, sub_len):\n",
    "    return [tuple(ngram[i:i+sub_len]) for i in range(len(ngram) - sub_len + 1)]\n",
    "\n",
    "# Function to detect alphanumerical tokens (like \"p53\", \"H1N1\")\n",
    "def contains_valid_alphanum(ngram):\n",
    "    return any(re.match(r'^[a-zA-Z]+\\d+$', word) or re.match(r'^\\d+[a-zA-Z]+$', word) for word in ngram)\n",
    "\n",
    "# Track alphanumeric n-grams separately to ensure they are never modified or removed\n",
    "protected_ngrams = defaultdict(Counter)\n",
    "\n",
    "# Subtract higher-order n-grams from lower-order ones\n",
    "for higher_n in range(4, 1, -1):\n",
    "    for higher_ngram, higher_count in ngram_data[higher_n].items():\n",
    "        for sub_len in range(1, higher_n):\n",
    "            for sub_ngram in get_subgrams(higher_ngram, sub_len):\n",
    "                if sub_ngram in ngram_data[sub_len]:\n",
    "                    ngram_data[sub_len][sub_ngram] -= higher_count\n",
    "\n",
    "# Store the original counts of alphanumeric n-grams\n",
    "for n in list(ngram_data.keys()):\n",
    "    for k, v in ngram_data[n].items():\n",
    "        if contains_valid_alphanum(k):\n",
    "            protected_ngrams[n][k] = v  # Save the original count for alphanumeric n-grams\n",
    "\n",
    "# Clean: Remove negative counts but protect alphanumeric n-grams\n",
    "for n in list(ngram_data.keys()):\n",
    "    updated = Counter()\n",
    "    for k, v in ngram_data[n].items():\n",
    "        if v > 0 or contains_valid_alphanum(k):  # Protect alphanumeric n-grams even if their count is ≤ 0\n",
    "            updated[k] = v\n",
    "    ngram_data[n] = updated\n",
    "\n",
    "# Apply frequency thresholds but keep alphanumerics intact\n",
    "for n in list(ngram_data.keys()):\n",
    "    ngram_data[n] = Counter({\n",
    "        k: v for k, v in ngram_data[n].items()\n",
    "        if v >= thresholds[n] or contains_valid_alphanum(k)  # Always keep alphanumeric n-grams\n",
    "    })\n",
    "\n",
    "# Reintroduce protected alphanumeric n-grams with their original counts (if not already present)\n",
    "for n in list(ngram_data.keys()):\n",
    "    for k, v in protected_ngrams[n].items():\n",
    "        if k not in ngram_data[n]:\n",
    "            ngram_data[n][k] = v  # Restore the original count\n",
    "\n",
    "# Write cleaned n-grams to files\n",
    "txt_path = os.path.join(output_dir, \"clean_ngram_2.txt\")\n",
    "csv_path = os.path.join(output_dir, \"clean_ngram_2.csv\")\n",
    "\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as txt_out, open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_out:\n",
    "    writer = csv.writer(csv_out)\n",
    "    writer.writerow([\"N-gram\", \"Frequency\"])\n",
    "    for n in sorted(ngram_data):\n",
    "        txt_out.write(f\"\\n{n}-grams:\\n\")\n",
    "        for ngram, count in ngram_data[n].most_common():\n",
    "            txt_out.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "            writer.writerow([\" \".join(ngram), count])\n",
    "\n",
    "# Save top 1500 n-grams (with alphanumeric protection) per n-level\n",
    "def save_top_clean_ngrams(filtered_ngrams, filename=\"top_1500_ngram_2.txt\", top_n=1500):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for n in range(1, 5):\n",
    "            f.write(f\"\\nTop {top_n} {['Unigrams', 'Bigrams', 'Trigrams', 'Fourgrams'][n-1]}:\\n\")\n",
    "            top_ngrams = filtered_ngrams[n].most_common(top_n)\n",
    "            seen = set(ngram for ngram, _ in top_ngrams)\n",
    "            alphanum_ngrams = [\n",
    "                (ngram, count) for ngram, count in filtered_ngrams[n].items()\n",
    "                if contains_valid_alphanum(ngram) and ngram not in seen\n",
    "            ]\n",
    "            combined = top_ngrams + alphanum_ngrams\n",
    "            for ngram, count in combined:\n",
    "                f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "top_ngrams_1500_path = os.path.join(output_dir, \"top_1500_ngram_2.txt\")\n",
    "save_top_clean_ngrams(ngram_data, filename=top_ngrams_1500_path, top_n=1500)\n",
    "\n",
    "print(\"✅ Cleaned and top n-gram files saved with alphanumeric protection.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967c1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned and Top Priority n-gram files saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Priority Cancer Keywords\n",
    "priority_keywords = set([\n",
    "    \"cancer\", \"tumor\", \"mutation\", \"gene\", \"treatment\", \"chemotherapy\", \"radiation\", \n",
    "    \"immunotherapy\", \"biopsy\", \"diagnosis\", \"survival\", \"metastasis\", \"cell\", \"therapy\", \n",
    "    \"drug\", \"prognosis\", \"malignancy\", \"oncologist\", \"carcinoma\"\n",
    "])\n",
    "\n",
    "# Helper Functions\n",
    "def get_subgrams(ngram, sub_len):\n",
    "    return [tuple(ngram[i:i+sub_len]) for i in range(len(ngram) - sub_len + 1)]\n",
    "\n",
    "def contains_valid_alphanum(ngram):\n",
    "    return any(re.match(r'^[a-zA-Z]+\\d+$', word) or re.match(r'^\\d+[a-zA-Z]+$', word) for word in ngram)\n",
    "\n",
    "def contains_priority_keyword(ngram):\n",
    "    return any(word.lower() in priority_keywords for word in ngram)\n",
    "\n",
    "# Thresholds\n",
    "thresholds = {1: 150, 2: 100, 3: 54, 4: 25}\n",
    "\n",
    "# Setup\n",
    "ngram_data = defaultdict(Counter)\n",
    "pattern = re.compile(r'^(\\d+)-grams.*:$')\n",
    "current_n = 0\n",
    "\n",
    "file_path = \"25-3-2025_14_29_3_5/ngrams.txt\"\n",
    "output_dir = \"25-3-2025_14_29_3_5/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read n-grams from file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Process the n-grams\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        current_n = int(match.group(1))\n",
    "        continue\n",
    "    if current_n > 0 and ':' in line:\n",
    "        try:\n",
    "            ngram_str, count = line.rsplit(':', 1)\n",
    "            ngram_tuple = tuple(ngram_str.strip().split())\n",
    "            ngram_data[current_n][ngram_tuple] += int(count.strip())\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "# Track protected alphanumeric ngrams\n",
    "protected_ngrams = defaultdict(Counter)\n",
    "\n",
    "# Subtract higher-order n-grams from lower-order\n",
    "for higher_n in range(4, 1, -1):\n",
    "    for higher_ngram, higher_count in ngram_data[higher_n].items():\n",
    "        for sub_len in range(1, higher_n):\n",
    "            for sub_ngram in get_subgrams(higher_ngram, sub_len):\n",
    "                if sub_ngram in ngram_data[sub_len]:\n",
    "                    ngram_data[sub_len][sub_ngram] -= higher_count\n",
    "\n",
    "# Save protected alphanumerics\n",
    "for n in list(ngram_data.keys()):\n",
    "    for k, v in ngram_data[n].items():\n",
    "        if contains_valid_alphanum(k):\n",
    "            protected_ngrams[n][k] = v\n",
    "\n",
    "# Clean: remove negatives but keep alphanumerics and priority keywords\n",
    "for n in list(ngram_data.keys()):\n",
    "    updated = Counter()\n",
    "    for k, v in ngram_data[n].items():\n",
    "        if v > 0 or contains_valid_alphanum(k) or contains_priority_keyword(k):\n",
    "            updated[k] = v\n",
    "    ngram_data[n] = updated\n",
    "\n",
    "# Apply thresholds but keep important n-grams\n",
    "for n in list(ngram_data.keys()):\n",
    "    ngram_data[n] = Counter({\n",
    "        k: v for k, v in ngram_data[n].items()\n",
    "        if v >= thresholds[n] or contains_valid_alphanum(k) or contains_priority_keyword(k)\n",
    "    })\n",
    "\n",
    "# Restore protected alphanumerics if missing\n",
    "for n in list(ngram_data.keys()):\n",
    "    for k, v in protected_ngrams[n].items():\n",
    "        if k not in ngram_data[n]:\n",
    "            ngram_data[n][k] = v\n",
    "\n",
    "# Write cleaned n-grams\n",
    "txt_path = os.path.join(output_dir, \"clean_ngram_2.txt\")\n",
    "csv_path = os.path.join(output_dir, \"clean_ngram_2.csv\")\n",
    "\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as txt_out, open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_out:\n",
    "    writer = csv.writer(csv_out)\n",
    "    writer.writerow([\"N-gram\", \"Frequency\"])\n",
    "    for n in sorted(ngram_data):\n",
    "        txt_out.write(f\"\\n{n}-grams:\\n\")\n",
    "        for ngram, count in ngram_data[n].most_common():\n",
    "            txt_out.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "            writer.writerow([\" \".join(ngram), count])\n",
    "\n",
    "# Save Top 1500 per ngram level (keeping priority and alphanumerics)\n",
    "def save_top_clean_ngrams(filtered_ngrams, filename=\"top_1500_ngram-2_priority.txt\", top_n=1500):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for n in range(1, 5):\n",
    "            f.write(f\"\\nTop {top_n} {['Unigrams', 'Bigrams', 'Trigrams', 'Fourgrams'][n-1]}:\\n\")\n",
    "            top_ngrams = filtered_ngrams[n].most_common(top_n)\n",
    "            seen = set(ngram for ngram, _ in top_ngrams)\n",
    "            alphanum_priority_ngrams = [\n",
    "                (ngram, count) for ngram, count in filtered_ngrams[n].items()\n",
    "                if (contains_valid_alphanum(ngram) or contains_priority_keyword(ngram)) and ngram not in seen\n",
    "            ]\n",
    "            combined = top_ngrams + alphanum_priority_ngrams\n",
    "            for ngram, count in combined:\n",
    "                f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "top_ngrams_priority_path = os.path.join(output_dir, \"top_1500_ngram-2_priority.txt\")\n",
    "save_top_clean_ngrams(ngram_data, filename=top_ngrams_priority_path, top_n=1500)\n",
    "\n",
    "print(\"✅ Cleaned and Top Priority n-gram files saved!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49686d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PyPDF2 import PdfReader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "ngram_txt_path = \"25-3-2025_14_29_3_5/top_1500_ngram_2.txt\"\n",
    "paper_folder = \"Cancer2/\"\n",
    "output_file = \"25-3-2025_14_29_3_5/tfidf_ngrams_papers.csv\"\n",
    "\n",
    "# Read the ngrams from the text file\n",
    "with open(ngram_txt_path, 'r') as file:\n",
    "    ngram_lines = file.readlines()\n",
    "\n",
    "# Extract unigrams (ignoring the frequency part)\n",
    "cleaned_ngrams = [line.split(\":\")[0].strip() for line in ngram_lines if \":\" in line]\n",
    "\n",
    "# If the cleaned_ngrams is empty, use the raw ngram lines\n",
    "if not cleaned_ngrams:\n",
    "    cleaned_ngrams = [line.strip() for line in ngram_lines]\n",
    "\n",
    "# Function to extract text from a single PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"Processing {os.path.basename(pdf_path)}...\")  # Print which paper is being processed\n",
    "    reader = PdfReader(pdf_path)\n",
    "    return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "# Get list of PDF files in the folder\n",
    "pdf_files = [file for file in os.listdir(paper_folder) if file.lower().endswith(\".pdf\")]\n",
    "pdf_paths = [os.path.join(paper_folder, file) for file in pdf_files]\n",
    "\n",
    "# Use ThreadPoolExecutor to extract text in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    documents = list(executor.map(extract_text_from_pdf, pdf_paths))\n",
    "\n",
    "# Vectorize the text data using the cleaned unigrams\n",
    "vectorizer = TfidfVectorizer(vocabulary=cleaned_ngrams, ngram_range=(1, 1), lowercase=True)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Create DataFrame for easy visualization and saving\n",
    "df = pd.DataFrame(tfidf_matrix.T.toarray(), index=cleaned_ngrams, columns=pdf_files)\n",
    "\n",
    "# Save the TF-IDF matrix to a CSV file\n",
    "df.to_csv(output_file)\n",
    "\n",
    "print(f\"TF-IDF matrix saved at: {output_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PyPDF2 import PdfReader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# === Paths ===\n",
    "ngram_txt_path = \"25-3-2025_14_29_3_5/top_1500_ngram_2.txt\"\n",
    "paper_folder = \"Cancer2/\"\n",
    "output_file = \"25-3-2025_14_29_3_5/tfidf_ngrams_papers.csv\"\n",
    "\n",
    "# === Read and clean n-grams from the text file ===\n",
    "with open(ngram_txt_path, 'r', encoding='utf-8') as file:\n",
    "    ngram_lines = file.readlines()\n",
    "\n",
    "# Extract n-gram phrases (remove counts and unwanted lines)\n",
    "cleaned_ngrams = [line.split(\":\")[0].strip() for line in ngram_lines if \":\" in line and line.strip()]\n",
    "\n",
    "if not cleaned_ngrams:\n",
    "    cleaned_ngrams = [line.strip() for line in ngram_lines if line.strip()]\n",
    "\n",
    "# === Function to extract text from a single PDF ===\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"Processing {os.path.basename(pdf_path)}...\")\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# === Get all PDF paths ===\n",
    "pdf_files = [file for file in os.listdir(paper_folder) if file.lower().endswith(\".pdf\")]\n",
    "pdf_paths = [os.path.join(paper_folder, file) for file in pdf_files]\n",
    "\n",
    "# === Extract PDF text using threads for speed ===\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    documents = list(executor.map(extract_text_from_pdf, pdf_paths))\n",
    "\n",
    "# === TF-IDF Vectorization using 1 to 4-gram matching ===\n",
    "vectorizer = TfidfVectorizer(vocabulary=cleaned_ngrams, ngram_range=(1, 4), lowercase=True)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# === Create and save DataFrame ===\n",
    "df = pd.DataFrame(tfidf_matrix.T.toarray(), index=cleaned_ngrams, columns=pdf_files)\n",
    "df.to_csv(output_file)\n",
    "\n",
    "print(f\"\\n✅ TF-IDF matrix saved at: {output_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3028cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# === Paths ===\n",
    "paper_folder = \"Cancer2/\"  # Folder containing all the PDF papers\n",
    "ngram_txt_path = \"25-3-2025_14_29_3/top_1500_ngram.txt\"  # Your n-grams file\n",
    "\n",
    "# === Function to extract text from a single PDF ===\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"Processing {os.path.basename(pdf_path)}...\")\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# === Get all PDF paths ===\n",
    "pdf_files = [file for file in os.listdir(paper_folder) if file.lower().endswith(\".pdf\")]\n",
    "pdf_paths = [os.path.join(paper_folder, file) for file in pdf_files]\n",
    "\n",
    "# === Extract text from all PDFs ===\n",
    "documents = [extract_text_from_pdf(pdf_path) for pdf_path in pdf_paths]\n",
    "\n",
    "# === Read and clean n-grams from the text file ===\n",
    "with open(ngram_txt_path, 'r', encoding='utf-8') as file:\n",
    "    ngram_lines = file.readlines()\n",
    "\n",
    "# Extract n-gram phrases (remove counts and unwanted lines)\n",
    "cleaned_ngrams = [line.split(\":\")[0].strip() for line in ngram_lines if \":\" in line and line.strip()]\n",
    "\n",
    "if not cleaned_ngrams:\n",
    "    cleaned_ngrams = [line.strip() for line in ngram_lines if line.strip()]\n",
    "\n",
    "# === Create CountVectorizer to count the occurrences of n-grams in the documents ===\n",
    "count_vectorizer = CountVectorizer(vocabulary=cleaned_ngrams, ngram_range=(1, 4), lowercase=True)\n",
    "\n",
    "# Transform the documents into the count matrix (n-gram x paper)\n",
    "count_matrix = count_vectorizer.transform(documents)\n",
    "\n",
    "# === Create a DataFrame for the n-gram x paper matrix ===\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), index=cleaned_ngrams, columns=pdf_files)\n",
    "\n",
    "# === Save the n-gram x paper matrix to CSV ===\n",
    "output_file = \"25-3-2025_14_29_3/ngram_count_per_paper.csv\"\n",
    "count_df.to_csv(output_file)\n",
    "\n",
    "print(f\"\\n✅ N-gram count per paper saved at: {output_file}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f63c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PyPDF2 import PdfReader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "ngram_txt_path = \"25-3-2025_14_29_3_5/top_1500_ngram_2.txt\"\n",
    "paper_folder = \"Cancer2/\"\n",
    "output_file = \"25-3-2025_14_29_3_5/tfidf_ngrams_papers.csv\"\n",
    "\n",
    "with open(ngram_txt_path, 'r') as file:\n",
    "    ngram_lines = file.readlines()\n",
    "\n",
    "cleaned_ngrams = [line.split(\":\")[0].strip() for line in ngram_lines if \":\" in line]\n",
    "if not cleaned_ngrams:\n",
    "    cleaned_ngrams = [line.strip() for line in ngram_lines]\n",
    "\n",
    "alphanum_ngrams = [term for term in cleaned_ngrams if any(c.isdigit() for c in term)]\n",
    "print(\"Alphanumerical n-grams found in vocabulary:\", alphanum_ngrams)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"Processing {os.path.basename(pdf_path)}...\")\n",
    "    reader = PdfReader(pdf_path)\n",
    "    return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "pdf_files = [file for file in os.listdir(paper_folder) if file.lower().endswith(\".pdf\")]\n",
    "pdf_paths = [os.path.join(paper_folder, file) for file in pdf_files]\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    documents = list(executor.map(extract_text_from_pdf, pdf_paths))\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=cleaned_ngrams, ngram_range=(1, 1), lowercase=False)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "df = pd.DataFrame(tfidf_matrix.T.toarray(), index=cleaned_ngrams, columns=pdf_files)\n",
    "df.to_csv(output_file)\n",
    "\n",
    "print(f\"TF-IDF matrix saved at: {output_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ab9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "paper_folder = \"Cancer2/\"\n",
    "ngram_txt_path = \"25-3-2025_14_29_3_5/filtered_ngrams_with_frequency.txt\"\n",
    "output_file = \"25-3-2025_14_29_3/ngram_count_per_paper.csv\"\n",
    "transposed_output_file = \"25-3-2025_14_29_3_5/ngram_count_transposed.csv\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"Processing {os.path.basename(pdf_path)}...\")\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "pdf_files = [file for file in os.listdir(paper_folder) if file.lower().endswith(\".pdf\")]\n",
    "pdf_paths = [os.path.join(paper_folder, file) for file in pdf_files]\n",
    "documents = [extract_text_from_pdf(path) for path in pdf_paths]\n",
    "\n",
    "with open(ngram_txt_path, 'r', encoding='utf-8') as file:\n",
    "    ngram_lines = file.readlines()\n",
    "\n",
    "raw_ngrams = [line.split(\":\")[0].strip() for line in ngram_lines if \":\" in line and line.strip()]\n",
    "if not raw_ngrams:\n",
    "    raw_ngrams = [line.strip() for line in ngram_lines if line.strip()]\n",
    "\n",
    "def is_alphanumeric_ngram(ngram):\n",
    "    return all(re.fullmatch(r\"[a-zA-Z0-9]+\", token) for token in ngram.split())\n",
    "\n",
    "cleaned_ngrams = [ng for ng in raw_ngrams if is_alphanumeric_ngram(ng)]\n",
    "\n",
    "count_vectorizer = CountVectorizer(vocabulary=cleaned_ngrams, ngram_range=(1, 4), lowercase=True)\n",
    "count_matrix = count_vectorizer.transform(documents)\n",
    "\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), index=pdf_files, columns=cleaned_ngrams)\n",
    "\n",
    "count_df.to_csv(output_file)\n",
    "print(f\"\\n✅ N-gram count per paper saved at: {output_file}\")\n",
    "\n",
    "count_df.transpose().to_csv(transposed_output_file)\n",
    "print(f\"📄 Transposed matrix saved at: {transposed_output_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vectorizer = CountVectorizer(vocabulary=cleaned_ngrams, ngram_range=(1, 4), lowercase=True)\n",
    "count_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "count_df = pd.DataFrame(count_matrix.toarray().T, index=cleaned_ngrams, columns=pdf_files)\n",
    "\n",
    "output_file = \"25-3-2025_14_29_3_5/ngram_count_per_paper.csv\"\n",
    "count_df.to_csv(output_file)\n",
    "\n",
    "print(f\"\\n✅ N-gram count matrix saved at: {output_file}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc73a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def build_sparse_cooccurrence_csv(input_folder, ngram_file, output_file):\n",
    "    # Read the n-grams and their frequencies from the ngram_file\n",
    "    ngrams_dict = {}\n",
    "    with open(ngram_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and \": \" in line:\n",
    "                ngram, count = line.split(\": \")\n",
    "                ngrams_dict[ngram] = int(count)\n",
    "\n",
    "    # List all PDF files in the input folder\n",
    "    paper_files = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "    cooccurrence_data = []\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Process each paper (PDF file) in the input folder\n",
    "    for file_name in paper_files:\n",
    "        pdf_path = os.path.join(input_folder, file_name)\n",
    "        raw_text = extract_text(pdf_path)\n",
    "\n",
    "        if not raw_text.strip():\n",
    "            print(f\"Skipped (no extractable text): {file_name}\")\n",
    "            continue\n",
    "\n",
    "        # Tokenize the text and remove stopwords\n",
    "        tokens = [word for word in word_tokenize(raw_text.lower()) if word.isalpha() and word not in stop_words]\n",
    "        \n",
    "        # Count n-grams in the document\n",
    "        paper_ngrams = Counter()\n",
    "        for ngram in ngrams_dict:\n",
    "            ngram_tokens = ngram.split()\n",
    "            # Check how many times this n-gram appears in the paper\n",
    "            for i in range(len(tokens) - len(ngram_tokens) + 1):\n",
    "                if tokens[i:i+len(ngram_tokens)] == ngram_tokens:\n",
    "                    paper_ngrams[ngram] += 1\n",
    "\n",
    "        # Add co-occurrence data for each found n-gram\n",
    "        for ngram, count in paper_ngrams.items():\n",
    "            if count > 0:\n",
    "                cooccurrence_data.append((ngram, file_name, count))\n",
    "\n",
    "        print(f\"Processed: {file_name}\")\n",
    "\n",
    "    # Save the co-occurrence matrix to CSV\n",
    "    df_sparse = pd.DataFrame(cooccurrence_data, columns=[\"Ngram\", \"Paper\", \"Count\"])\n",
    "    df_sparse.to_csv(output_file, index=False)\n",
    "    print(f\"Co-occurrence matrix saved to: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "# Specify the input folder containing PDF files, ngram file, and output file\n",
    "input_folder = \"Cancer2/\"\n",
    "ngram_file = \"25-3-2025_14_29_3_5/top_1500_ngram.txt\"\n",
    "output_file = \"25-3-2025_14_29_3_5/fourgram_cooccurrence.csv\"\n",
    "\n",
    "# Run the function\n",
    "build_sparse_cooccurrence_csv(input_folder, ngram_file, output_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the existing ngram count per paper matrix\n",
    "count_df = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_count_per_paper.csv\", index_col=0)\n",
    "\n",
    "# Convert to binary presence (1 if appears in paper, 0 otherwise)\n",
    "binary_df = count_df.gt(0).astype(int)\n",
    "\n",
    "# Get total number of papers in which each n-gram appears (row-wise sum)\n",
    "ngram_doc_freq = binary_df.sum(axis=1)\n",
    "\n",
    "# Broadcast to get Jaccard-like matrix: each value = 1 / number of papers where ngram appears (if it appears in that paper)\n",
    "jaccard_matrix = binary_df.div(ngram_doc_freq, axis=0)\n",
    "\n",
    "# Save the Jaccard matrix\n",
    "output_path = \"25-3-2025_14_29_3/ngram_jaccard_similarity_ngram_x_paper.csv\"\n",
    "jaccard_matrix.to_csv(output_path)\n",
    "\n",
    "print(f\"✅ Jaccard similarity matrix (ngram x paper) saved at: {output_path}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdda259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ngram x paper count matrix (if not already loaded)\n",
    "count_df = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_count_per_paper.csv\", index_col=0)\n",
    "\n",
    "# Compute total frequency of each ngram across all papers\n",
    "ngram_total_freq = count_df.sum(axis=1)\n",
    "\n",
    "# Save as CSV\n",
    "ngram_total_freq.to_csv(\"25-3-2025_14_29_3_5/ngram_total_frequency.csv\", header=[\"total_frequency\"])\n",
    "\n",
    "print(\"✅ Total frequency per n-gram saved at: 25-3-2025_14_29_3_5/ngram_total_frequency.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe55bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_total_frequency.csv\")\n",
    "\n",
    "# Convert the total_frequency column to numeric, forcing errors to NaN for non-numeric rows\n",
    "df['total_frequency'] = pd.to_numeric(df['total_frequency'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in total_frequency\n",
    "df = df.dropna(subset=['total_frequency'])\n",
    "\n",
    "# Filter rows where total_frequency > 15\n",
    "filtered_df = df[df['total_frequency'] > 15]\n",
    "\n",
    "# Save to a new CSV file\n",
    "filtered_df.to_csv(\"25-3-2025_14_29_3_5/ngrams_above_15.csv\", index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdad33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# === Paths ===\n",
    "paper_folder = \"Cancer2/\"\n",
    "ngram_txt_path = \"25-3-2025_14_29_3_5/ngrams_above_15.csv\"\n",
    "output_file = \"25-3-2025_14_29_3_5/ngram_count_per_paper_hjjnjknmmkmlxyuy.csv\"\n",
    "\n",
    "# === Function to extract text from a single PDF ===\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"Processing {os.path.basename(pdf_path)}...\")\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# === Load PDF texts ===\n",
    "pdf_files = [file for file in os.listdir(paper_folder) if file.lower().endswith(\".pdf\")]\n",
    "pdf_paths = [os.path.join(paper_folder, file) for file in pdf_files]\n",
    "documents = [extract_text_from_pdf(path) for path in pdf_paths]\n",
    "\n",
    "# === Load and clean n-grams ===\n",
    "with open(ngram_txt_path, 'r', encoding='utf-8') as f:\n",
    "    ngram_lines = f.readlines()\n",
    "cleaned_ngrams = [line.split(\":\")[0].strip() for line in ngram_lines if \":\" in line and line.strip()]\n",
    "if not cleaned_ngrams:\n",
    "    cleaned_ngrams = [line.strip() for line in ngram_lines if line.strip()]\n",
    "\n",
    "# === Count n-gram occurrences ===\n",
    "vectorizer = CountVectorizer(vocabulary=cleaned_ngrams, ngram_range=(1, 4), lowercase=True)\n",
    "matrix = vectorizer.transform(documents)  # shape: [num_docs x num_ngrams]\n",
    "\n",
    "# === Convert to DataFrame: rows=ngrams, columns=pdfs ===\n",
    "count_df = pd.DataFrame(matrix.toarray().T, index=cleaned_ngrams, columns=pdf_files)\n",
    "\n",
    "# === Save ===\n",
    "count_df.to_csv(output_file)\n",
    "print(f\"\\n✅ N-gram count matrix saved at: {output_file}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3131712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('25-3-2025_14_29_3/ngram_jaccard_similarity_ngram_x_paper.csv')\n",
    "\n",
    "# Transpose the DataFrame\n",
    "df_transposed = df.T\n",
    "\n",
    "# Save the transposed DataFrame to a new CSV\n",
    "df_transposed.to_csv('25-3-2025_14_29_3/transposed_file.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Paths ===\n",
    "jaccard_csv_path = \"25-3-2025_14_29_3/ngram_jaccard_similarity_ngram_x_paper.csv\"\n",
    "output_path = \"25-3-2025_14_29_3_5/ngram_combined_jaccard.csv\"\n",
    "\n",
    "# === Load Jaccard similarity matrix (n-gram × paper) ===\n",
    "jaccard_df = pd.read_csv(jaccard_csv_path, index_col=0)\n",
    "\n",
    "# === Load list of filtered n-grams ===\n",
    "with open(filtered_ngrams_txt_path, 'r', encoding='utf-8') as f:\n",
    "    ngrams = [line.strip().split(\":\")[0] for line in f if line.strip()]\n",
    "\n",
    "# === Clean index and match only those in both files ===\n",
    "jaccard_df.index = jaccard_df.index.str.strip()\n",
    "ngrams_set = set(ngrams)\n",
    "valid_ngrams = jaccard_df.index.intersection(ngrams_set)\n",
    "filtered_jaccard_df = jaccard_df.loc[valid_ngrams]\n",
    "\n",
    "# === Compute average Jaccard per n-gram across all papers ===\n",
    "filtered_jaccard_df[\"Combined_Jaccard\"] = filtered_jaccard_df.mean(axis=1)\n",
    "\n",
    "# === Save only the combined column ===\n",
    "filtered_jaccard_df[[\"Combined_Jaccard\"]].to_csv(output_path)\n",
    "print(f\"✅ Combined Jaccard scores saved at: {output_path}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c42d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths ===\n",
    "ngram_txt_path = \"25-3-2025_14_29_3_5/cleaned_ngrams.csv\"\n",
    "output_file = \"25-3-2025_14_29_3_5/ngram_jaccard_similarity_2.csv\"\n",
    "\n",
    "# === Load and clean n-grams ===\n",
    "with open(ngram_txt_path, 'r', encoding='utf-8') as f:\n",
    "    ngram_lines = f.readlines()\n",
    "\n",
    "# Extract n-grams from the file\n",
    "raw_ngrams = [line.split(\":\")[0].strip() for line in ngram_lines if \":\" in line and line.strip()]\n",
    "if not raw_ngrams:\n",
    "    raw_ngrams = [line.strip() for line in ngram_lines if line.strip()]\n",
    "\n",
    "# === Filter only alphanumeric ngrams ===\n",
    "def is_alphanumeric_ngram(ngram):\n",
    "    return all(re.fullmatch(r\"[a-zA-Z0-9]+\", token) for token in ngram.split())\n",
    "\n",
    "cleaned_ngrams = [ng for ng in raw_ngrams if is_alphanumeric_ngram(ng)]\n",
    "\n",
    "# === Jaccard Similarity Function ===\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0.0\n",
    "\n",
    "# === Calculate Jaccard similarity matrix ===\n",
    "ngram_sets = [set(ngram.split()) for ngram in cleaned_ngrams]  # Split each n-gram into a set of tokens\n",
    "jaccard_matrix = []\n",
    "\n",
    "for i, set1 in enumerate(ngram_sets):\n",
    "    row = []\n",
    "    for j, set2 in enumerate(ngram_sets):\n",
    "        similarity = jaccard_similarity(set1, set2)\n",
    "        row.append(similarity)\n",
    "    jaccard_matrix.append(row)\n",
    "\n",
    "# === Create DataFrame for Jaccard similarities ===\n",
    "# The index and columns are both the n-grams themselves\n",
    "jaccard_df = pd.DataFrame(jaccard_matrix, index=cleaned_ngrams, columns=cleaned_ngrams)\n",
    "\n",
    "# === Save the Jaccard similarity matrix ===\n",
    "jaccard_df.to_csv(output_file)\n",
    "print(f\"\\n✅ Jaccard similarity matrix saved at: {output_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths ===\n",
    "ngram_csv_path = \"25-3-2025_14_29_3_5/cleaned_ngrams.csv\"\n",
    "output_file = \"25-3-2025_14_29_3_5/ngram_jaccard_similarity_2.csv\"\n",
    "\n",
    "# === Load ngrams from CSV ===\n",
    "df = pd.read_csv(ngram_csv_path)\n",
    "\n",
    "# Rename if needed (in case it's still 'Unnamed: 0')\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df.rename(columns={'Unnamed: 0': 'ngram'}, inplace=True)\n",
    "\n",
    "# Ensure 'ngram' column exists\n",
    "if 'ngram' not in df.columns:\n",
    "    raise KeyError(\"❌ Column 'ngram' not found in the CSV!\")\n",
    "\n",
    "# === Filter only alphanumeric ngrams ===\n",
    "def is_alphanumeric_ngram(ngram):\n",
    "    return all(re.fullmatch(r\"[a-zA-Z0-9]+\", token) for token in ngram.split())\n",
    "\n",
    "cleaned_ngrams = df['ngram'].dropna().unique().tolist()\n",
    "cleaned_ngrams = [ng for ng in cleaned_ngrams if is_alphanumeric_ngram(ng)]\n",
    "\n",
    "# === Jaccard Similarity Function ===\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0.0\n",
    "\n",
    "# === Compute Jaccard similarity matrix ===\n",
    "ngram_sets = [set(ng.split()) for ng in cleaned_ngrams]\n",
    "jaccard_matrix = [\n",
    "    [jaccard_similarity(set1, set2) for set2 in ngram_sets]\n",
    "    for set1 in ngram_sets\n",
    "]\n",
    "\n",
    "# === Save to CSV ===\n",
    "jaccard_df = pd.DataFrame(jaccard_matrix, index=cleaned_ngrams, columns=cleaned_ngrams)\n",
    "jaccard_df.to_csv(output_file)\n",
    "\n",
    "print(f\"✅ Jaccard similarity matrix saved at: {output_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from PyPDF2 import PdfReader\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# === Paths ===\n",
    "paper_folder = \"Cancer2/\"\n",
    "ngram_txt_path = \"25-3-2025_14_29_3_5/top_1500_ngram_2.txt\"\n",
    "output_file = \"25-3-2025_14_29_3/ngram_jaccard_similarity_ngram_x_paper.csv\"\n",
    "threshold = 15\n",
    "\n",
    "# === Initialize spaCy model ===\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === Function to extract text from a single PDF ===\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(f\"Processing {os.path.basename(pdf_path)}...\")\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        return \" \".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# === Load PDF texts ===\n",
    "pdf_files = [file for file in os.listdir(paper_folder) if file.lower().endswith(\".pdf\")]\n",
    "pdf_paths = [os.path.join(paper_folder, file) for file in pdf_files]\n",
    "documents = [extract_text_from_pdf(path) for path in pdf_paths]\n",
    "\n",
    "# === Load and clean n-grams ===\n",
    "with open(ngram_txt_path, 'r', encoding='utf-8') as f:\n",
    "    ngram_lines = f.readlines()\n",
    "raw_ngrams = [line.split(\":\")[0].strip() for line in ngram_lines if \":\" in line and line.strip()]\n",
    "if not raw_ngrams:\n",
    "    raw_ngrams = [line.strip() for line in ngram_lines if line.strip()]\n",
    "\n",
    "# === Filter only alphanumeric ngrams ===\n",
    "def is_alphanumeric_ngram(ngram):\n",
    "    return all(re.fullmatch(r\"[a-zA-Z0-9]+\", token) for token in ngram.split())\n",
    "\n",
    "cleaned_ngrams = [ng for ng in raw_ngrams if is_alphanumeric_ngram(ng)]\n",
    "\n",
    "# === Count n-gram occurrences ===\n",
    "count_vectorizer = CountVectorizer(vocabulary=cleaned_ngrams, ngram_range=(1, 4), lowercase=True)\n",
    "count_matrix = count_vectorizer.transform(documents)\n",
    "\n",
    "# === Create DataFrame for n-gram counts ===\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), index=pdf_files, columns=cleaned_ngrams)\n",
    "\n",
    "# === Sum the total frequency of each n-gram across all papers ===\n",
    "ngram_total_freq = count_df.sum(axis=0)\n",
    "\n",
    "# === Filter n-grams above the threshold ===\n",
    "filtered_ngrams = ngram_total_freq[ngram_total_freq >= threshold].index.tolist()\n",
    "\n",
    "# === Rebuild CountVectorizer and transform documents with filtered n-grams ===\n",
    "filtered_vectorizer = CountVectorizer(vocabulary=filtered_ngrams, ngram_range=(1, 4), lowercase=True)\n",
    "filtered_matrix = filtered_vectorizer.transform(documents)\n",
    "\n",
    "# === Create filtered DataFrame ===\n",
    "filtered_count_df = pd.DataFrame(filtered_matrix.toarray(), index=pdf_files, columns=filtered_ngrams)\n",
    "\n",
    "# === Save the filtered n-gram count matrix ===\n",
    "filtered_count_df.to_csv(output_file)\n",
    "print(f\"\\n✅ N-gram count per paper (filtered) saved at: {output_file}\")\n",
    "\n",
    "# === Dependency Parsing and Relationship Extraction ===\n",
    "\n",
    "# Define patterns for medical relations (can be customized based on domain-specific needs)   \n",
    "def extract_medical_relations(doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # Define patterns for medical relationships (e.g., symptom-disease, drug-treatment, etc.)\n",
    "    patterns = [\n",
    "        {\"label\": \"SYMPTOM_DISEASE\", \"pattern\": [{\"dep\": \"nsubj\"}, {\"dep\": \"ROOT\"}, {\"dep\": \"dobj\"}]},  # symptom -> disease\n",
    "        {\"label\": \"DRUG_TREATMENT\", \"pattern\": [{\"dep\": \"nsubj\"}, {\"dep\": \"ROOT\"}, {\"dep\": \"dobj\"}]},  # drug -> treatment\n",
    "    ]\n",
    "    \n",
    "    # Add patterns to the matcher\n",
    "    for pattern in patterns:\n",
    "        matcher.add(pattern[\"label\"], [pattern[\"pattern\"]])\n",
    "\n",
    "    # Apply matcher to the document\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # Extract relationships from the matches\n",
    "    relations = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # Extract the span of the matched pattern\n",
    "        label = nlp.vocab.strings[match_id]\n",
    "        relations.append((label, span.text))\n",
    "\n",
    "    return relations\n",
    "\n",
    "# Extract medical relationships from each document\n",
    "medical_relations = []\n",
    "for doc_text in documents:\n",
    "    doc = nlp(doc_text)\n",
    "    relations = extract_medical_relations(doc)\n",
    "    medical_relations.append(relations)\n",
    "\n",
    "# === Output Medical Relations ===\n",
    "medical_relations_df = pd.DataFrame(medical_relations, columns=[\"Relations\"])\n",
    "medical_relations_df.to_csv(\"25-3-2025_14_29_3/medical_relations.csv\", index=False)\n",
    "print(\"✅ Medical relations saved at: 25-3-2025_14_29_3/medical_relations.csv\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ngram CSV file (replace with your actual file path)\n",
    "ngram_df = pd.read_csv(\"25-3-2025_14_29_3_5/ngrams_above_15.csv\")  # Replace with your file path\n",
    "\n",
    "# Check the column names\n",
    "print(ngram_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b84b107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'total_frequency'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ngram CSV file (replace with your actual file path)\n",
    "ngram_df = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_total_frequency.csv\")  # Replace with your file path\n",
    "\n",
    "# Check the column names\n",
    "print(ngram_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dbc0291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered ngrams saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the priority cancer keywords\n",
    "priority_keywords = set([\n",
    "    \"cancer\", \"tumor\", \"tumour\", \"metastasis\", \"oncology\", \"chemotherapy\", \"radiation\", \"immunotherapy\", \"biopsy\",\n",
    "    \"mutation\", \"gene\", \"cell\", \"leukemia\", \"lymphoma\", \"sarcoma\", \"melanoma\",\n",
    "    \"therapy\", \"drug\", \"diagnosis\", \"survival\", \"treatment\", \"carcinoma\", \"neoplasm\", \"malignancy\", \"genetic\", \"genomics\"\n",
    "])\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_total_frequency.csv\")\n",
    "\n",
    "# Rename the 'Unnamed: 0' column to 'ngram'\n",
    "df = df.rename(columns={'Unnamed: 0': 'ngram'})\n",
    "\n",
    "# Convert the total_frequency column to numeric\n",
    "df['total_frequency'] = pd.to_numeric(df['total_frequency'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in total_frequency\n",
    "df = df.dropna(subset=['total_frequency'])\n",
    "\n",
    "# Define function to check for priority keywords\n",
    "def contains_priority_keyword(ngram):\n",
    "    return any(word.lower() in priority_keywords for word in ngram.split())\n",
    "\n",
    "# Correct filtering:\n",
    "filtered_df = df[\n",
    "    (df['total_frequency'] > 15) | \n",
    "    ((df['total_frequency'] <= 15) & (df['ngram'].apply(contains_priority_keyword)))\n",
    "]\n",
    "\n",
    "# Save to a new CSV file\n",
    "filtered_df.to_csv(\"25-3-2025_14_29_3_5/ngrams_above_15_with_priority.csv\", index=False)\n",
    "\n",
    "print(\"✅ Filtered ngrams saved successfully.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de9899ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Duplicates removed and cleaned data saved as 'cleaned_ngrams.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "ngram_df = pd.read_csv(\"25-3-2025_14_29_3_5/ngrams_above_15.csv\")\n",
    "\n",
    "# Rename the 'Unnamed: 0' column to 'ngram' for clarity\n",
    "ngram_df.rename(columns={\"Unnamed: 0\": \"ngram\"}, inplace=True)\n",
    "\n",
    "# Function to normalize ngrams by sorting their tokens\n",
    "def normalize_ngram(text):\n",
    "    tokens = text.lower().split()\n",
    "    sorted_tokens = sorted(tokens)\n",
    "    return ' '.join(sorted_tokens)\n",
    "\n",
    "# Apply normalization\n",
    "ngram_df[\"normalized_ngram\"] = ngram_df[\"ngram\"].apply(normalize_ngram)\n",
    "\n",
    "# Drop duplicates based on normalized ngram\n",
    "ngram_df = ngram_df.drop_duplicates(subset=[\"normalized_ngram\"])\n",
    "\n",
    "# Drop the helper column if you want\n",
    "ngram_df = ngram_df.drop(columns=[\"normalized_ngram\"])\n",
    "\n",
    "# Save the cleaned output\n",
    "ngram_df.to_csv(\"25-3-2025_14_29_3_5/cleaned_ngrams.csv\", index=False)\n",
    "\n",
    "print(\"✅ Duplicates removed and cleaned data saved as 'cleaned_ngrams.csv'\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab5bd037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Duplicates removed and cleaned data saved as 'cleaned_ngrams_priority.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "ngram_df = pd.read_csv(\"25-3-2025_14_29_3_5/ngrams_above_15_with_priority.csv\")\n",
    "\n",
    "# Rename the 'Unnamed: 0' column to 'ngram' for clarity\n",
    "ngram_df.rename(columns={\"Unnamed: 0\": \"ngram\"}, inplace=True)\n",
    "\n",
    "# Function to normalize ngrams by sorting their tokens\n",
    "def normalize_ngram(text):\n",
    "    tokens = text.lower().split()\n",
    "    sorted_tokens = sorted(tokens)\n",
    "    return ' '.join(sorted_tokens)\n",
    "\n",
    "# Apply normalization\n",
    "ngram_df[\"normalized_ngram\"] = ngram_df[\"ngram\"].apply(normalize_ngram)\n",
    "\n",
    "# Drop duplicates based on normalized ngram\n",
    "ngram_df = ngram_df.drop_duplicates(subset=[\"normalized_ngram\"])\n",
    "\n",
    "# Drop the helper column if you want\n",
    "ngram_df = ngram_df.drop(columns=[\"normalized_ngram\"])\n",
    "\n",
    "# Save the cleaned output\n",
    "ngram_df.to_csv(\"25-3-2025_14_29_3_5/cleaned_ngrams_priority.csv\", index=False)\n",
    "\n",
    "print(\"✅ Duplicates removed and cleaned data saved as 'cleaned_ngrams_priority.csv'\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "ngrams_freq = pd.read_csv(\"25-3-2025_14_29_3_5/ngrams_above_15.csv\")\n",
    "ngram_paper = pd.read_csv(\"25-3-2025_14_29_3_5/output_with_entities.csv\", index_col=0)\n",
    "ngram_jaccard = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_jaccard_similarity.csv\", index_col=0)\n",
    "\n",
    "# Rename columns for consistency\n",
    "ngrams_freq.columns = [\"ngram\", \"frequency\"]\n",
    "ngram_frequencies = ngrams_freq.set_index(\"ngram\")[\"frequency\"]\n",
    "\n",
    "# Filter 1: Ngram-Ngram Jaccard > 0.33\n",
    "jaccard_threshold = 0.33\n",
    "ngram_edges = ngram_jaccard.where(ngram_jaccard > jaccard_threshold)\n",
    "ngram_edges = ngram_edges.stack().reset_index()\n",
    "ngram_edges.columns = [\"source_ngram\", \"target_ngram\", \"similarity\"]\n",
    "ngram_edges[\"source_freq\"] = ngram_edges[\"source_ngram\"].map(ngram_frequencies)\n",
    "ngram_edges[\"target_freq\"] = ngram_edges[\"target_ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# Save ngram-ngram dependencies\n",
    "ngram_edges.to_csv(\"25-3-2025_14_29_3_5/medical_ngram_dependencies.csv\", index=False)\n",
    "\n",
    "# Ensure ngram_paper contains numeric values for relevance scores\n",
    "ngram_paper = ngram_paper.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Filter 2: Ngram-Paper relevance > 0\n",
    "ngram_paper_long = ngram_paper[ngram_paper > 0].stack().reset_index()\n",
    "ngram_paper_long.columns = [\"ngram\", \"paper\", \"score\"]\n",
    "ngram_paper_long[\"ngram_freq\"] = ngram_paper_long[\"ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# Save ngram-paper links\n",
    "ngram_paper_long.to_csv(\"25-3-2025_14_29_3_5/ngram_paper_links.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files saved: medical_ngram_dependencies.csv & ngram_paper_links.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Load CSV files ===\n",
    "# Load cleaned n-grams (with placeholder frequency if not present)\n",
    "ngrams_freq = pd.read_csv(\"25-3-2025_14_29_3_5/cleaned_ngrams.csv\", names=[\"ngram\", \"frequency\"])\n",
    "ngram_paper = pd.read_csv(\"25-3-2025_14_29_3_5/output_with_entities.csv\", index_col=0)\n",
    "ngram_jaccard = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_jaccard_similarity_2.csv\", index_col=0)\n",
    "\n",
    "# === Frequency mapping ===\n",
    "ngram_frequencies = ngrams_freq.set_index(\"ngram\")[\"frequency\"]\n",
    "\n",
    "# === Filter 1: Ngram-Ngram Jaccard > threshold ===\n",
    "jaccard_threshold = 0.33\n",
    "ngram_edges = ngram_jaccard.where(ngram_jaccard > jaccard_threshold)\n",
    "ngram_edges = ngram_edges.stack().reset_index()\n",
    "ngram_edges.columns = [\"source_ngram\", \"target_ngram\", \"similarity\"]\n",
    "ngram_edges[\"source_freq\"] = ngram_edges[\"source_ngram\"].map(ngram_frequencies)\n",
    "ngram_edges[\"target_freq\"] = ngram_edges[\"target_ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-ngram relationships ===\n",
    "ngram_edges.to_csv(\"25-3-2025_14_29_3_5/medical_ngram_dependencies.csv\", index=False)\n",
    "\n",
    "# === Filter 2: Ngram-Paper Relevance > 0 ===\n",
    "ngram_paper = ngram_paper.apply(pd.to_numeric, errors='coerce')\n",
    "ngram_paper_long = ngram_paper[ngram_paper > 0].stack().reset_index()\n",
    "ngram_paper_long.columns = [\"ngram\", \"paper\", \"score\"]\n",
    "ngram_paper_long[\"ngram_freq\"] = ngram_paper_long[\"ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-paper links ===\n",
    "ngram_paper_long.to_csv(\"25-3-2025_14_29_3_5/ngram_paper_links.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files saved: medical_ngram_dependencies.csv & ngram_paper_links.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68bbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from scispacy.linking import EntityLinker\n",
    "import scispacy\n",
    "\n",
    "# === Load SciSpacy's Medical model (sm version) ===\n",
    "nlp = spacy.load(\"en_core_sci_sm\")  # Using the small version of the model\n",
    "linker = EntityLinker.from_pretrained(\"scispacy-linker-md\")\n",
    "\n",
    "# === Load CSV files ===\n",
    "# Load cleaned n-grams (with placeholder frequency if not present)\n",
    "ngrams_freq = pd.read_csv(\"25-3-2025_14_29_3_5/cleaned_ngrams.csv\", names=[\"ngram\", \"frequency\"])\n",
    "ngram_paper = pd.read_csv(\"25-3-2025_14_29_3_5/output_with_entities.csv\", index_col=0)\n",
    "ngram_jaccard = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_jaccard_similarity_2.csv\", index_col=0)\n",
    "\n",
    "# === Frequency mapping ===\n",
    "ngram_frequencies = ngrams_freq.set_index(\"ngram\")[\"frequency\"]\n",
    "\n",
    "# === Function to perform medical dependency parsing using SciSpacy ===\n",
    "def get_medical_dependencies(ngram):\n",
    "    # Process the ngram with SciSpacy to extract dependencies\n",
    "    doc = nlp(ngram)\n",
    "    doc._.linker = linker  # Apply the entity linker for medical entity recognition\n",
    "    dependencies = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Store each token's word, its dependency relation, and the head word\n",
    "        dependencies.append((token.text, token.dep_, token.head.text))\n",
    "    \n",
    "    return dependencies\n",
    "\n",
    "# === Apply dependency parsing to all n-grams ===\n",
    "ngrams_freq['dependencies'] = ngrams_freq['ngram'].apply(get_medical_dependencies)\n",
    "\n",
    "# === Filter 1: Ngram-Ngram Jaccard > threshold ===\n",
    "jaccard_threshold = 0.33\n",
    "ngram_edges = ngram_jaccard.where(ngram_jaccard > jaccard_threshold)\n",
    "ngram_edges = ngram_edges.stack().reset_index()\n",
    "ngram_edges.columns = [\"source_ngram\", \"target_ngram\", \"similarity\"]\n",
    "ngram_edges[\"source_freq\"] = ngram_edges[\"source_ngram\"].map(ngram_frequencies)\n",
    "ngram_edges[\"target_freq\"] = ngram_edges[\"target_ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-ngram relationships with medical dependencies ===\n",
    "ngram_edges_with_dependencies = ngram_edges.merge(ngrams_freq[['ngram', 'dependencies']], left_on='source_ngram', right_on='ngram', how='left')\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.drop(columns=['ngram_y'])\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.rename(columns={'ngram_x': 'source_ngram', 'dependencies': 'source_dependencies'})\n",
    "\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.merge(ngrams_freq[['ngram', 'dependencies']], left_on='target_ngram', right_on='ngram', how='left')\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.drop(columns=['ngram_y'])\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.rename(columns={'ngram_x': 'target_ngram', 'dependencies': 'target_dependencies'})\n",
    "\n",
    "# Save ngram-ngram dependencies with their parsed medical dependencies\n",
    "ngram_edges_with_dependencies.to_csv(\"25-3-2025_14_29_3_5/medical_ngram_dependencies_with_medical_parsing_sm.csv\", index=False)\n",
    "\n",
    "# === Filter 2: Ngram-Paper Relevance > 0 ===\n",
    "ngram_paper = ngram_paper.apply(pd.to_numeric, errors='coerce')\n",
    "ngram_paper_long = ngram_paper[ngram_paper > 0].stack().reset_index()\n",
    "ngram_paper_long.columns = [\"ngram\", \"paper\", \"score\"]\n",
    "ngram_paper_long[\"ngram_freq\"] = ngram_paper_long[\"ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-paper links ===\n",
    "ngram_paper_long.to_csv(\"25-3-2025_14_29_3_5/ngram_paper_links.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files saved: medical_ngram_dependencies_with_medical_parsing_sm.csv & ngram_paper_links.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "# === Load SciSpacy's Medical model (sm version) ===\n",
    "nlp = spacy.load(\"en_core_sci_sm\")  # Using the small version of the model\n",
    "\n",
    "# Load the entity linker\n",
    "linker = EntityLinker(resolve_abbreviations=True)\n",
    "\n",
    "# === Load CSV files ===\n",
    "# Load cleaned n-grams (with placeholder frequency if not present)\n",
    "ngrams_freq = pd.read_csv(\"25-3-2025_14_29_3_5/cleaned_ngrams.csv\", names=[\"ngram\", \"frequency\"])\n",
    "ngram_paper = pd.read_csv(\"25-3-2025_14_29_3_5/output_with_entities.csv\", index_col=0)\n",
    "ngram_jaccard = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_jaccard_similarity_2.csv\", index_col=0)\n",
    "\n",
    "# === Frequency mapping ===\n",
    "ngram_frequencies = ngrams_freq.set_index(\"ngram\")[\"frequency\"]\n",
    "\n",
    "# === Function to perform medical dependency parsing using SciSpacy ===\n",
    "def get_medical_dependencies(ngram):\n",
    "    # Process the ngram with SciSpacy to extract dependencies\n",
    "    doc = nlp(ngram)\n",
    "    doc._.linker = linker  # Apply the entity linker for medical entity recognition\n",
    "    dependencies = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Store each token's word, its dependency relation, and the head word\n",
    "        dependencies.append((token.text, token.dep_, token.head.text))\n",
    "    \n",
    "    return dependencies\n",
    "\n",
    "# === Apply dependency parsing to all n-grams ===\n",
    "ngrams_freq['dependencies'] = ngrams_freq['ngram'].apply(get_medical_dependencies)\n",
    "\n",
    "# === Filter 1: Ngram-Ngram Jaccard > threshold ===\n",
    "jaccard_threshold = 0.33\n",
    "ngram_edges = ngram_jaccard.where(ngram_jaccard > jaccard_threshold)\n",
    "ngram_edges = ngram_edges.stack().reset_index()\n",
    "ngram_edges.columns = [\"source_ngram\", \"target_ngram\", \"similarity\"]\n",
    "ngram_edges[\"source_freq\"] = ngram_edges[\"source_ngram\"].map(ngram_frequencies)\n",
    "ngram_edges[\"target_freq\"] = ngram_edges[\"target_ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-ngram relationships with medical dependencies ===\n",
    "ngram_edges_with_dependencies = ngram_edges.merge(ngrams_freq[['ngram', 'dependencies']], left_on='source_ngram', right_on='ngram', how='left')\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.drop(columns=['ngram_y'])\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.rename(columns={'ngram_x': 'source_ngram', 'dependencies': 'source_dependencies'})\n",
    "\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.merge(ngrams_freq[['ngram', 'dependencies']], left_on='target_ngram', right_on='ngram', how='left')\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.drop(columns=['ngram_y'])\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.rename(columns={'ngram_x': 'target_ngram', 'dependencies': 'target_dependencies'})\n",
    "\n",
    "# Save ngram-ngram dependencies with their parsed medical dependencies\n",
    "ngram_edges_with_dependencies.to_csv(\"25-3-2025_14_29_3_5/medical_ngram_dependencies_with_medical_parsing_sm.csv\", index=False)\n",
    "\n",
    "# === Filter 2: Ngram-Paper Relevance > 0 ===\n",
    "ngram_paper = ngram_paper.apply(pd.to_numeric, errors='coerce')\n",
    "ngram_paper_long = ngram_paper[ngram_paper > 0].stack().reset_index()\n",
    "ngram_paper_long.columns = [\"ngram\", \"paper\", \"score\"]\n",
    "ngram_paper_long[\"ngram_freq\"] = ngram_paper_long[\"ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-paper links ===\n",
    "ngram_paper_long.to_csv(\"25-3-2025_14_29_3_5/ngram_paper_links.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files saved: medical_ngram_dependencies_with_medical_parsing_sm.csv & ngram_paper_links.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2449e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# === Load SciSpacy's Medical model (small version) ===\n",
    "nlp = spacy.load(\"en_core_sci_sm\")  # SciSpacy's small model for medical/scientific text\n",
    "\n",
    "# === Load CSV files ===\n",
    "# Load cleaned n-grams (with placeholder frequency if not present)\n",
    "ngrams_freq = pd.read_csv(\"25-3-2025_14_29_3_5/cleaned_ngrams.csv\", names=[\"ngram\", \"frequency\"])\n",
    "ngram_paper = pd.read_csv(\"25-3-2025_14_29_3_5/output_with_entities.csv\", index_col=0)\n",
    "ngram_jaccard = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_jaccard_similarity_2.csv\", index_col=0)\n",
    "\n",
    "# === Frequency mapping ===\n",
    "ngram_frequencies = ngrams_freq.set_index(\"ngram\")[\"frequency\"]\n",
    "\n",
    "# === Function to perform dependency parsing ===\n",
    "def get_dependencies(ngram):\n",
    "    # Process the ngram with spaCy to extract dependencies\n",
    "    doc = nlp(ngram)\n",
    "    dependencies = []\n",
    "    for token in doc:\n",
    "        # Store each token's word, its dependency relation, and the head word\n",
    "        dependencies.append((token.text, token.dep_, token.head.text))\n",
    "    return dependencies\n",
    "\n",
    "# === Apply dependency parsing to all n-grams ===\n",
    "ngrams_freq['dependencies'] = ngrams_freq['ngram'].apply(get_dependencies)\n",
    "\n",
    "# === Filter 1: Ngram-Ngram Jaccard > threshold ===\n",
    "jaccard_threshold = 0.33\n",
    "ngram_edges = ngram_jaccard.where(ngram_jaccard > jaccard_threshold)\n",
    "ngram_edges = ngram_edges.stack().reset_index()\n",
    "ngram_edges.columns = [\"source_ngram\", \"target_ngram\", \"similarity\"]\n",
    "ngram_edges[\"source_freq\"] = ngram_edges[\"source_ngram\"].map(ngram_frequencies)\n",
    "ngram_edges[\"target_freq\"] = ngram_edges[\"target_ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-ngram relationships with dependencies ===\n",
    "ngram_edges_with_dependencies = ngram_edges.merge(ngrams_freq[['ngram', 'dependencies']], left_on='source_ngram', right_on='ngram', how='left')\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.drop(columns=['ngram_y'])\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.rename(columns={'ngram_x': 'source_ngram', 'dependencies': 'source_dependencies'})\n",
    "\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.merge(ngrams_freq[['ngram', 'dependencies']], left_on='target_ngram', right_on='ngram', how='left')\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.drop(columns=['ngram_y'])\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.rename(columns={'ngram_x': 'target_ngram', 'dependencies': 'target_dependencies'})\n",
    "\n",
    "# Save ngram-ngram dependencies with their parsed dependencies\n",
    "ngram_edges_with_dependencies.to_csv(\"25-3-2025_14_29_3_5/medical_ngram_dependencies_with_parsing.csv\", index=False)\n",
    "\n",
    "# === Filter 2: Ngram-Paper Relevance > 0 ===\n",
    "ngram_paper = ngram_paper.apply(pd.to_numeric, errors='coerce')\n",
    "ngram_paper_long = ngram_paper[ngram_paper > 0].stack().reset_index()\n",
    "ngram_paper_long.columns = [\"ngram\", \"paper\", \"score\"]\n",
    "ngram_paper_long[\"ngram_freq\"] = ngram_paper_long[\"ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-paper links ===\n",
    "ngram_paper_long.to_csv(\"25-3-2025_14_29_3_5/ngram_paper_links.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files saved: medical_ngram_dependencies_with_parsing.csv & ngram_paper_links.csv\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# === Load SciSpacy's Medical model (small version) ===\n",
    "nlp = spacy.load(\"en_core_sci_sm\")  # SciSpacy's small model for medical/scientific text\n",
    "\n",
    "# === Load CSV files ===\n",
    "# Load cleaned n-grams (with placeholder frequency if not present)\n",
    "ngrams_freq = pd.read_csv(\"25-3-2025_14_29_3_5/cleaned_ngrams.csv\", names=[\"ngram\", \"frequency\"])\n",
    "ngram_paper = pd.read_csv(\"25-3-2025_14_29_3_5/output_with_entities.csv\", index_col=0)\n",
    "ngram_jaccard = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_jaccard_similarity_2.csv\", index_col=0)\n",
    "\n",
    "# === Frequency mapping ===\n",
    "ngram_frequencies = ngrams_freq.set_index(\"ngram\")[\"frequency\"]\n",
    "\n",
    "# === Function to perform dependency parsing ===\n",
    "def get_dependencies(ngram):\n",
    "    # Process the ngram with spaCy to extract dependencies\n",
    "    doc = nlp(ngram)\n",
    "    dependencies = []\n",
    "    for token in doc:\n",
    "        # Store each token's word, its dependency relation, and the head word\n",
    "        dependencies.append((token.text, token.dep_, token.head.text))\n",
    "    return dependencies\n",
    "\n",
    "# === Apply dependency parsing to all n-grams ===\n",
    "ngrams_freq['dependencies'] = ngrams_freq['ngram'].apply(get_dependencies)   \n",
    "\n",
    "# === Filter 1: Ngram-Ngram Jaccard > threshold ===\n",
    "jaccard_threshold = 0.33\n",
    "ngram_edges = ngram_jaccard.where(ngram_jaccard > jaccard_threshold)\n",
    "ngram_edges = ngram_edges.stack().reset_index()\n",
    "ngram_edges.columns = [\"source_ngram\", \"target_ngram\", \"similarity\"]\n",
    "ngram_edges[\"source_freq\"] = ngram_edges[\"source_ngram\"].map(ngram_frequencies)\n",
    "ngram_edges[\"target_freq\"] = ngram_edges[\"target_ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-ngram relationships with dependencies ===\n",
    "ngram_edges_with_dependencies = ngram_edges.merge(ngrams_freq[['ngram', 'dependencies']], left_on='source_ngram', right_on='ngram', how='left')\n",
    "\n",
    "# Print column names after first merge for debugging\n",
    "print(ngram_edges_with_dependencies.columns)  # Check column names\n",
    "\n",
    "# If 'ngram_y' exists, drop it\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.drop(columns=['ngram_y'], errors='ignore')\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.rename(columns={'ngram_x': 'source_ngram', 'dependencies': 'source_dependencies'})\n",
    "\n",
    "# Merge for the target_ngram\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.merge(ngrams_freq[['ngram', 'dependencies']], left_on='target_ngram', right_on='ngram', how='left')\n",
    "\n",
    "# Print column names after second merge for debugging\n",
    "print(ngram_edges_with_dependencies.columns)  # Check column names again\n",
    "\n",
    "# If 'ngram_y' exists again, drop it\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.drop(columns=['ngram_y'], errors='ignore')\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.rename(columns={'ngram_x': 'target_ngram', 'dependencies': 'target_dependencies'})\n",
    "\n",
    "# Save ngram-ngram dependencies with their parsed dependencies\n",
    "ngram_edges_with_dependencies.to_csv(\"25-3-2025_14_29_3_5/medical_ngram_dependencies_with_parsing.csv\", index=False)\n",
    "\n",
    "# === Filter 2: Ngram-Paper Relevance > 0 ===\n",
    "ngram_paper = ngram_paper.apply(pd.to_numeric, errors='coerce')\n",
    "ngram_paper_long = ngram_paper[ngram_paper > 0].stack().reset_index()\n",
    "ngram_paper_long.columns = [\"ngram\", \"paper\", \"score\"]\n",
    "ngram_paper_long[\"ngram_freq\"] = ngram_paper_long[\"ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-paper links ===\n",
    "ngram_paper_long.to_csv(\"25-3-2025_14_29_3_5/ngram_paper_links.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files saved: medical_ngram_dependencies_with_parsing.csv & ngram_paper_links.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f35025f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source_ngram', 'target_ngram', 'similarity', 'source_freq',\n",
      "       'target_freq', 'ngram', 'dependencies'],\n",
      "      dtype='object')\n",
      "Index(['source_ngram', 'target_ngram', 'similarity', 'source_freq',\n",
      "       'target_freq', 'ngram_x', 'source_dependencies', 'ngram_y',\n",
      "       'dependencies'],\n",
      "      dtype='object')\n",
      "✅ Files saved: medical_ngram_dependencies_with_parsing.csv & ngram_paper_links.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# === Load SciSpacy's Medical model (small version) ===\n",
    "nlp = spacy.load(\"en_core_sci_sm\")  # SciSpacy's small model for medical/scientific text\n",
    "\n",
    "# === Load CSV files ===\n",
    "# Load cleaned n-grams (with placeholder frequency if not present)\n",
    "ngrams_freq = pd.read_csv(\"25-3-2025_14_29_3_5/cleaned_ngrams_priority.csv\", names=[\"ngram\", \"frequency\"])\n",
    "ngram_paper = pd.read_csv(\"25-3-2025_14_29_3_5/output_with_entities.csv\", index_col=0)\n",
    "ngram_jaccard = pd.read_csv(\"25-3-2025_14_29_3_5/ngram_jaccard_similarity_2.csv\", index_col=0)\n",
    "\n",
    "# === Frequency mapping ===\n",
    "ngram_frequencies = ngrams_freq.set_index(\"ngram\")[\"frequency\"]\n",
    "\n",
    "# === Function to perform dependency parsing ===\n",
    "def get_dependencies(ngram):\n",
    "    # Process the ngram with spaCy to extract dependencies\n",
    "    doc = nlp(ngram)\n",
    "    dependencies = []\n",
    "    for token in doc:\n",
    "        # Store each token's word, its dependency relation, and the head word\n",
    "        dependencies.append((token.text, token.dep_, token.head.text))\n",
    "    return dependencies\n",
    "\n",
    "# === Apply dependency parsing to all n-grams ===\n",
    "ngrams_freq['dependencies'] = ngrams_freq['ngram'].apply(get_dependencies)\n",
    "\n",
    "# === Filter 1: Ngram-Ngram Jaccard > threshold ===\n",
    "#jaccard_threshold = 0.33\n",
    "ngram_edges = ngram_jaccard.where((ngram_jaccard > 0.33) & (ngram_jaccard < 1.0))\n",
    "\n",
    "ngram_edges = ngram_edges.stack().reset_index()\n",
    "ngram_edges.columns = [\"source_ngram\", \"target_ngram\", \"similarity\"]\n",
    "ngram_edges[\"source_freq\"] = ngram_edges[\"source_ngram\"].map(ngram_frequencies)\n",
    "ngram_edges[\"target_freq\"] = ngram_edges[\"target_ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-ngram relationships with dependencies ===\n",
    "ngram_edges_with_dependencies = ngram_edges.merge(ngrams_freq[['ngram', 'dependencies']], left_on='source_ngram', right_on='ngram', how='left')\n",
    "\n",
    "# Print column names after first merge for debugging\n",
    "print(ngram_edges_with_dependencies.columns)  # Check column names\n",
    "\n",
    "# If 'ngram_y' exists, drop it\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.drop(columns=['ngram_y'], errors='ignore')\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.rename(columns={'ngram_x': 'source_ngram', 'dependencies': 'source_dependencies'})\n",
    "\n",
    "# Merge for the target_ngram\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.merge(ngrams_freq[['ngram', 'dependencies']], left_on='target_ngram', right_on='ngram', how='left')\n",
    "\n",
    "# Print column names after second merge for debugging\n",
    "print(ngram_edges_with_dependencies.columns)  # Check column names again\n",
    "\n",
    "# If 'ngram_y' exists again, drop it\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.drop(columns=['ngram_y'], errors='ignore')\n",
    "ngram_edges_with_dependencies = ngram_edges_with_dependencies.rename(columns={'ngram_x': 'target_ngram', 'dependencies': 'target_dependencies'})\n",
    "\n",
    "# Save ngram-ngram dependencies with their parsed dependencies\n",
    "ngram_edges_with_dependencies.to_csv(\"25-3-2025_14_29_3_5/medical_ngram_dependencies_with_parsing_prio.csv\", index=False)\n",
    "\n",
    "# === Filter 2: Ngram-Paper Relevance > 0 ===\n",
    "ngram_paper = ngram_paper.apply(pd.to_numeric, errors='coerce')\n",
    "ngram_paper_long = ngram_paper[ngram_paper > 0].stack().reset_index()\n",
    "ngram_paper_long.columns = [\"ngram\", \"paper\", \"score\"]\n",
    "ngram_paper_long[\"ngram_freq\"] = ngram_paper_long[\"ngram\"].map(ngram_frequencies)\n",
    "\n",
    "# === Save ngram-paper links ===\n",
    "ngram_paper_long.to_csv(\"25-3-2025_14_29_3_5/ngram_paper_links_prio.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files saved: medical_ngram_dependencies_with_parsing.csv & ngram_paper_links.csv\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7ab8fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in c:\\users\\soura\\anaconda3\\lib\\site-packages (5.28.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\soura\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\soura\\anaconda3\\lib\\site-packages (from neo4j) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soura\\AppData\\Local\\Temp\\ipykernel_17452\\1006550497.py:54: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install dependencies (only run once)\n",
    "!pip install neo4j pandas\n",
    "\n",
    "# Step 2: Import libraries\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Step 3: Connect to Neo4j\n",
    "uri = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"sourabh@123\"  # 🔐 Replace with your Neo4j password\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "# Step 4: Load the dependency CSV\n",
    "file_path = \"25-3-2025_14_29_3_5/medical_ngram_dependencies_with_parsing.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 5: Convert stringified dependencies back to lists of tuples\n",
    "df['source_dependencies'] = df['source_dependencies'].apply(ast.literal_eval)\n",
    "df['target_dependencies'] = df['target_dependencies'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 6: Define upload logic with better relationships\n",
    "def create_ngram_graph(tx, source, target, similarity, source_deps, target_deps):\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (src:Ngram {text: $source})\n",
    "        MERGE (tgt:Ngram {text: $target})\n",
    "        MERGE (src)-[s:SIMILAR_TO]->(tgt)\n",
    "        SET s.similarity = $similarity\n",
    "    \"\"\", source=source, target=target, similarity=similarity)\n",
    "\n",
    "    for word, dep, head in source_deps:\n",
    "        tx.run(\"\"\"\n",
    "            MATCH (n:Ngram {text: $ngram})\n",
    "            MERGE (t:Token {text: $word})\n",
    "            MERGE (h:Token {text: $head})\n",
    "            MERGE (t)-[:DEPENDS_ON {type: $dep}]->(h)\n",
    "            MERGE (n)-[:CONTAINS]->(t)\n",
    "        \"\"\", ngram=source, word=word, dep=dep, head=head)\n",
    "\n",
    "    for word, dep, head in target_deps:\n",
    "        tx.run(\"\"\"\n",
    "            MATCH (n:Ngram {text: $ngram})\n",
    "            MERGE (t:Token {text: $word})\n",
    "            MERGE (h:Token {text: $head})\n",
    "            MERGE (t)-[:DEPENDS_ON {type: $dep}]->(h)\n",
    "            MERGE (n)-[:CONTAINS]->(t)\n",
    "        \"\"\", ngram=target, word=word, dep=dep, head=head)\n",
    "\n",
    "# Step 7: Upload to Neo4j\n",
    "with driver.session() as session:\n",
    "    for _, row in df.iterrows():\n",
    "        session.write_transaction(\n",
    "            create_ngram_graph,\n",
    "            row['source_ngram'],\n",
    "            row['target_ngram'],\n",
    "            row['similarity'],\n",
    "            row['source_dependencies'],\n",
    "            row['target_dependencies']\n",
    "        )\n",
    " \n",
    "driver.close()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
