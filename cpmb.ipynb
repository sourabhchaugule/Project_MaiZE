{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3f3d731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "08349b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\soura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\soura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff074dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"Checking/\"\n",
    "output_folder = \"25-3-2025_14_3/\"\n",
    "os.makedirs(output_folder, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1e965644",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "efebe50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_words = set({    \n",
    "    \"article\", \"et\", \"al\", \"in\" , \"to\",\"and\" , \"for\", \"to\", \"a\", \"y\", \"is\", \"of\", \"all\" ,\"the\",\"from\", \"are\",\"terms\", \"conditions\", \"publication\", \"citation\",\n",
    "    \"open\", \"access\", \"license\", \"cc\", \"by\", \"creative\", \"commons\", \"attribution\",\n",
    "    \"shown\", \"method\", \"state\", \"date\", \"plot\", \"trials\", \"per\", \"cent\", \"new\",\n",
    "    \"present\", \"iii\", \"iv\", \"v\", \"etc\", \"proc\", \"natl\", \"acad\", \"sci\", \"usa\",\n",
    "    \"vol\", \"pp\", \"using\", \"also\", \"used\", \"based\", \"may\", \"however\", \"one\", \"two\",\n",
    "    \"three\", \"four\", \"five\", \"data\", \"set\", \"including\", \"due\", \"figure\", \"table\",\n",
    "    \"fig\", \"found\", \"work\", \"among\", \"study\", \"analysis\", \"different\", \"several\",\n",
    "    \"order\", \"low\", \"high\", \"higher\", \"lower\", \"within\", \"between\", \"without\",\n",
    "    \"results\", \"approach\", \"across\", \"group\", \"suggest\", \"suggests\", \"indicate\",\n",
    "    \"indicates\", \"according\", \"within\", \"among\", \"amongst\", \"within\", \"even\",\n",
    "    \"although\", \"further\", \"well\", \"known\", \"previously\", \"recent\", \"recently\",\n",
    "    \"first\", \"second\", \"third\", \"various\", \"varied\", \"example\", \"examples\",\n",
    "    \"others\", \"another\", \"obtained\",  \"show\", \"shows\", \"including\",\n",
    "    \"would\", \"could\", \"can\", \"might\", \"many\", \"much\", \"several\", \"certain\",\n",
    "    \"some\", \"such\", \"well\", \"particular\", \"various\", \"often\", \"sometimes\",\n",
    "    \"always\", \"never\", \"previous\", \"past\", \"future\", \"new\", \"old\", \"young\",\n",
    "    \"common\", \"uncommon\", \"rare\", \"frequent\",\n",
    "    \"other\", \"additional\", \"extra\", \"further\", \n",
    "    \"pros\", \"cons\", \"effective\", \"ineffective\", \"efficacy\", \"efficiency\", \"slow\",\n",
    "    \"early\", \"late\", \"earlier\", \"latest\", \"delayed\",\n",
    "    \"quick\", \"quicker\", \"quickest\", \"rapid\", \"rapidly\", \"slowly\", \"gradual\",\n",
    "    \"sudden\", \"short\", \"long\", \"shorter\", \"shortest\", \"longer\", \"longest\",\n",
    "    \"temporary\", \"permanent\", \"transient\", \"persistent\", \n",
    "    \"mild\", \"moderate\", \"severe\", \"slight\", \"significant\", \"insignificant\",\n",
    "    \"noticeable\", \"unnoticeable\", \"detectable\",\n",
    "    \"wiley\", \"online\", \"library\", \"downloaded\", \"https\", \"see\", \"rules\", \"use\", \"oa\", \"articles\", \"governed\", \"applicable\",\n",
    "    \"national\", \"center\", \"health\", \"statistics\", \"centers\",\n",
    "    \"mortality\", \"public\", \"tapes\", \"american\", \"society\", \"atlanta\", \"tape\", \"reviews\", \"december\", \"volume\", \"nature\",\n",
    "    \"publishing\", \"average\", \"annual\", \"percent\", \"change\", \"note\", \"trends\", \"analyzed\", \"joinpoint\", \"program\",\n",
    "    \"total\", \"leading\", \"causes\", \"us\", \"standard\", \"end\", \"seer\", \"department\",\n",
    "    \"individual\", \"b\", \"dvm\", \"phd\", \"year\", \"approximately\", \"fewer\", \"cases\", \"estimated\",\"alabama\", \"alaska\", \"arizona\", \"arkansas\", \"california\", \"colorado\", \"connecticut\", \"delaware\", \"dist\",\n",
    "    \"columbia\", \"florida\", \"georgia\", \"hawaii\", \"idaho\", \"illinois\", \"indiana\", \"iowa\", \"kansas\", \"kentucky\",\n",
    "    \"louisiana\", \"maine\", \"maryland\", \"massachusetts\", \"michigan\", \"minnesota\", \"mississippi\", \"missouri\",\n",
    "    \"montana\", \"nebraska\", \"nevada\", \"hampshire\", \"jersey\", \"mexico\", \"york\", \"north\", \"carolina\", \"dakota\",\n",
    "    \"ohio\", \"oklahoma\", \"oregon\", \"pennsylvania\", \"rhode\", \"island\", \"south\", \"tennessee\", \"texas\", \"utah\",\n",
    "    \"vermont\", \"virginia\", \"washington\", \"west\", \"wisconsin\", \"wyoming\", \"ca\", \"clin\", \"j\", \"apc\", \"aapc\", \"ons\", \"mp\", \"cl\", \"ries\", \"lag\", \"eisner\", \"naaccr\" \n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2a7b1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(pdf_file):\n",
    "    doc = fitz.open(pdf_file)\n",
    "    text = \"\"\n",
    "    capture = False\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()\n",
    "        if \"abstract\" in page_text.lower():\n",
    "            capture = True\n",
    "        if \"references\" in page_text.lower():\n",
    "            capture = False\n",
    "        if capture:\n",
    "            text += page_text + \" \"\n",
    "    return text.lower()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7a4eed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_ngram = {i: Counter() for i in range(1, 5)}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "22b2af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_data = os.path.join(input_folder, file_name)\n",
    "        raw_text = extract_text(pdf_data)\n",
    "\n",
    "        if not raw_text:\n",
    "            continue\n",
    "\n",
    "        tokens = word_tokenize(raw_text)\n",
    "        tokens = [word for word in tokens if word.isalpha() and word not in unwanted_words]\n",
    "\n",
    "        for n in range(1, 5):\n",
    "            global_ngram[n].update(Counter(ngrams(tokens, n)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2034e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_file_path = os.path.join(output_folder, \"ngrams.txt\")\n",
    "with open(ngram_file_path, \"w\", encoding=\"utf-8\") as ngram_file:\n",
    "    for n in range(1, 5):\n",
    "        ngram_file.write(f\"\\n{n}-grams:\\n\")\n",
    "        for ngram, count in global_ngram[n].items():\n",
    "            ngram_file.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "        ngram_file.write(\"\\n\" + \"=\" * 50 + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ec3372f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ngrams(global_ngram):\n",
    "    filtered_ngrams = {n: Counter() for n in range(1, 5)}\n",
    "    \n",
    "    for n in range(1, 5):\n",
    "        for ngram, count in global_ngram[n].items():\n",
    "            is_redundant = False\n",
    "            for m in range(n + 1, 5):\n",
    "                for higher_ngram in global_ngram[m]:\n",
    "                    if set(ngram).issubset(set(higher_ngram)):\n",
    "                        is_redundant = True\n",
    "                        break\n",
    "                if is_redundant:\n",
    "                    break\n",
    "            if not is_redundant:\n",
    "                filtered_ngrams[n][ngram] = count\n",
    "\n",
    "    return filtered_ngrams     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5302ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ngrams = filter_ngrams(global_ngram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2d6b4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ngram_file_path = os.path.join(output_folder, \"clean_ngram.txt\")\n",
    "with open(clean_ngram_file_path, \"w\", encoding=\"utf-8\") as clean_file:\n",
    "    for n in range(1, 5):\n",
    "        clean_file.write(f\"\\n{n}-grams (cleaned):\\n\")\n",
    "        for ngram, count in cleaned_ngrams[n].items():\n",
    "            clean_file.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "        clean_file.write(\"\\n\" + \"=\" * 50 + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8c0b5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_ngrams(filtered_ngrams, filename=\"top_200ngram.txt\", top_n=200):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Top 200 Bigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[2].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "        f.write(\"\\nTop 200 Trigrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[3].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\")\n",
    "\n",
    "        f.write(\"\\nTop 200 Fourgrams:\\n\")\n",
    "        for ngram, count in filtered_ngrams[4].most_common(top_n):\n",
    "            f.write(f\"{' '.join(ngram)}: {count}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9b928a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_top_ngrams(cleaned_ngrams, os.path.join(output_folder, \"top_200ngram.txt\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e7b68bb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(input_folder, top_ngrams_file, output_file):\n",
    "    paper_files = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "    fourgrams = []\n",
    "    with open(top_ngrams_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        capture = False\n",
    "        for line in lines:\n",
    "            if \"Top 200 Fourgrams:\" in line:\n",
    "                capture = True \n",
    "                continue\n",
    "            if capture and \":\" in line:\n",
    "                fourgrams.append(line.split(\":\")[0].strip())\n",
    "\n",
    "    cooccurrence_matrix = pd.DataFrame(0, index=fourgrams, columns=paper_files)\n",
    "\n",
    "    for file_name in paper_files:\n",
    "        pdf_data = os.path.join(input_folder, file_name)\n",
    "        raw_text = extract_text(pdf_data)\n",
    "\n",
    "        if not raw_text:\n",
    "            continue\n",
    "\n",
    "        tokens = [word for word in word_tokenize(raw_text) if word.isalpha() and word not in stop_words]\n",
    "        paper_ngrams = Counter(ngrams(tokens, 4))\n",
    "\n",
    "        for fourgram in fourgrams:\n",
    "            fourgram_tuple = tuple(fourgram.split())\n",
    "            cooccurrence_matrix.at[fourgram, file_name] = paper_ngrams.get(fourgram_tuple, 0)\n",
    "\n",
    "    cooccurrence_matrix.to_csv(output_file)\n",
    "\n",
    "top_ngrams_path = os.path.join(output_folder, \"top_200ngram.txt\")\n",
    "cooccurrence_output = os.path.join(output_folder, \"fourgram_cooccurrence.csv\")\n",
    "\n",
    "build_cooccurrence_matrix(input_folder, top_ngrams_path, cooccurrence_output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "14ee9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = os.path.join(output_folder, \"fourgram_cooccurrence.csv\")\n",
    "cooccurrence_matrix = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "jaccard_similarity = {}\n",
    "\n",
    "for paper in cooccurrence_matrix.columns:\n",
    "    paper_fourgrams = set(cooccurrence_matrix[paper][cooccurrence_matrix[paper] > 0].index)\n",
    "    \n",
    "    similarities = {}\n",
    "    for fourgram in cooccurrence_matrix.index:\n",
    "        fourgram_set = {fourgram}\n",
    "        intersection = len(fourgram_set & paper_fourgrams)\n",
    "        union = len(fourgram_set | paper_fourgrams)\n",
    "        similarity = intersection / union if union != 0 else 0\n",
    "        similarities[fourgram] = similarity\n",
    "    \n",
    "    jaccard_similarity[paper] = similarities\n",
    "\n",
    "jaccard_df = pd.DataFrame(jaccard_similarity)\n",
    "jaccard_df.to_csv(os.path.join(output_folder, \"fourgram_jaccard_similarity.csv\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d22182d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\soura\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (1.10.14)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\soura\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "                                              0.0/12.8 MB ? eta -:--:--\n",
      "                                              0.0/12.8 MB ? eta -:--:--\n",
      "                                              0.0/12.8 MB ? eta -:--:--\n",
      "                                             0.1/12.8 MB 558.5 kB/s eta 0:00:23\n",
      "                                             0.1/12.8 MB 581.0 kB/s eta 0:00:22\n",
      "                                             0.1/12.8 MB 653.6 kB/s eta 0:00:20\n",
      "                                             0.1/12.8 MB 653.6 kB/s eta 0:00:20\n",
      "                                             0.1/12.8 MB 653.6 kB/s eta 0:00:20\n",
      "                                             0.3/12.8 MB 850.6 kB/s eta 0:00:15\n",
      "                                             0.3/12.8 MB 922.8 kB/s eta 0:00:14\n",
      "     -                                       0.4/12.8 MB 857.5 kB/s eta 0:00:15\n",
      "     -                                       0.4/12.8 MB 851.5 kB/s eta 0:00:15\n",
      "     -                                       0.5/12.8 MB 880.6 kB/s eta 0:00:15\n",
      "     -                                       0.5/12.8 MB 879.9 kB/s eta 0:00:14\n",
      "     -                                       0.5/12.8 MB 879.2 kB/s eta 0:00:14\n",
      "     -                                       0.6/12.8 MB 889.7 kB/s eta 0:00:14\n",
      "     -                                       0.6/12.8 MB 889.7 kB/s eta 0:00:14\n",
      "     --                                      0.7/12.8 MB 940.0 kB/s eta 0:00:13\n",
      "     --                                      0.7/12.8 MB 933.4 kB/s eta 0:00:13\n",
      "     --                                      0.7/12.8 MB 925.1 kB/s eta 0:00:14\n",
      "     --                                      0.8/12.8 MB 910.0 kB/s eta 0:00:14\n",
      "     --                                      0.8/12.8 MB 910.0 kB/s eta 0:00:14\n",
      "     --                                      0.9/12.8 MB 921.8 kB/s eta 0:00:13\n",
      "     --                                      0.9/12.8 MB 927.9 kB/s eta 0:00:13\n",
      "     --                                      0.9/12.8 MB 907.1 kB/s eta 0:00:14\n",
      "     --                                      0.9/12.8 MB 903.8 kB/s eta 0:00:14\n",
      "     --                                      1.0/12.8 MB 883.4 kB/s eta 0:00:14\n",
      "     ---                                     1.0/12.8 MB 873.8 kB/s eta 0:00:14\n",
      "     ---                                     1.0/12.8 MB 856.2 kB/s eta 0:00:14\n",
      "     ---                                     1.0/12.8 MB 837.4 kB/s eta 0:00:15\n",
      "     ---                                     1.0/12.8 MB 837.4 kB/s eta 0:00:15\n",
      "     ---                                     1.1/12.8 MB 850.1 kB/s eta 0:00:14\n",
      "     ---                                     1.2/12.8 MB 856.4 kB/s eta 0:00:14\n",
      "     ---                                     1.2/12.8 MB 868.7 kB/s eta 0:00:14\n",
      "     ---                                     1.3/12.8 MB 878.5 kB/s eta 0:00:14\n",
      "     ---                                     1.3/12.8 MB 878.5 kB/s eta 0:00:14\n",
      "     ----                                    1.3/12.8 MB 847.3 kB/s eta 0:00:14\n",
      "     ----                                    1.4/12.8 MB 867.1 kB/s eta 0:00:14\n",
      "     ----                                    1.4/12.8 MB 867.1 kB/s eta 0:00:14\n",
      "     ----                                    1.4/12.8 MB 839.0 kB/s eta 0:00:14\n",
      "     ----                                    1.5/12.8 MB 875.8 kB/s eta 0:00:13\n",
      "     ----                                    1.5/12.8 MB 861.7 kB/s eta 0:00:14\n",
      "     ----                                    1.6/12.8 MB 868.2 kB/s eta 0:00:13\n",
      "     ----                                    1.6/12.8 MB 866.2 kB/s eta 0:00:13\n",
      "     -----                                   1.6/12.8 MB 873.8 kB/s eta 0:00:13\n",
      "     -----                                   1.6/12.8 MB 873.8 kB/s eta 0:00:13\n",
      "     -----                                   1.6/12.8 MB 873.8 kB/s eta 0:00:13\n",
      "     -----                                   1.8/12.8 MB 877.0 kB/s eta 0:00:13\n",
      "     -----                                   1.8/12.8 MB 875.5 kB/s eta 0:00:13\n",
      "     -----                                   1.8/12.8 MB 875.5 kB/s eta 0:00:13\n",
      "     -----                                   1.8/12.8 MB 839.6 kB/s eta 0:00:14\n",
      "     -----                                   1.8/12.8 MB 839.6 kB/s eta 0:00:14\n",
      "     -----                                   1.9/12.8 MB 846.0 kB/s eta 0:00:13\n",
      "     -----                                   1.9/12.8 MB 849.1 kB/s eta 0:00:13\n",
      "     -----                                   1.9/12.8 MB 849.1 kB/s eta 0:00:13\n",
      "     -----                                   1.9/12.8 MB 831.3 kB/s eta 0:00:14\n",
      "     -----                                   2.0/12.8 MB 830.0 kB/s eta 0:00:14\n",
      "     -----                                   2.0/12.8 MB 830.0 kB/s eta 0:00:14\n",
      "     -----                                   2.0/12.8 MB 830.0 kB/s eta 0:00:14\n",
      "     -----                                   2.0/12.8 MB 830.0 kB/s eta 0:00:14\n",
      "     -----                                   2.0/12.8 MB 830.0 kB/s eta 0:00:14\n",
      "     ------                                  2.2/12.8 MB 858.0 kB/s eta 0:00:13\n",
      "     ------                                  2.2/12.8 MB 858.0 kB/s eta 0:00:13\n",
      "     ------                                  2.3/12.8 MB 839.3 kB/s eta 0:00:13\n",
      "     -------                                 2.3/12.8 MB 849.9 kB/s eta 0:00:13\n",
      "     -------                                 2.3/12.8 MB 849.9 kB/s eta 0:00:13\n",
      "     -------                                 2.4/12.8 MB 855.5 kB/s eta 0:00:13\n",
      "     -------                                 2.5/12.8 MB 877.2 kB/s eta 0:00:12\n",
      "     -------                                 2.5/12.8 MB 878.4 kB/s eta 0:00:12\n",
      "     --------                                2.7/12.8 MB 899.2 kB/s eta 0:00:12\n",
      "     --------                                2.7/12.8 MB 912.0 kB/s eta 0:00:12\n",
      "     --------                                2.7/12.8 MB 896.3 kB/s eta 0:00:12\n",
      "     --------                                2.7/12.8 MB 896.3 kB/s eta 0:00:12\n",
      "     --------                                2.9/12.8 MB 943.2 kB/s eta 0:00:11\n",
      "     ---------                               3.0/12.8 MB 958.5 kB/s eta 0:00:11\n",
      "     ---------                               3.1/12.8 MB 963.6 kB/s eta 0:00:11\n",
      "     ---------                               3.1/12.8 MB 963.6 kB/s eta 0:00:11\n",
      "     ---------                               3.2/12.8 MB 962.5 kB/s eta 0:00:11\n",
      "     ---------                               3.2/12.8 MB 969.0 kB/s eta 0:00:10\n",
      "     ---------                               3.2/12.8 MB 963.1 kB/s eta 0:00:10\n",
      "     ----------                              3.3/12.8 MB 966.3 kB/s eta 0:00:10\n",
      "     ----------                              3.3/12.8 MB 966.3 kB/s eta 0:00:10\n",
      "     ----------                              3.4/12.8 MB 972.7 kB/s eta 0:00:10\n",
      "     ----------                              3.4/12.8 MB 966.7 kB/s eta 0:00:10\n",
      "     ----------                              3.4/12.8 MB 966.7 kB/s eta 0:00:10\n",
      "     ----------                              3.4/12.8 MB 966.7 kB/s eta 0:00:10\n",
      "     ----------                              3.5/12.8 MB 970.3 kB/s eta 0:00:10\n",
      "     ----------                              3.6/12.8 MB 967.7 kB/s eta 0:00:10\n",
      "     ----------                              3.6/12.8 MB 960.9 kB/s eta 0:00:10\n",
      "     ----------                              3.6/12.8 MB 960.9 kB/s eta 0:00:10\n",
      "     ----------                              3.6/12.8 MB 960.9 kB/s eta 0:00:10\n",
      "     ----------                              3.6/12.8 MB 960.9 kB/s eta 0:00:10\n",
      "     -----------                             3.7/12.8 MB 952.7 kB/s eta 0:00:10\n",
      "     -----------                             3.7/12.8 MB 950.4 kB/s eta 0:00:10\n",
      "     -----------                             3.8/12.8 MB 946.9 kB/s eta 0:00:10\n",
      "     -----------                             3.8/12.8 MB 945.0 kB/s eta 0:00:10\n",
      "     -----------                             3.8/12.8 MB 945.0 kB/s eta 0:00:10\n",
      "     -----------                             3.9/12.8 MB 941.6 kB/s eta 0:00:10\n",
      "     -----------                             3.9/12.8 MB 941.6 kB/s eta 0:00:10\n",
      "     -----------                             3.9/12.8 MB 941.6 kB/s eta 0:00:10\n",
      "     -----------                             3.9/12.8 MB 941.6 kB/s eta 0:00:10\n",
      "     -----------                             3.9/12.8 MB 916.1 kB/s eta 0:00:10\n",
      "     -----------                             3.9/12.8 MB 916.1 kB/s eta 0:00:10\n",
      "     ------------                            4.0/12.8 MB 910.9 kB/s eta 0:00:10\n",
      "     ------------                            4.0/12.8 MB 904.8 kB/s eta 0:00:10\n",
      "     ------------                            4.0/12.8 MB 904.8 kB/s eta 0:00:10\n",
      "     ------------                            4.0/12.8 MB 904.8 kB/s eta 0:00:10\n",
      "     ------------                            4.0/12.8 MB 893.4 kB/s eta 0:00:10\n",
      "     ------------                            4.1/12.8 MB 891.8 kB/s eta 0:00:10\n",
      "     ------------                            4.1/12.8 MB 891.8 kB/s eta 0:00:10\n",
      "     ------------                            4.1/12.8 MB 891.8 kB/s eta 0:00:10\n",
      "     ------------                            4.1/12.8 MB 891.8 kB/s eta 0:00:10\n",
      "     ------------                            4.1/12.8 MB 891.8 kB/s eta 0:00:10\n",
      "     ------------                            4.2/12.8 MB 868.8 kB/s eta 0:00:10\n",
      "     ------------                            4.2/12.8 MB 868.8 kB/s eta 0:00:10\n",
      "     ------------                            4.2/12.8 MB 857.0 kB/s eta 0:00:11\n",
      "     ------------                            4.2/12.8 MB 853.0 kB/s eta 0:00:11\n",
      "     ------------                            4.2/12.8 MB 854.4 kB/s eta 0:00:11\n",
      "     ------------                            4.2/12.8 MB 851.1 kB/s eta 0:00:11\n",
      "     -------------                           4.3/12.8 MB 850.8 kB/s eta 0:00:11\n",
      "     -------------                           4.3/12.8 MB 849.5 kB/s eta 0:00:11\n",
      "     -------------                           4.3/12.8 MB 838.6 kB/s eta 0:00:11\n",
      "     -------------                           4.4/12.8 MB 842.1 kB/s eta 0:00:11\n",
      "     -------------                           4.4/12.8 MB 835.8 kB/s eta 0:00:11\n",
      "     -------------                           4.4/12.8 MB 834.2 kB/s eta 0:00:11\n",
      "     -------------                           4.4/12.8 MB 834.5 kB/s eta 0:00:11\n",
      "     -------------                           4.5/12.8 MB 830.6 kB/s eta 0:00:11\n",
      "     -------------                           4.5/12.8 MB 831.5 kB/s eta 0:00:10\n",
      "     -------------                           4.5/12.8 MB 831.9 kB/s eta 0:00:10\n",
      "     -------------                           4.6/12.8 MB 830.4 kB/s eta 0:00:10\n",
      "     --------------                          4.6/12.8 MB 828.9 kB/s eta 0:00:10\n",
      "     --------------                          4.6/12.8 MB 830.2 kB/s eta 0:00:10\n",
      "     --------------                          4.7/12.8 MB 828.3 kB/s eta 0:00:10\n",
      "     --------------                          4.7/12.8 MB 829.6 kB/s eta 0:00:10\n",
      "     --------------                          4.7/12.8 MB 829.6 kB/s eta 0:00:10\n",
      "     --------------                          4.7/12.8 MB 829.6 kB/s eta 0:00:10\n",
      "     --------------                          4.8/12.8 MB 817.9 kB/s eta 0:00:10\n",
      "     --------------                          4.8/12.8 MB 817.9 kB/s eta 0:00:10\n",
      "     --------------                          4.8/12.8 MB 811.8 kB/s eta 0:00:10\n",
      "     --------------                          4.8/12.8 MB 812.7 kB/s eta 0:00:10\n",
      "     --------------                          4.8/12.8 MB 812.7 kB/s eta 0:00:10\n",
      "     --------------                          4.9/12.8 MB 808.2 kB/s eta 0:00:10\n",
      "     --------------                          4.9/12.8 MB 808.2 kB/s eta 0:00:10\n",
      "     --------------                          4.9/12.8 MB 799.1 kB/s eta 0:00:10\n",
      "     ---------------                         4.9/12.8 MB 800.1 kB/s eta 0:00:10\n",
      "     ---------------                         5.0/12.8 MB 799.0 kB/s eta 0:00:10\n",
      "     ---------------                         5.0/12.8 MB 797.9 kB/s eta 0:00:10\n",
      "     ---------------                         5.0/12.8 MB 795.2 kB/s eta 0:00:10\n",
      "     ---------------                         5.1/12.8 MB 795.8 kB/s eta 0:00:10\n",
      "     ---------------                         5.1/12.8 MB 797.0 kB/s eta 0:00:10\n",
      "     ---------------                         5.1/12.8 MB 794.5 kB/s eta 0:00:10\n",
      "     ---------------                         5.1/12.8 MB 793.0 kB/s eta 0:00:10\n",
      "     ---------------                         5.2/12.8 MB 794.3 kB/s eta 0:00:10\n",
      "     ---------------                         5.2/12.8 MB 787.9 kB/s eta 0:00:10\n",
      "     ---------------                         5.2/12.8 MB 790.4 kB/s eta 0:00:10\n",
      "     ----------------                        5.3/12.8 MB 791.0 kB/s eta 0:00:10\n",
      "     ----------------                        5.3/12.8 MB 790.0 kB/s eta 0:00:10\n",
      "     ----------------                        5.3/12.8 MB 791.6 kB/s eta 0:00:10\n",
      "     ----------------                        5.3/12.8 MB 788.5 kB/s eta 0:00:10\n",
      "     ----------------                        5.4/12.8 MB 789.4 kB/s eta 0:00:10\n",
      "     ----------------                        5.4/12.8 MB 790.3 kB/s eta 0:00:10\n",
      "     ----------------                        5.4/12.8 MB 787.9 kB/s eta 0:00:10\n",
      "     ----------------                        5.5/12.8 MB 789.6 kB/s eta 0:00:10\n",
      "     ----------------                        5.5/12.8 MB 784.1 kB/s eta 0:00:10\n",
      "     ----------------                        5.5/12.8 MB 785.6 kB/s eta 0:00:10\n",
      "     ----------------                        5.5/12.8 MB 779.8 kB/s eta 0:00:10\n",
      "     ----------------                        5.6/12.8 MB 783.8 kB/s eta 0:00:10\n",
      "     -----------------                       5.6/12.8 MB 781.3 kB/s eta 0:00:10\n",
      "     -----------------                       5.6/12.8 MB 780.5 kB/s eta 0:00:10\n",
      "     -----------------                       5.6/12.8 MB 778.4 kB/s eta 0:00:10\n",
      "     -----------------                       5.7/12.8 MB 780.6 kB/s eta 0:00:10\n",
      "     -----------------                       5.7/12.8 MB 779.7 kB/s eta 0:00:10\n",
      "     -----------------                       5.7/12.8 MB 779.7 kB/s eta 0:00:10\n",
      "     -----------------                       5.8/12.8 MB 779.8 kB/s eta 0:00:10\n",
      "     -----------------                       5.8/12.8 MB 779.2 kB/s eta 0:00:09\n",
      "     -----------------                       5.8/12.8 MB 779.2 kB/s eta 0:00:09\n",
      "     -----------------                       5.9/12.8 MB 780.7 kB/s eta 0:00:09\n",
      "     -----------------                       5.9/12.8 MB 777.2 kB/s eta 0:00:09\n",
      "     ------------------                      5.9/12.8 MB 776.2 kB/s eta 0:00:09\n",
      "     ------------------                      5.9/12.8 MB 775.9 kB/s eta 0:00:09\n",
      "     ------------------                      6.0/12.8 MB 773.7 kB/s eta 0:00:09\n",
      "     ------------------                      6.0/12.8 MB 773.1 kB/s eta 0:00:09\n",
      "     ------------------                      6.0/12.8 MB 775.1 kB/s eta 0:00:09\n",
      "     ------------------                      6.0/12.8 MB 775.1 kB/s eta 0:00:09\n",
      "     ------------------                      6.1/12.8 MB 773.7 kB/s eta 0:00:09\n",
      "     ------------------                      6.1/12.8 MB 774.7 kB/s eta 0:00:09\n",
      "     ------------------                      6.2/12.8 MB 774.0 kB/s eta 0:00:09\n",
      "     ------------------                      6.2/12.8 MB 773.1 kB/s eta 0:00:09\n",
      "     ------------------                      6.2/12.8 MB 773.7 kB/s eta 0:00:09\n",
      "     -------------------                     6.3/12.8 MB 773.0 kB/s eta 0:00:09\n",
      "     -------------------                     6.3/12.8 MB 771.3 kB/s eta 0:00:09\n",
      "     -------------------                     6.3/12.8 MB 773.1 kB/s eta 0:00:09\n",
      "     -------------------                     6.3/12.8 MB 772.7 kB/s eta 0:00:09\n",
      "     -------------------                     6.4/12.8 MB 771.0 kB/s eta 0:00:09\n",
      "     -------------------                     6.4/12.8 MB 771.0 kB/s eta 0:00:09\n",
      "     -------------------                     6.4/12.8 MB 771.0 kB/s eta 0:00:09\n",
      "     -------------------                     6.4/12.8 MB 764.2 kB/s eta 0:00:09\n",
      "     -------------------                     6.4/12.8 MB 765.2 kB/s eta 0:00:09\n",
      "     -------------------                     6.5/12.8 MB 763.3 kB/s eta 0:00:09\n",
      "     -------------------                     6.5/12.8 MB 761.4 kB/s eta 0:00:09\n",
      "     -------------------                     6.5/12.8 MB 761.0 kB/s eta 0:00:09\n",
      "     -------------------                     6.5/12.8 MB 760.4 kB/s eta 0:00:09\n",
      "     -------------------                     6.6/12.8 MB 758.4 kB/s eta 0:00:09\n",
      "     --------------------                    6.6/12.8 MB 757.9 kB/s eta 0:00:09\n",
      "     --------------------                    6.6/12.8 MB 757.9 kB/s eta 0:00:09\n",
      "     --------------------                    6.6/12.8 MB 758.3 kB/s eta 0:00:09\n",
      "     --------------------                    6.7/12.8 MB 756.6 kB/s eta 0:00:09\n",
      "     --------------------                    6.7/12.8 MB 755.9 kB/s eta 0:00:09\n",
      "     --------------------                    6.7/12.8 MB 755.2 kB/s eta 0:00:09\n",
      "     --------------------                    6.8/12.8 MB 756.4 kB/s eta 0:00:08\n",
      "     --------------------                    6.8/12.8 MB 757.8 kB/s eta 0:00:08\n",
      "     --------------------                    6.8/12.8 MB 757.8 kB/s eta 0:00:08\n",
      "     --------------------                    6.9/12.8 MB 755.8 kB/s eta 0:00:08\n",
      "     ---------------------                   6.9/12.8 MB 757.6 kB/s eta 0:00:08\n",
      "     ---------------------                   6.9/12.8 MB 757.2 kB/s eta 0:00:08\n",
      "     ---------------------                   7.0/12.8 MB 757.9 kB/s eta 0:00:08\n",
      "     ---------------------                   7.0/12.8 MB 757.3 kB/s eta 0:00:08\n",
      "     ---------------------                   7.0/12.8 MB 755.6 kB/s eta 0:00:08\n",
      "     ---------------------                   7.1/12.8 MB 755.1 kB/s eta 0:00:08\n",
      "     ---------------------                   7.1/12.8 MB 756.0 kB/s eta 0:00:08\n",
      "     ---------------------                   7.1/12.8 MB 755.3 kB/s eta 0:00:08\n",
      "     ---------------------                   7.2/12.8 MB 757.2 kB/s eta 0:00:08\n",
      "     ---------------------                   7.2/12.8 MB 756.5 kB/s eta 0:00:08\n",
      "     ----------------------                  7.2/12.8 MB 758.3 kB/s eta 0:00:08\n",
      "     ----------------------                  7.2/12.8 MB 758.3 kB/s eta 0:00:08\n",
      "     ----------------------                  7.2/12.8 MB 758.3 kB/s eta 0:00:08\n",
      "     ----------------------                  7.3/12.8 MB 757.0 kB/s eta 0:00:08\n",
      "     ----------------------                  7.3/12.8 MB 757.0 kB/s eta 0:00:08\n",
      "     ----------------------                  7.4/12.8 MB 752.8 kB/s eta 0:00:08\n",
      "     ----------------------                  7.4/12.8 MB 753.6 kB/s eta 0:00:08\n",
      "     ----------------------                  7.4/12.8 MB 754.4 kB/s eta 0:00:08\n",
      "     ----------------------                  7.5/12.8 MB 754.6 kB/s eta 0:00:08\n",
      "     ----------------------                  7.5/12.8 MB 756.7 kB/s eta 0:00:08\n",
      "     ----------------------                  7.5/12.8 MB 753.8 kB/s eta 0:00:07\n",
      "     -----------------------                 7.6/12.8 MB 755.4 kB/s eta 0:00:07\n",
      "     -----------------------                 7.6/12.8 MB 759.3 kB/s eta 0:00:07\n",
      "     -----------------------                 7.7/12.8 MB 757.6 kB/s eta 0:00:07\n",
      "     -----------------------                 7.7/12.8 MB 757.6 kB/s eta 0:00:07\n",
      "     -----------------------                 7.7/12.8 MB 757.6 kB/s eta 0:00:07\n",
      "     -----------------------                 7.7/12.8 MB 757.6 kB/s eta 0:00:07\n",
      "     -----------------------                 7.8/12.8 MB 760.1 kB/s eta 0:00:07\n",
      "     -----------------------                 7.8/12.8 MB 760.1 kB/s eta 0:00:07\n",
      "     -----------------------                 7.8/12.8 MB 756.1 kB/s eta 0:00:07\n",
      "     ------------------------                7.9/12.8 MB 756.6 kB/s eta 0:00:07\n",
      "     ------------------------                7.9/12.8 MB 756.3 kB/s eta 0:00:07\n",
      "     ------------------------                7.9/12.8 MB 755.8 kB/s eta 0:00:07\n",
      "     ------------------------                8.0/12.8 MB 754.4 kB/s eta 0:00:07\n",
      "     ------------------------                8.0/12.8 MB 754.4 kB/s eta 0:00:07\n",
      "     ------------------------                8.0/12.8 MB 751.7 kB/s eta 0:00:07\n",
      "     ------------------------                8.0/12.8 MB 751.6 kB/s eta 0:00:07\n",
      "     ------------------------                8.0/12.8 MB 748.0 kB/s eta 0:00:07\n",
      "     ------------------------                8.1/12.8 MB 749.4 kB/s eta 0:00:07\n",
      "     ------------------------                8.1/12.8 MB 749.4 kB/s eta 0:00:07\n",
      "     ------------------------                8.1/12.8 MB 748.8 kB/s eta 0:00:07\n",
      "     ------------------------                8.1/12.8 MB 748.8 kB/s eta 0:00:07\n",
      "     ------------------------                8.2/12.8 MB 747.1 kB/s eta 0:00:07\n",
      "     ------------------------                8.2/12.8 MB 747.9 kB/s eta 0:00:07\n",
      "     -------------------------               8.2/12.8 MB 745.5 kB/s eta 0:00:07\n",
      "     -------------------------               8.3/12.8 MB 745.1 kB/s eta 0:00:07\n",
      "     -------------------------               8.3/12.8 MB 745.8 kB/s eta 0:00:07\n",
      "     -------------------------               8.3/12.8 MB 745.8 kB/s eta 0:00:07\n",
      "     -------------------------               8.3/12.8 MB 745.0 kB/s eta 0:00:06\n",
      "     -------------------------               8.4/12.8 MB 745.6 kB/s eta 0:00:06\n",
      "     -------------------------               8.4/12.8 MB 745.6 kB/s eta 0:00:06\n",
      "     -------------------------               8.4/12.8 MB 744.2 kB/s eta 0:00:06\n",
      "     -------------------------               8.5/12.8 MB 744.7 kB/s eta 0:00:06\n",
      "     -------------------------               8.5/12.8 MB 745.2 kB/s eta 0:00:06\n",
      "     -------------------------               8.5/12.8 MB 746.0 kB/s eta 0:00:06\n",
      "     --------------------------              8.5/12.8 MB 744.8 kB/s eta 0:00:06\n",
      "     --------------------------              8.5/12.8 MB 744.8 kB/s eta 0:00:06\n",
      "     --------------------------              8.6/12.8 MB 744.8 kB/s eta 0:00:06\n",
      "     --------------------------              8.6/12.8 MB 744.8 kB/s eta 0:00:06\n",
      "     --------------------------              8.7/12.8 MB 742.4 kB/s eta 0:00:06\n",
      "     --------------------------              8.7/12.8 MB 742.9 kB/s eta 0:00:06\n",
      "     --------------------------              8.7/12.8 MB 743.7 kB/s eta 0:00:06\n",
      "     --------------------------              8.8/12.8 MB 744.1 kB/s eta 0:00:06\n",
      "     --------------------------              8.8/12.8 MB 743.8 kB/s eta 0:00:06\n",
      "     --------------------------              8.8/12.8 MB 742.5 kB/s eta 0:00:06\n",
      "     --------------------------              8.8/12.8 MB 742.5 kB/s eta 0:00:06\n",
      "     --------------------------              8.8/12.8 MB 742.5 kB/s eta 0:00:06\n",
      "     ---------------------------             8.9/12.8 MB 741.5 kB/s eta 0:00:06\n",
      "     ---------------------------             8.9/12.8 MB 741.5 kB/s eta 0:00:06\n",
      "     ---------------------------             8.9/12.8 MB 741.5 kB/s eta 0:00:06\n",
      "     ---------------------------             8.9/12.8 MB 741.5 kB/s eta 0:00:06\n",
      "     ---------------------------             8.9/12.8 MB 741.5 kB/s eta 0:00:06\n",
      "     ---------------------------             9.0/12.8 MB 735.0 kB/s eta 0:00:06\n",
      "     ---------------------------             9.0/12.8 MB 733.8 kB/s eta 0:00:06\n",
      "     ---------------------------             9.0/12.8 MB 734.5 kB/s eta 0:00:06\n",
      "     ---------------------------             9.1/12.8 MB 733.2 kB/s eta 0:00:06\n",
      "     ---------------------------             9.1/12.8 MB 734.6 kB/s eta 0:00:06\n",
      "     ---------------------------             9.2/12.8 MB 735.2 kB/s eta 0:00:05\n",
      "     ---------------------------             9.2/12.8 MB 734.1 kB/s eta 0:00:05\n",
      "     ----------------------------            9.2/12.8 MB 734.6 kB/s eta 0:00:05\n",
      "     ----------------------------            9.2/12.8 MB 735.3 kB/s eta 0:00:05\n",
      "     ----------------------------            9.3/12.8 MB 735.0 kB/s eta 0:00:05\n",
      "     ----------------------------            9.3/12.8 MB 735.0 kB/s eta 0:00:05\n",
      "     ----------------------------            9.3/12.8 MB 735.0 kB/s eta 0:00:05\n",
      "     ----------------------------            9.4/12.8 MB 736.7 kB/s eta 0:00:05\n",
      "     ----------------------------            9.4/12.8 MB 736.7 kB/s eta 0:00:05\n",
      "     ----------------------------            9.4/12.8 MB 736.7 kB/s eta 0:00:05\n",
      "     ----------------------------            9.4/12.8 MB 736.7 kB/s eta 0:00:05\n",
      "     ----------------------------            9.4/12.8 MB 731.6 kB/s eta 0:00:05\n",
      "     ----------------------------            9.4/12.8 MB 731.6 kB/s eta 0:00:05\n",
      "     ----------------------------            9.5/12.8 MB 728.6 kB/s eta 0:00:05\n",
      "     ----------------------------            9.5/12.8 MB 729.2 kB/s eta 0:00:05\n",
      "     -----------------------------           9.5/12.8 MB 729.0 kB/s eta 0:00:05\n",
      "     -----------------------------           9.6/12.8 MB 729.4 kB/s eta 0:00:05\n",
      "     -----------------------------           9.6/12.8 MB 728.4 kB/s eta 0:00:05\n",
      "     -----------------------------           9.6/12.8 MB 729.1 kB/s eta 0:00:05\n",
      "     -----------------------------           9.6/12.8 MB 729.1 kB/s eta 0:00:05\n",
      "     -----------------------------           9.7/12.8 MB 727.7 kB/s eta 0:00:05\n",
      "     -----------------------------           9.7/12.8 MB 727.7 kB/s eta 0:00:05\n",
      "     -----------------------------           9.7/12.8 MB 725.0 kB/s eta 0:00:05\n",
      "     -----------------------------           9.7/12.8 MB 725.0 kB/s eta 0:00:05\n",
      "     -----------------------------           9.8/12.8 MB 723.7 kB/s eta 0:00:05\n",
      "     -----------------------------           9.8/12.8 MB 721.9 kB/s eta 0:00:05\n",
      "     -----------------------------           9.8/12.8 MB 721.6 kB/s eta 0:00:05\n",
      "     -----------------------------           9.8/12.8 MB 721.6 kB/s eta 0:00:05\n",
      "     ------------------------------          9.9/12.8 MB 722.0 kB/s eta 0:00:05\n",
      "     ------------------------------          9.9/12.8 MB 722.7 kB/s eta 0:00:05\n",
      "     ------------------------------          9.9/12.8 MB 721.8 kB/s eta 0:00:05\n",
      "     -----------------------------          10.0/12.8 MB 723.7 kB/s eta 0:00:04\n",
      "     -----------------------------          10.0/12.8 MB 722.8 kB/s eta 0:00:04\n",
      "     -----------------------------          10.0/12.8 MB 722.4 kB/s eta 0:00:04\n",
      "     -----------------------------          10.1/12.8 MB 723.9 kB/s eta 0:00:04\n",
      "     -----------------------------          10.1/12.8 MB 723.9 kB/s eta 0:00:04\n",
      "     ------------------------------         10.2/12.8 MB 723.2 kB/s eta 0:00:04\n",
      "     ------------------------------         10.2/12.8 MB 723.2 kB/s eta 0:00:04\n",
      "     ------------------------------         10.2/12.8 MB 721.4 kB/s eta 0:00:04\n",
      "     ------------------------------         10.2/12.8 MB 722.7 kB/s eta 0:00:04\n",
      "     ------------------------------         10.3/12.8 MB 725.9 kB/s eta 0:00:04\n",
      "     ------------------------------         10.3/12.8 MB 725.8 kB/s eta 0:00:04\n",
      "     ------------------------------         10.4/12.8 MB 725.9 kB/s eta 0:00:04\n",
      "     ------------------------------         10.4/12.8 MB 729.9 kB/s eta 0:00:04\n",
      "     ------------------------------         10.4/12.8 MB 727.4 kB/s eta 0:00:04\n",
      "     ------------------------------         10.4/12.8 MB 727.4 kB/s eta 0:00:04\n",
      "     ------------------------------         10.4/12.8 MB 727.4 kB/s eta 0:00:04\n",
      "     ------------------------------         10.4/12.8 MB 721.8 kB/s eta 0:00:04\n",
      "     -------------------------------        10.5/12.8 MB 719.5 kB/s eta 0:00:04\n",
      "     -------------------------------        10.5/12.8 MB 719.5 kB/s eta 0:00:04\n",
      "     -------------------------------        10.5/12.8 MB 716.3 kB/s eta 0:00:04\n",
      "     -------------------------------        10.5/12.8 MB 714.0 kB/s eta 0:00:04\n",
      "     -------------------------------        10.5/12.8 MB 714.0 kB/s eta 0:00:04\n",
      "     -------------------------------        10.6/12.8 MB 712.4 kB/s eta 0:00:04\n",
      "     -------------------------------        10.6/12.8 MB 715.5 kB/s eta 0:00:04\n",
      "     -------------------------------        10.6/12.8 MB 712.4 kB/s eta 0:00:04\n",
      "     -------------------------------        10.7/12.8 MB 713.2 kB/s eta 0:00:03\n",
      "     -------------------------------        10.7/12.8 MB 713.2 kB/s eta 0:00:03\n",
      "     -------------------------------        10.7/12.8 MB 711.6 kB/s eta 0:00:03\n",
      "     -------------------------------        10.8/12.8 MB 710.1 kB/s eta 0:00:03\n",
      "     --------------------------------       10.8/12.8 MB 711.6 kB/s eta 0:00:03\n",
      "     --------------------------------       10.8/12.8 MB 711.6 kB/s eta 0:00:03\n",
      "     --------------------------------       10.9/12.8 MB 708.5 kB/s eta 0:00:03\n",
      "     --------------------------------       10.9/12.8 MB 708.5 kB/s eta 0:00:03\n",
      "     --------------------------------       10.9/12.8 MB 707.8 kB/s eta 0:00:03\n",
      "     --------------------------------       11.0/12.8 MB 707.0 kB/s eta 0:00:03\n",
      "     --------------------------------       11.0/12.8 MB 705.5 kB/s eta 0:00:03\n",
      "     --------------------------------       11.0/12.8 MB 707.0 kB/s eta 0:00:03\n",
      "     --------------------------------       11.1/12.8 MB 705.5 kB/s eta 0:00:03\n",
      "     --------------------------------       11.1/12.8 MB 704.8 kB/s eta 0:00:03\n",
      "     --------------------------------       11.1/12.8 MB 707.0 kB/s eta 0:00:03\n",
      "     ---------------------------------      11.2/12.8 MB 704.8 kB/s eta 0:00:03\n",
      "     ---------------------------------      11.2/12.8 MB 708.5 kB/s eta 0:00:03\n",
      "     ---------------------------------      11.2/12.8 MB 708.5 kB/s eta 0:00:03\n",
      "     ---------------------------------      11.3/12.8 MB 708.6 kB/s eta 0:00:03\n",
      "     ---------------------------------      11.3/12.8 MB 710.1 kB/s eta 0:00:03\n",
      "     ---------------------------------      11.3/12.8 MB 708.5 kB/s eta 0:00:03\n",
      "     ---------------------------------      11.4/12.8 MB 709.4 kB/s eta 0:00:03\n",
      "     ---------------------------------      11.4/12.8 MB 707.8 kB/s eta 0:00:02\n",
      "     ---------------------------------      11.4/12.8 MB 707.8 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.5/12.8 MB 711.6 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.5/12.8 MB 710.1 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.5/12.8 MB 710.1 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.6/12.8 MB 710.1 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.6/12.8 MB 710.1 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.6/12.8 MB 707.0 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.7/12.8 MB 704.7 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.7/12.8 MB 704.0 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.7/12.8 MB 704.0 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.7/12.8 MB 704.0 kB/s eta 0:00:02\n",
      "     ----------------------------------     11.7/12.8 MB 704.0 kB/s eta 0:00:02\n",
      "     -----------------------------------    11.8/12.8 MB 704.0 kB/s eta 0:00:02\n",
      "     -----------------------------------    11.8/12.8 MB 700.2 kB/s eta 0:00:02\n",
      "     -----------------------------------    11.8/12.8 MB 698.7 kB/s eta 0:00:02\n",
      "     -----------------------------------    11.9/12.8 MB 702.5 kB/s eta 0:00:02\n",
      "     -----------------------------------    11.9/12.8 MB 701.0 kB/s eta 0:00:02\n",
      "     -----------------------------------    11.9/12.8 MB 699.5 kB/s eta 0:00:02\n",
      "     -----------------------------------    12.0/12.8 MB 702.5 kB/s eta 0:00:02\n",
      "     -----------------------------------    12.0/12.8 MB 705.5 kB/s eta 0:00:02\n",
      "     -----------------------------------    12.1/12.8 MB 703.2 kB/s eta 0:00:02\n",
      "     -----------------------------------    12.1/12.8 MB 701.0 kB/s eta 0:00:02\n",
      "     ------------------------------------   12.2/12.8 MB 704.0 kB/s eta 0:00:01\n",
      "     ------------------------------------   12.2/12.8 MB 714.0 kB/s eta 0:00:01\n",
      "     ------------------------------------   12.2/12.8 MB 713.2 kB/s eta 0:00:01\n",
      "     ------------------------------------   12.3/12.8 MB 710.9 kB/s eta 0:00:01\n",
      "     ------------------------------------   12.3/12.8 MB 709.3 kB/s eta 0:00:01\n",
      "     ------------------------------------   12.3/12.8 MB 707.1 kB/s eta 0:00:01\n",
      "     ------------------------------------   12.4/12.8 MB 704.7 kB/s eta 0:00:01\n",
      "     ------------------------------------   12.5/12.8 MB 702.4 kB/s eta 0:00:01\n",
      "     ------------------------------------   12.5/12.8 MB 702.4 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.5/12.8 MB 704.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.5/12.8 MB 704.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.5/12.8 MB 704.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.5/12.8 MB 704.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 698.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 698.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 698.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.7/12.8 MB 695.8 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.7/12.8 MB 695.0 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.7/12.8 MB 695.0 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.8/12.8 MB 690.6 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.8/12.8 MB 690.6 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.8/12.8 MB 688.5 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.8/12.8 MB 687.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 12.8/12.8 MB 685.3 kB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b0082ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parsing completed. Results saved in: 25-3-2025_14_3/fourgram_dependency_parsing.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fourgram</th>\n",
       "      <th>Word</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Head</th>\n",
       "      <th>Head_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male female male female</td>\n",
       "      <td>male</td>\n",
       "      <td>amod</td>\n",
       "      <td>female</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male female male female</td>\n",
       "      <td>female</td>\n",
       "      <td>amod</td>\n",
       "      <td>female</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male female male female</td>\n",
       "      <td>male</td>\n",
       "      <td>amod</td>\n",
       "      <td>female</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male female male female</td>\n",
       "      <td>female</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>female</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cancer cancer journal clinicians</td>\n",
       "      <td>cancer</td>\n",
       "      <td>compound</td>\n",
       "      <td>cancer</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Fourgram    Word Dependency    Head Head_POS\n",
       "0           male female male female    male       amod  female     NOUN\n",
       "1           male female male female  female       amod  female     NOUN\n",
       "2           male female male female    male       amod  female     NOUN\n",
       "3           male female male female  female       ROOT  female     NOUN\n",
       "4  cancer cancer journal clinicians  cancer   compound  cancer     NOUN"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "output_folder = \"25-3-2025_14_3/\"\n",
    "input_file = os.path.join(output_folder, \"fourgram_cooccurrence.csv\")\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"{input_file} not found!\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def analyze_dependency_structure(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.dep_, token.head.text, token.head.pos_) for token in doc]\n",
    "\n",
    "df = pd.read_csv(input_file, index_col=0)\n",
    "\n",
    "dependency_data = []\n",
    "for fourgram in df.index:\n",
    "    dependencies = analyze_dependency_structure(fourgram)\n",
    "    for word, dep, head, head_pos in dependencies:\n",
    "        dependency_data.append([fourgram, word, dep, head, head_pos])\n",
    "\n",
    "dependency_df = pd.DataFrame(dependency_data, columns=[\"Fourgram\", \"Word\", \"Dependency\", \"Head\", \"Head_POS\"])\n",
    "output_file = os.path.join(output_folder, \"fourgram_dependency_parsing.csv\")\n",
    "dependency_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Dependency parsing completed. Results saved in:\", output_file)\n",
    "dependency_df.head()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f6f037ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVO extraction completed. Results saved in: 25-3-2025_14_3/fourgram_SVO_triplets.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fourgram</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>incidence rates selected cancers</td>\n",
       "      <td>rates</td>\n",
       "      <td>selected</td>\n",
       "      <td>cancers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>estimates not add rounding</td>\n",
       "      <td>estimates</td>\n",
       "      <td>add</td>\n",
       "      <td>rounding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rates selected cancers sex</td>\n",
       "      <td>rates</td>\n",
       "      <td>selected</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>injuries accidents tional injuries</td>\n",
       "      <td>injuries</td>\n",
       "      <td>accidents</td>\n",
       "      <td>injuries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rates selected cancers race</td>\n",
       "      <td>rates</td>\n",
       "      <td>selected</td>\n",
       "      <td>race</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Fourgram    Subject       Verb    Object\n",
       "0    incidence rates selected cancers      rates   selected   cancers\n",
       "1          estimates not add rounding  estimates        add  rounding\n",
       "2          rates selected cancers sex      rates   selected       sex\n",
       "3  injuries accidents tional injuries   injuries  accidents  injuries\n",
       "4         rates selected cancers race      rates   selected      race"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "output_folder = \"25-3-2025_14_3/\"\n",
    "input_file = os.path.join(output_folder, \"fourgram_dependency_parsing.csv\")\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"{input_file} not found!\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_SVO(text):\n",
    "    doc = nlp(text)\n",
    "    subject, verb, obj = None, None, None\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "            subject = token.text\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            verb = token.text\n",
    "        if token.dep_ in {\"dobj\", \"pobj\"}:\n",
    "            obj = token.text\n",
    "            \n",
    "    return (subject, verb, obj) if subject and verb and obj else None\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "svos = []\n",
    "\n",
    "for fourgram in df[\"Fourgram\"].unique():\n",
    "    svo_triplet = extract_SVO(fourgram)\n",
    "    if svo_triplet:\n",
    "        svos.append([fourgram] + list(svo_triplet))\n",
    "\n",
    "svo_df = pd.DataFrame(svos, columns=[\"Fourgram\", \"Subject\", \"Verb\", \"Object\"])\n",
    "output_file = os.path.join(output_folder, \"fourgram_SVO_triplets.csv\")\n",
    "svo_df.to_csv(output_file, index=False) \n",
    "print(\"SVO extraction completed. Results saved in:\", output_file)\n",
    "svo_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9e3190ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_svo(text):\n",
    "    doc = nlp(text)\n",
    "    svos = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"ROOT\", \"xcomp\"} and token.pos_ == \"VERB\":\n",
    "            subject = [w.text for w in token.lefts if w.dep_ in {\"nsubj\", \"nsubjpass\"}]\n",
    "            objects = [w.text for w in token.rights if w.dep_ in {\"dobj\", \"pobj\", \"attr\"}]\n",
    "            if subject and objects:\n",
    "                svos.append((subject[0], token.text, objects[0]))\n",
    "    return svos\n",
    "\n",
    "def build_relation_mapping(fourgram_file, output_file):\n",
    "    df = pd.read_csv(fourgram_file, index_col=0)\n",
    "    relations = []\n",
    "    \n",
    "    for fourgram in df.index:\n",
    "        text = fourgram.replace(\"_\", \" \")\n",
    "        svo_triplets = extract_svo(text)\n",
    "        \n",
    "        for subj, verb, obj in svo_triplets:\n",
    "            relations.append({\"Fourgram\": fourgram, \"Subject\": subj, \"Verb\": verb, \"Object\": obj})\n",
    "\n",
    "    relation_df = pd.DataFrame(relations)\n",
    "    relation_df.to_csv(output_file, index=False)\n",
    "\n",
    "build_relation_mapping(\"25-3-2025_14_3/fourgram_cooccurrence.csv\", \"25-3-2025_14_3/relation_mapping.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be639f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parsing completed. Results saved in: 25-3-2025_14_3/fourgram_dependency_parsing.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "output_folder = \"25-3-2025_14_3/\"\n",
    "input_file = os.path.join(output_folder, \"fourgram_cooccurrence.csv\")\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"{input_file} not found!\")\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def analyze_dependency_structure(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.dep_, token.head.text, token.head.pos_) for token in doc]\n",
    "\n",
    "df = pd.read_csv(input_file, index_col=0)\n",
    "\n",
    "dependency_dict = {}\n",
    "\n",
    "for fourgram in df.index:\n",
    "    dependencies = analyze_dependency_structure(fourgram)\n",
    "    \n",
    "    dependency_dict[fourgram] = [\n",
    "        {\"word\": word, \"dependency\": dep, \"head\": head, \"head_pos\": head_pos}\n",
    "        for word, dep, head, head_pos in dependencies\n",
    "    ]\n",
    "\n",
    "output_file = os.path.join(output_folder, \"fourgram_dependency_parsing.json\")\n",
    "\n",
    "with open(output_file, \"w\") as json_file:\n",
    "    json.dump(dependency_dict, json_file, indent=4)\n",
    "\n",
    "print(\"Dependency parsing completed. Results saved in:\", output_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c9f8374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations extracted and saved to 25-3-2025_14_3/relations_extracted.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "output_folder = \"25-3-2025_14_3/\"\n",
    "input_json = os.path.join(output_folder, \"fourgram_dependency_parsing.json\")\n",
    "output_csv = os.path.join(output_folder, \"relations_extracted.csv\")\n",
    "\n",
    "if not os.path.exists(input_json):\n",
    "    raise FileNotFoundError(f\"{input_json} not found!\")\n",
    "\n",
    "def extract_relations_from_json(input_json, output_csv):\n",
    "    with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    relations = []\n",
    "\n",
    "    for fourgram, dependencies in data.items():\n",
    "        subject, verb, obj = None, None, None\n",
    "        possessor, modifier, preposition = None, None, None\n",
    "\n",
    "        for dep in dependencies:\n",
    "            word, dep_type, head, head_pos = dep[\"word\"], dep[\"dependency\"], dep[\"head\"], dep[\"head_pos\"]\n",
    "\n",
    "            if dep_type == \"nsubj\":\n",
    "                subject = word\n",
    "            elif dep_type == \"ROOT\":\n",
    "                verb = word\n",
    "            elif dep_type in (\"dobj\", \"obj\"):\n",
    "                obj = word\n",
    "            elif dep_type == \"poss\":\n",
    "                possessor = (word, head)\n",
    "            elif dep_type == \"amod\":\n",
    "                modifier = (word, head)\n",
    "            elif dep_type == \"prep\":\n",
    "                preposition = (word, head)\n",
    "\n",
    "        if subject and verb and obj:\n",
    "            relations.append((fourgram, \"SVO\", subject, verb, obj))\n",
    "\n",
    "        if possessor:\n",
    "            relations.append((fourgram, \"Possessive\", possessor[0], \"owns\", possessor[1]))\n",
    "\n",
    "        if modifier:\n",
    "            relations.append((fourgram, \"Modifier\", modifier[0], \"describes\", modifier[1]))\n",
    "\n",
    "        if preposition:\n",
    "            relations.append((fourgram, \"Preposition\", preposition[0], \"related to\", preposition[1]))\n",
    "\n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Fourgram\", \"Relation Type\", \"Entity1\", \"Relation\", \"Entity2\"])\n",
    "        writer.writerows(relations)\n",
    "\n",
    "    print(f\"Relations extracted and saved to {output_csv}\")\n",
    "\n",
    "extract_relations_from_json(input_json, output_csv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b0e724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py2neo\n",
      "  Downloading py2neo-2021.2.4-py2.py3-none-any.whl (177 kB)\n",
      "                                              0.0/177.2 kB ? eta -:--:--\n",
      "                                              0.0/177.2 kB ? eta -:--:--\n",
      "     ---------                                41.0/177.2 kB ? eta -:--:--\n",
      "     -----------                             51.2/177.2 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------                        71.7/177.2 kB 660.6 kB/s eta 0:00:01\n",
      "     --------------------                 102.4/177.2 kB 590.8 kB/s eta 0:00:01\n",
      "     --------------------                 102.4/177.2 kB 590.8 kB/s eta 0:00:01\n",
      "     --------------------                 102.4/177.2 kB 590.8 kB/s eta 0:00:01\n",
      "     ---------------------------          133.1/177.2 kB 437.3 kB/s eta 0:00:01\n",
      "     -------------------------------      153.6/177.2 kB 482.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 177.2/177.2 kB 445.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\soura\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\soura\\anaconda3\\lib\\site-packages (from py2neo) (2023.7.22)\n",
      "Collecting interchange~=2021.0.4 (from py2neo)\n",
      "  Downloading interchange-2021.0.4-py2.py3-none-any.whl (28 kB)\n",
      "Collecting monotonic (from py2neo)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\soura\\anaconda3\\lib\\site-packages (from py2neo) (24.1)\n",
      "Collecting pansi>=2020.7.3 (from py2neo)\n",
      "  Downloading pansi-2024.11.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pygments>=2.0.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from py2neo) (2.15.1)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from py2neo) (1.16.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from py2neo) (1.26.16)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\soura\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\soura\\anaconda3\\lib\\site-packages (from pansi>=2020.7.3->py2neo) (9.4.0)\n",
      "Installing collected packages: monotonic, pansi, interchange, py2neo\n",
      "Successfully installed interchange-2021.0.4 monotonic-1.6 pansi-2024.11.0 py2neo-2021.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install py2neo pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83519e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neo4j\n",
      "  Downloading neo4j-5.28.1-py3-none-any.whl (312 kB)\n",
      "                                              0.0/312.3 kB ? eta -:--:--\n",
      "     -------------                          112.6/312.3 kB 3.3 MB/s eta 0:00:01\n",
      "     --------------                         122.9/312.3 kB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------      276.5/312.3 kB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  307.2/312.3 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 312.3/312.3 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz in c:\\users\\soura\\anaconda3\\lib\\site-packages (from neo4j) (2022.7)\n",
      "Installing collected packages: neo4j\n",
      "Successfully installed neo4j-5.28.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install neo4j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe8d75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Fourgram    Word Dependency    Head Head_POS\n",
      "0           male female male female    male       amod  female     NOUN\n",
      "1           male female male female  female       amod  female     NOUN\n",
      "2           male female male female    male       amod  female     NOUN\n",
      "3           male female male female  female       ROOT  female     NOUN\n",
      "4  cancer cancer journal clinicians  cancer   compound  cancer     NOUN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_folder = \"25-3-2025_14_3/\"\n",
    "input_csv = os.path.join(output_folder, \"fourgram_dependency_parsing.csv\")\n",
    "\n",
    "df = pd.read_csv(input_csv)\n",
    "print(df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a63ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rates', 'selected', 'cancers'), ('estimates', 'add', 'rounding'), ('rates', 'selected', 'sex'), ('injuries', 'accidents', 'injuries'), ('rates', 'selected', 'race')]\n"
     ]
    }
   ],
   "source": [
    "def extract_relations(df):\n",
    "    relations = []\n",
    "    \n",
    "    for fourgram in df[\"Fourgram\"].unique():\n",
    "        subset = df[df[\"Fourgram\"] == fourgram]\n",
    "        \n",
    "        subject, verb, obj = None, None, None\n",
    "\n",
    "        for _, row in subset.iterrows():\n",
    "            word, dep, head = row[\"Word\"], row[\"Dependency\"], row[\"Head\"]\n",
    "\n",
    "            if dep == \"nsubj\": \n",
    "                subject = word\n",
    "            elif dep == \"ROOT\": \n",
    "                verb = word\n",
    "            elif dep in (\"dobj\", \"obj\"): \n",
    "                obj = word\n",
    "\n",
    "        if subject and verb and obj:\n",
    "            relations.append((subject, verb, obj))\n",
    "\n",
    "    return relations\n",
    "\n",
    "relations = extract_relations(df)\n",
    "print(relations[:5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ffbf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully connected to Neo4j!\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "URI = \"neo4j+s://79977030.databases.neo4j.io\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD=\"whEzm2r-M-_QklgGy-fKMwfcRAyZcixTHlxjiU19xwk\"\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def run_query(self, query, parameters={}):\n",
    "        with self._driver.session() as session:\n",
    "            return session.run(query, parameters)\n",
    "\n",
    "# Connect to Neo4j\n",
    "conn = Neo4jConnection(URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "print(\" Successfully connected to Neo4j!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "103422d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Relations stored successfully in Neo4j!\n"
     ]
    }
   ],
   "source": [
    "def store_relations_in_neo4j(conn, relations):\n",
    "    query = \"\"\"\n",
    "    MERGE (s:Entity {name: $subject})\n",
    "    MERGE (v:Action {name: $verb})\n",
    "    MERGE (o:Entity {name: $object})\n",
    "    MERGE (s)-[:PERFORMS]->(v)\n",
    "    MERGE (v)-[:AFFECTS]->(o)\n",
    "    \"\"\"\n",
    "    \n",
    "    for subject, verb, obj in relations:\n",
    "        conn.run_query(query, {\"subject\": subject, \"verb\": verb, \"object\": obj})\n",
    "\n",
    "    print(\" Relations stored successfully in Neo4j!\")\n",
    "\n",
    "store_relations_in_neo4j(conn, relations)\n",
    "conn.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf571e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Define the connection parameters for Neo4j\n",
    "uri = \"bolt://localhost:7687\"  # Neo4j URI (replace if using Neo4j Aura or another URI)\n",
    "user = \"neo4j\"  # Neo4j Username (default is \"neo4j\")\n",
    "password = \"Sourabh@123\"  # Replace with your Neo4j password\n",
    "\n",
    "# Create a connection to Neo4j\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self._uri = uri\n",
    "        self._user = user\n",
    "        self._password = pwd\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def query(self, query, parameters=None):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(query, parameters)\n",
    "            return result\n",
    "\n",
    "# Instantiate the connection object\n",
    "connection = Neo4jConnection(uri, user, password) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef79ef41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Subject       Verb    Object\n",
      "0      rates   selected   cancers\n",
      "1  estimates        add  rounding\n",
      "2      rates   selected       sex\n",
      "3   injuries  accidents  injuries\n",
      "4      rates   selected      race\n",
      "                           Fourgram    Word Dependency    Head Head_POS\n",
      "0           male female male female    male       amod  female     NOUN\n",
      "1           male female male female  female       amod  female     NOUN\n",
      "2           male female male female    male       amod  female     NOUN\n",
      "3           male female male female  female       ROOT  female     NOUN\n",
      "4  cancer cancer journal clinicians  cancer   compound  cancer     NOUN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "svo_data = pd.read_csv(\"25-3-2025_14_3/fourgram_SVO_reaction.csv\")\n",
    "dependency_data = pd.read_csv(\"25-3-2025_14_3/fourgram_dependency_parsing.csv\")\n",
    "print(svo_data.head())\n",
    "print(dependency_data.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38f47511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rates', 'selected', 'cancers'), ('estimates', 'add', 'rounding'), ('rates', 'selected', 'sex'), ('injuries', 'accidents', 'injuries'), ('rates', 'selected', 'race')]\n"
     ]
    }
   ],
   "source": [
    "def extract_svo_triples(svo_data):\n",
    "    svo_triples = []\n",
    "    for _, row in svo_data.iterrows():\n",
    "        subject = row['Subject']\n",
    "        verb = row['Verb']\n",
    "        obj = row['Object']\n",
    "        svo_triples.append((subject, verb, obj))\n",
    "    return svo_triples\n",
    "\n",
    "svo_triples = extract_svo_triples(svo_data)\n",
    "print(svo_triples[:5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92cdcd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_svo_in_neo4j(subject, verb, obj):\n",
    "    query = f\"\"\"\n",
    "    MERGE (s:Subject {{name: '{subject}'}})\n",
    "    MERGE (v:Verb {{action: '{verb}'}})\n",
    "    MERGE (o:Object {{name: '{obj}'}})\n",
    "    MERGE (s)-[:PERFORMS]->(v)-[:ACTS_ON]->(o)\n",
    "    \"\"\"\n",
    "    connection.query(query)\n",
    "    \n",
    "for subject, verb, obj in svo_triples:\n",
    "    store_svo_in_neo4j(subject, verb, obj) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae880730",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be93dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"bolt://localhost:7687\"  \n",
    "user = \"neo4j\" \n",
    "password = \"Sourabh@123\"\n",
    "\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self._uri = uri\n",
    "        self._user = user\n",
    "        self._password = pwd\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def close(self):\n",
    "        self._driver.close()\n",
    "\n",
    "    def query(self, query, parameters=None):\n",
    "        with self._driver.session() as session:\n",
    "            result = session.run(query, parameters)\n",
    "            return result\n",
    "\n",
    "        \n",
    "connection = Neo4jConnection(uri, user, password)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0302ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_dependency_in_neo4j(fourgram, word, dependency, head, head_pos):\n",
    "    query = f\"\"\"\n",
    "    MERGE (w:Word {{text: '{word}', fourgram: '{fourgram}'}})\n",
    "    MERGE (h:Word {{text: '{head}', pos: '{head_pos}'}})\n",
    "    MERGE (w)-[:{dependency.upper()}]->(h)\n",
    "    \"\"\"\n",
    "    connection.query(query)\n",
    "             \n",
    "for _, row in dependency_data.iterrows():\n",
    "    fourgram = row['Fourgram']\n",
    "    word = row['Word']\n",
    "    dependency = row['Dependency']\n",
    "    head = row['Head']\n",
    "    head_pos = row['Head_POS']\n",
    "    \n",
    "    store_dependency_in_neo4j(fourgram, word, dependency, head, head_pos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a38d8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff0235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d85c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee6266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cdfbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e9cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1cf11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c4a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d52fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57ef2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ccdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e1d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d37574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity Distribution:\n",
      "count    200.000000\n",
      "mean       0.641429\n",
      "std        0.681139\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.571429\n",
      "75%        0.857143\n",
      "max        4.142857\n",
      "Name: Avg_Jaccard, dtype: float64\n",
      "Number of Fourgrams after filtering: 125\n",
      "Dependency parsing and SVO extraction complete and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "output_folder = \"25-3-2025_14_3\"\n",
    "cooccurrence_file = os.path.join(output_folder, \"fourgram_cooccurrence.csv\")\n",
    "jaccard_threshold = 0.3  \n",
    "\n",
    "if not os.path.exists(cooccurrence_file):\n",
    "    raise FileNotFoundError(f\"{cooccurrence_file} not found!\")\n",
    "\n",
    "jac_df = pd.read_csv(cooccurrence_file, index_col=0)\n",
    "jac_df[\"Avg_Jaccard\"] = jac_df.mean(axis=1)\n",
    "\n",
    "print(\"Jaccard Similarity Distribution:\")\n",
    "print(jac_df[\"Avg_Jaccard\"].describe())\n",
    "\n",
    "filtered_fourgrams = jac_df[jac_df[\"Avg_Jaccard\"] >= jaccard_threshold].index.tolist()\n",
    "print(f\"Number of Fourgrams after filtering: {len(filtered_fourgrams)}\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "dependency_data = {}\n",
    "for fg in filtered_fourgrams:\n",
    "    doc = nlp(fg.replace(\"_\", \" \"))\n",
    "    dependency_data[fg] = [\n",
    "        {\n",
    "            \"word\": token.text,\n",
    "            \"dependency\": token.dep_,\n",
    "            \"head\": token.head.text,\n",
    "            \"head_pos\": token.head.pos_,\n",
    "        }\n",
    "        for token in doc\n",
    "    ]\n",
    "\n",
    "dep_json_path = os.path.join(output_folder, \"dependency_parsing_jaccard.json\")\n",
    "with open(dep_json_path, \"w\") as f:\n",
    "    json.dump(dependency_data, f, indent=4)\n",
    "\n",
    "dep_rows = []\n",
    "for fg, deps in dependency_data.items():\n",
    "    for d in deps:\n",
    "        dep_rows.append([fg, d[\"word\"], d[\"dependency\"], d[\"head\"], d[\"head_pos\"]])\n",
    "\n",
    "dep_df = pd.DataFrame(dep_rows, columns=[\"Fourgram\", \"Word\", \"Dependency\", \"Head\", \"Head_POS\"])\n",
    "dep_csv_path = os.path.join(output_folder, \"dependency_parsing_jaccard.csv\")\n",
    "dep_df.to_csv(dep_csv_path, index=False)\n",
    "\n",
    "svo_list = []\n",
    "for fg in filtered_fourgrams:\n",
    "    doc = nlp(fg.replace(\"_\", \" \"))\n",
    "    subject, verb, obj = None, None, None\n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "            subject = token.text\n",
    "        elif token.dep_ == \"ROOT\":\n",
    "            verb = token.text\n",
    "        elif token.dep_ in {\"dobj\", \"pobj\", \"obj\"}:\n",
    "            obj = token.text\n",
    "    if subject and verb and obj:\n",
    "        svo_list.append({\"Fourgram\": fg, \"Subject\": subject, \"Verb\": verb, \"Object\": obj})\n",
    "\n",
    "svo_json_path = os.path.join(output_folder, \"SVO_jaccard.json\")\n",
    "with open(svo_json_path, \"w\") as f:\n",
    "    json.dump(svo_list, f, indent=4)\n",
    "\n",
    "svo_df = pd.DataFrame(svo_list)\n",
    "svo_csv_path = os.path.join(output_folder, \"SVO_jaccard.csv\") \n",
    "svo_df.to_csv(svo_csv_path, index=False)\n",
    "\n",
    "print(\"Dependency parsing and SVO extraction complete and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e1ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_svo(text):\n",
    "    doc = nlp(text)\n",
    "    svos = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"ROOT\", \"xcomp\"} and token.pos_ == \"VERB\":\n",
    "            subject = [w.text for w in token.lefts if w.dep_ in {\"nsubj\", \"nsubjpass\"}]\n",
    "            objects = [w.text for w in token.rights if w.dep_ in {\"dobj\", \"pobj\", \"attr\"}]\n",
    "            if subject and objects:\n",
    "                svos.append((subject[0], token.text, objects[0]))\n",
    "    return svos\n",
    "\n",
    "def build_relation_mapping(fourgram_file, output_file):\n",
    "    df = pd.read_csv(fourgram_file, index_col=0)\n",
    "    relations = []\n",
    "    \n",
    "    for fourgram in df.index:\n",
    "        text = fourgram.replace(\"_\", \" \")\n",
    "        svo_triplets = extract_svo(text)\n",
    "        \n",
    "        for subj, verb, obj in svo_triplets:\n",
    "            relations.append({\"Fourgram\": fourgram, \"Subject\": subj, \"Verb\": verb, \"Object\": obj})\n",
    "\n",
    "    relation_df = pd.DataFrame(relations)\n",
    "    relation_df.to_csv(output_file, index=False)\n",
    "\n",
    "build_relation_mapping(\"25-3-2025_14_3/dependency_parsing_jaccard.csv\", \"25-3-2025_14_3/relation_mapping_jaccard.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba9110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Fourgram    Word Dependency    Head Head_POS\n",
      "0  male female male female    male       amod  female     NOUN\n",
      "1  male female male female  female       amod  female     NOUN\n",
      "2  male female male female    male       amod  female     NOUN\n",
      "3  male female male female  female       ROOT  female     NOUN\n",
      "4  female male female male  female       amod    male     NOUN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_folder = \"25-3-2025_14_3/\"\n",
    "input_csv = os.path.join(output_folder, \"dependency_parsing_jaccard.csv\")\n",
    "\n",
    "df = pd.read_csv(input_csv)\n",
    "print(df.head())          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315352a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rates', 'selected', 'cancers'), ('rates', 'selected', 'sex'), ('injuries', 'accidents', 'injuries'), ('rates', 'selected', 'race'), ('rates', 'adjusted', 'life')]\n"
     ]
    }
   ],
   "source": [
    "def extract_relations(df):\n",
    "    relations = []\n",
    "    \n",
    "    for fourgram in df[\"Fourgram\"].unique():\n",
    "        subset = df[df[\"Fourgram\"] == fourgram]\n",
    "        \n",
    "        subject, verb, obj = None, None, None\n",
    "\n",
    "        for _, row in subset.iterrows():\n",
    "            word, dep, head = row[\"Word\"], row[\"Dependency\"], row[\"Head\"]\n",
    "\n",
    "            if dep == \"nsubj\": \n",
    "                subject = word\n",
    "            elif dep == \"ROOT\": \n",
    "                verb = word\n",
    "            elif dep in (\"dobj\", \"obj\"): \n",
    "                obj = word\n",
    "\n",
    "        if subject and verb and obj:\n",
    "            relations.append((subject, verb, obj))\n",
    "\n",
    "    return relations \n",
    "\n",
    "relations = extract_relations(df)\n",
    "print(relations[:5])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bba399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Fourgram   Subject       Verb    Object\n",
      "0    incidence rates selected cancers     rates   selected   cancers\n",
      "1          rates selected cancers sex     rates   selected       sex\n",
      "2  injuries accidents tional injuries  injuries  accidents  injuries\n",
      "3         rates selected cancers race     rates   selected      race\n",
      "4          rates adjusted normal life     rates   adjusted      life\n",
      "                  Fourgram    Word Dependency    Head Head_POS\n",
      "0  male female male female    male       amod  female     NOUN\n",
      "1  male female male female  female       amod  female     NOUN\n",
      "2  male female male female    male       amod  female     NOUN\n",
      "3  male female male female  female       ROOT  female     NOUN\n",
      "4  female male female male  female       amod    male     NOUN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "svo_data = pd.read_csv(\"25-3-2025_14_3/SVO_jaccard.csv\")\n",
    "dependency_data = pd.read_csv(\"25-3-2025_14_3/dependency_parsing_jaccard.csv\")\n",
    "print(svo_data.head())\n",
    "print(dependency_data.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ded916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rates', 'selected', 'cancers'), ('rates', 'selected', 'sex'), ('injuries', 'accidents', 'injuries'), ('rates', 'selected', 'race'), ('rates', 'adjusted', 'life')]\n"
     ]
    }
   ],
   "source": [
    "def extract_svo_triples(svo_data):\n",
    "    svo_triples = []\n",
    "    for _, row in svo_data.iterrows():\n",
    "        subject = row['Subject']\n",
    "        verb = row['Verb']\n",
    "        obj = row['Object']\n",
    "        svo_triples.append((subject, verb, obj))\n",
    "    return svo_triples\n",
    "\n",
    "svo_triples = extract_svo_triples(svo_data)\n",
    "print(svo_triples[:5])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93254ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"Sourabh@123\"  \n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855fd6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soura\\AppData\\Local\\Temp\\ipykernel_12296\\3191770680.py:15: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(insert_svo, row['Subject'], row['Verb'], row['Object'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SVO Triplets uploaded.\n"
     ]
    }
   ],
   "source": [
    "def upload_svo_to_neo4j(svo_file):\n",
    "    df = pd.read_csv(svo_file)\n",
    "\n",
    "    def insert_svo(tx, subject, verb, obj):\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (s:Entity {name: $subject})\n",
    "            MERGE (v:Action {name: $verb})\n",
    "            MERGE (o:Entity {name: $object})\n",
    "            MERGE (s)-[:PERFORMS]->(v)\n",
    "            MERGE (v)-[:ACTS_ON]->(o)\n",
    "        \"\"\", subject=subject, verb=verb, object=obj)\n",
    "\n",
    "    with driver.session() as session:\n",
    "        for _, row in df.iterrows():\n",
    "            session.write_transaction(insert_svo, row['Subject'], row['Verb'], row['Object'])\n",
    "\n",
    "upload_svo_to_neo4j(\"25-3-2025_14_3/SVO_jaccard.csv\")\n",
    "print(\" SVO Triplets uploaded.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5c09f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soura\\AppData\\Local\\Temp\\ipykernel_12296\\2648816164.py:13: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(insert_dependency, row['Word'], row['Head'], row['Dependency'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dependency Parsing uploaded.\n"
     ]
    }
   ],
   "source": [
    "def upload_dependencies_to_neo4j(dep_file):\n",
    "    df = pd.read_csv(dep_file)\n",
    "\n",
    "    def insert_dependency(tx, word, head, dep_type):\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (w:Word {name: $word})\n",
    "            MERGE (h:Word {name: $head})\n",
    "            MERGE (w)-[:DEPENDS_ON {type: $dep_type}]->(h)\n",
    "        \"\"\", word=word, head=head, dep_type=dep_type)\n",
    "\n",
    "    with driver.session() as session:\n",
    "        for _, row in df.iterrows():\n",
    "            session.write_transaction(insert_dependency, row['Word'], row['Head'], row['Dependency'])\n",
    "\n",
    "upload_dependencies_to_neo4j(\"25-3-2025_14_3/dependency_parsing_jaccard.csv\")\n",
    "print(\" Dependency Parsing uploaded.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb359507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a6ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff37b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8aaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895c9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c94f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4951ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad6e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
